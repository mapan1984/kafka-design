{"config":{"indexing":"full","lang":["en","ja"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"kafka book \u57fa\u4e8e 2.7.1","title":"0. kafka \u8bbe\u8ba1\u5b9e\u73b0\u89e3\u6790"},{"location":"#kafka-book","text":"\u57fa\u4e8e 2.7.1","title":"kafka book"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/","text":"1 server \u89e3\u6790","title":"1. server \u89e3\u6790"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/#1-server","text":"","title":"1 server \u89e3\u6790"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/1-server%20%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/","text":"1.1 server \u542f\u52a8\u8fc7\u7a0b \u542f\u52a8 \u5728\u542f\u52a8\u811a\u672c kafka-server-start.sh \u4e2d\u53ef\u4ee5\u770b\u5230\uff0c\u8fd0\u884c\u7c7b\u4e3a kafka.Kafka \uff0c\u5176 main \u51fd\u6570\u5373\u4e3a\u670d\u52a1\u542f\u52a8\u7684\u5165\u53e3\u3002 \u67e5\u770b Kafka \u7684 main \u51fd\u6570\uff0c\u5176\u5185\u5bb9\u4e3a\uff1a \u89e3\u6790\u53c2\u6570 \u6784\u5efa KafkaServer (2.8.0 \u4e4b\u540e\u652f\u6301 KafkaRaftServer ) \u5bf9\u6784\u9020\u7684 KafkaServer \u5bf9\u8c61\u8c03\u7528 startup() \u65b9\u6cd5 \u67e5\u770b KafkaServer \u4ee3\u7801\uff0c\u5176 startup \u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a initZkClient createZkClient get or create cluster_id load metadata check cluster id generate brokerId initialize dynamic borker configs from ZooKeeper start scheduler create and configure metrics","title":"1.1 server \u542f\u52a8\u8fc7\u7a0b"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/1-server%20%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/#11-server","text":"","title":"1.1 server \u542f\u52a8\u8fc7\u7a0b"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/1-server%20%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/#_1","text":"\u5728\u542f\u52a8\u811a\u672c kafka-server-start.sh \u4e2d\u53ef\u4ee5\u770b\u5230\uff0c\u8fd0\u884c\u7c7b\u4e3a kafka.Kafka \uff0c\u5176 main \u51fd\u6570\u5373\u4e3a\u670d\u52a1\u542f\u52a8\u7684\u5165\u53e3\u3002 \u67e5\u770b Kafka \u7684 main \u51fd\u6570\uff0c\u5176\u5185\u5bb9\u4e3a\uff1a \u89e3\u6790\u53c2\u6570 \u6784\u5efa KafkaServer (2.8.0 \u4e4b\u540e\u652f\u6301 KafkaRaftServer ) \u5bf9\u6784\u9020\u7684 KafkaServer \u5bf9\u8c61\u8c03\u7528 startup() \u65b9\u6cd5 \u67e5\u770b KafkaServer \u4ee3\u7801\uff0c\u5176 startup \u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a initZkClient createZkClient get or create cluster_id load metadata check cluster id generate brokerId initialize dynamic borker configs from ZooKeeper start scheduler create and configure metrics","title":"\u542f\u52a8"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","text":"1.2 server \u7f51\u7edc\u6a21\u578b \u4ece\u4e0a\u4e00\u8282\u6211\u4eec\u5f97\u77e5 kafka \u670d\u52a1\u542f\u52a8\u7684\u52a8\u4f5c\u5bf9\u5e94\u7684\u4ee3\u7801\u662f KafkaServer \u7684 startup() \u51fd\u6570\uff0c\u5728 startup() \u51fd\u6570\u4e2d\uff0c \u521b\u5efa\u5e76\u542f\u52a8\u4e86\u4e00\u7cfb\u5217\u7ec4\u4ef6\uff0c\u5176\u4e2d\u5305\u62ec SocketServer \u3002 // Create and start the socket server acceptor threads so that the bound port is known. // Delay starting processors until the end of the initialization sequence to ensure // that credentials have been loaded before processing authentications. // // Note that we allow the use of KRaft mode controller APIs when forwarding is enabled // so that the Envelope request is exposed. This is only used in testing currently. socketServer = new SocketServer ( config , metrics , time , credentialProvider , apiVersionManager ) socketServer . startup ( startProcessingRequests = false ) \u8fd9\u4e00\u8282\u89e3\u6790 server \u7aef\u7684\u7f51\u7edc\u6a21\u578b\uff0c\u5373 server \u7aef\u63a5\u6536\u8bf7\u6c42\uff0c\u5904\u7406\u5e76\u8fd4\u56de\u54cd\u5e94\u7684\u8fc7\u7a0b\u3002 \u5bf9\u8bf7\u6c42\u7684\u76d1\u542c\u6b63\u662f\u901a\u8fc7 SocketServer \u7ec4\u4ef6\u5b8c\u6210\u3002 SocketServer SocketServer \u652f\u6301 2 \u79cd\u7c7b\u578b\u7684\u8bf7\u6c42\uff1a \u6570\u636e(data plane)\uff1a \u5904\u7406\u6765\u81ea client \u6216\u8005\u540c\u96c6\u7fa4\u5176\u4ed6 broker \u7684\u8bf7\u6c42 \u7ebf\u7a0b\u6a21\u578b\u662f\uff1a \u53ef\u4ee5\u914d\u7f6e\u591a\u4e2a listener\uff0c\u6bcf\u4e2a listener \u5bf9\u5e94 1 \u4e2a Acceptor \u7ebf\u7a0b\uff0c\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42 Acceptor \u6709 N \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea sockets \u7684\u8bf7\u6c42( num.network.threads ) M \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b( num.io.threads ) \u63a7\u5236(control plane)\uff1a \u5904\u7406\u6765\u81ea controller \u7684\u8bf7\u6c42\u3002\u8fd9\u662f\u53ef\u9009\u7684\u5e76\u4e14\u901a\u8fc7\u6307\u5b9a control.plane.listener.name \u8fdb\u884c\u914d\u7f6e\u3002\u5982\u679c\u6ca1\u6709\u914d\u7f6e\uff0ccontroller \u7684\u8bf7\u6c42\u4e5f\u4f1a\u50cf\u6570\u636e\u8bf7\u6c42\u4e00\u6837\u901a\u8fc7 listener \u5904\u7406 \u7ebf\u7a0b\u6a21\u578b\u662f\uff1a 1 \u4e2a Acceptor \u7ebf\u7a0b\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42 Acceptor \u6709 1 \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea socket \u7684\u8bf7\u6c42 1 \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b \u67e5\u770b SocketServer \u4ee3\u7801\uff0c SocketServer \u6784\u9020\u65f6\u4f1a\u521d\u59cb\u4ee5\u4e0b\u5173\u952e\u5c5e\u6027\uff1a // data-plane private val dataPlaneProcessors = new ConcurrentHashMap [ Int , Processor ]() private [ network ] val dataPlaneAcceptors = new ConcurrentHashMap [ EndPoint , Acceptor ]() val dataPlaneRequestChannel = new RequestChannel ( maxQueuedRequests , DataPlaneMetricPrefix , time ) // control-plane private var controlPlaneProcessorOpt : Option [ Processor ] = None private [ network ] var controlPlaneAcceptorOpt : Option [ Acceptor ] = None val controlPlaneRequestChannelOpt : Option [ RequestChannel ] = config . controlPlaneListenerName . map ( _ => new RequestChannel ( 20 , ControlPlaneMetricPrefix , time )) SocketServer \u7684 startup() \u51fd\u6570\u4e2d\u4e3b\u8981\u52a8\u4f5c\u4e3a\uff1a this . synchronized { connectionQuotas = new ConnectionQuotas ( config , time , metrics ) // 1. \u521b\u5efa\u5904\u7406 controller \u8bf7\u6c42\u7684 Acceptor, Processor createControlPlaneAcceptorAndProcessor ( config . controlPlaneListener ) // 2. \u521b\u5efa\u5904\u7406\u6570\u636e\u8bf7\u6c42\u7684 Acceptor, Processor createDataPlaneAcceptorsAndProcessors ( config . numNetworkThreads , config . dataPlaneListeners ) if ( startProcessingRequests ) { // 3. \u542f\u52a8\u7ebf\u7a0b kafkaThread\uff0c\u540e\u53f0\u8fd0\u884c Acceptor \u4e0e Processor this . startProcessingRequests () } } \u4e3b\u8981\u6709 3 \u6b65\uff1a \u521b\u5efa\u5904\u7406 controller \u8bf7\u6c42\u7684 Acceptor, Processor \u521b\u5efa\u5904\u7406\u6570\u636e\u8bf7\u6c42\u7684 Acceptor, Processor \u542f\u52a8\u7ebf\u7a0b kafkaThread\uff0c\u540e\u53f0\u8fd0\u884c Acceptor \u4e0e Processor private def createDataPlaneAcceptorsAndProcessors ( dataProcessorsPerListener : Int , endpoints : Seq [ EndPoint ]): Unit = { // \u5bf9\u6bcf\u4e2a endpoint endpoints . foreach { endpoint => connectionQuotas . addListener ( config , endpoint . listenerName ) // \u521b\u5efa Acceptor val dataPlaneAcceptor = createAcceptor ( endpoint , DataPlaneMetricPrefix ) // 1. \u521b\u5efa dataProcessorsPerListener \u4e2a Processor // 2. \u5c06 Processor \u52a0\u5165\u5230 dataPlaneRequestChannel \u7684 processors \u4e2d // 3. \u5c06 Processor \u52a0\u5165\u5230 SocketServer \u7684 dataPlaneProcessors \u4e2d // 4. \u5c06 Processor \u52a0\u5165\u5230 Acceptor \u7684 processors \u4e2d addDataPlaneProcessors ( dataPlaneAcceptor , endpoint , dataProcessorsPerListener ) // \u5c06 Acceptor \u52a0\u5165\u5230 SocketServer \u7684 dataPlaneAcceptors \u4e2d dataPlaneAcceptors . put ( endpoint , dataPlaneAcceptor ) info ( s\"Created data-plane acceptor and processors for endpoint : ${ endpoint . listenerName } \" ) } } private def createControlPlaneAcceptorAndProcessor ( endpointOpt : Option [ EndPoint ]): Unit = { endpointOpt . foreach { endpoint => connectionQuotas . addListener ( config , endpoint . listenerName ) val controlPlaneAcceptor = createAcceptor ( endpoint , ControlPlaneMetricPrefix ) val controlPlaneProcessor = newProcessor ( nextProcessorId , controlPlaneRequestChannelOpt . get , connectionQuotas , endpoint . listenerName , endpoint . securityProtocol , memoryPool ) controlPlaneAcceptorOpt = Some ( controlPlaneAcceptor ) controlPlaneProcessorOpt = Some ( controlPlaneProcessor ) val listenerProcessors = new ArrayBuffer [ Processor ]() listenerProcessors += controlPlaneProcessor controlPlaneRequestChannelOpt . foreach ( _ . addProcessor ( controlPlaneProcessor )) nextProcessorId += 1 controlPlaneAcceptor . addProcessors ( listenerProcessors , ControlPlaneThreadPrefix ) info ( s\"Created control-plane acceptor and processor for endpoint : ${ endpoint . listenerName } \" ) } } \u8fd9\u91cc\u505a\u7684\u5de5\u4f5c\u4e3a\uff1a \u4e3a\u6bcf\u4e2a endpoint \u521b\u5efa 1 \u4e2a Acceptor \uff0c Acceptor \u8d1f\u8d23\u76d1\u542c\u8bf7\u6c42 \u5bf9\u6bcf\u4e2a endpoint \u521b\u5efa N ( num.network.threads ) \u4e2a Processor \uff0c\u540c\u65f6\uff1a \u5c06 Processor \u8bb0\u5f55\u5230 RequestChannel \u7684 processors hash map \u4e2d \u5c06 Processor \u8bb0\u5f55\u5230 SocketServer \u7684 dataPlaneProcessors hash map \u4e2d \u5c06 Processor \u52a0\u5165\u5230 Acceptor \u7684 processors \u6570\u7ec4\u4e2d \u5c06 Acceptor \u52a0\u5165\u5230 SocketServer \u7684 dataPlaneAcceptors hash map \u4e2d \u8fd9\u91cc\u5b8c\u6210\u4e4b\u540e\uff0c\u5904\u7406\u8bf7\u6c42\u6240\u9700\u7684 SocketServer , Acceptor , Processor , RequestChannel \u5c31\u90fd\u521b\u5efa\u5b8c\u6210\u4e86\uff0c\u4e0b\u9762\u4e3b\u8981\u770b\u4e00\u4e0b Acceptor , Processor , RequestChannel \u5404\u81ea\u7684\u4f5c\u7528\u3002 Acceptor Acceptor Processor Processor RequestChannel RequestChannel KafkaRequestHandler KafkaRequestHandler \u76d1\u63a7 requestHandlerAvgIdlePercent Meter \u7c7b\u578b\uff0c\u5728 KafkaRequestHandlerPool \u4e2d\u521b\u5efa\uff0c\u521b\u5efa\u6bcf\u4e2a KafkaRequestHandler \u65f6\u4f20\u5165 /* a meter to track the average free capacity of the request handlers */ private val aggregateIdleMeter = newMeter ( requestHandlerAvgIdleMetricName , \"percent\" , TimeUnit . NANOSECONDS ) val runnables = new mutable . ArrayBuffer [ KafkaRequestHandler ]( numThreads ) for ( i <- 0 until numThreads ) { createHandler ( i ) } def createHandler ( id : Int ): Unit = synchronized { runnables += new KafkaRequestHandler ( id , brokerId , aggregateIdleMeter , threadPoolSize , requestChannel , apis , time ) KafkaThread . daemon ( logAndThreadNamePrefix + \"-kafka-request-handler-\" + id , runnables ( id )). start () } KafkaRequestHandler \u4f5c\u4e3a\u540e\u53f0\u7ebf\u7a0b\u542f\u52a8\uff0c\u542f\u52a8\u540e\u4e0d\u540c\u4ece RequestChannel \u4e2d\u83b7\u53d6\u8bf7\u6c42\uff0c Meter.mark() \u65f6\uff0c\u4ee5\u83b7\u53d6\u8bf7\u6c42\u7684\u8017\u65f6\u9664\u4ee5\u7ebf\u7a0b\u603b\u6570\u3002 def run (): Unit = { while ( ! stopped ) { // We use a single meter for aggregate idle percentage for the thread pool. // Since meter is calculated as total_recorded_value / time_window and // time_window is independent of the number of threads, each recorded idle // time should be discounted by # threads. val startSelectTime = time . nanoseconds val req = requestChannel . receiveRequest ( 300 ) val endTime = time . nanoseconds val idleTime = endTime - startSelectTime // NOTE: \u4ece RequestChannel \u4e2d\u83b7\u53d6\u8bf7\u6c42\u7684\u65f6\u95f4\uff0c\u662f KafkaRequestHandler \u7a7a\u95f2\u7684\u65f6\u95f4 aggregateIdleMeter . mark ( idleTime / totalHandlerThreads . get ) // NOTE: \u5904\u7406\u8bf7\u6c42\u7684\u65f6\u95f4\u4e3a KafkaRequestHandler \u975e\u7a7a\u95f2\u7684\u65f6\u95f4 req match { case RequestChannel . ShutdownRequest => debug ( s\"Kafka request handler $ id on broker $ brokerId received shut down command\" ) shutdownComplete . countDown () return case request : RequestChannel . Request => try { request . requestDequeueTimeNanos = endTime trace ( s\"Kafka request handler $ id on broker $ brokerId handling request $ request \" ) apis . handle ( request ) } catch { case e : FatalExitError => shutdownComplete . countDown () Exit . exit ( e . statusCode ) case e : Throwable => error ( \"Exception when handling request\" , e ) } finally { request . releaseBuffer () } case null => // continue } } shutdownComplete . countDown () } requestHandler \u7ebf\u7a0b\u4e0d\u65ad\u5faa\u73af\u505a 2 \u4ef6\u4e8b\uff1a \u4ece channel \u4e2d\u83b7\u53d6\u5f85\u5904\u7406\u8bf7\u6c42 \u5904\u7406\u8bf7\u6c42\uff08\u8bfb/\u5199\u8bf7\u6c42/\u5176\u4ed6\uff09 \u5047\u8bbe\u6bcf\u6b21\u5faa\u73af\uff0c\u7b2c 1 \u4ef6\u4e8b\u8017\u65f6 A\uff0c\u7b2c 2 \u4ef6\u4e8b\u8017\u65f6 B \u7a7a\u95f2\u7387 = A / (A + B) \u6240\u4ee5\u7a7a\u95f2\u7387\u589e\u52a0\uff0c\u53ef\u80fd\u7684\u539f\u56e0\u662f\u8bf7\u6c42\u6570\u51cf\u5c11\uff0c\u5bfc\u81f4 A \u589e\u52a0\uff1b\u6216\u8005\u8bfb/\u5199\u78c1\u76d8\u66f4\u5feb\uff0c\u5904\u7406\u8bf7\u6c42\u8017\u65f6\u51cf\u5c11 NetworkProcessorAvgIdlePercent Gauge \u7c7b\u578b\uff0c\u5728 SocketServer.startup() \u65f6\u5b9a\u4e49 newGauge ( s\" ${ DataPlaneMetricPrefix } NetworkProcessorAvgIdlePercent\" , () => SocketServer . this . synchronized { val ioWaitRatioMetricNames = dataPlaneProcessors . values . asScala . iterator . map { p => metrics . metricName ( \"io-wait-ratio\" , MetricsGroup , p . metricTags ) } ioWaitRatioMetricNames . map { metricName => Option ( metrics . metric ( metricName )). fold ( 0.0 )( m => Math . min ( m . metricValue . asInstanceOf [ Double ], 1.0 )) }. sum / dataPlaneProcessors . size }) \u8ba1\u7b97\u65b9\u6cd5\u662f\u53d6\u5f97\u6bcf\u4e2a networkProcessor \u7684 io-wait-ratio\uff0c\u9664\u4ee5 networkProcessor \u603b\u6570\u3002io-wait \u7684\u6bd4\u4f8b\uff0c\u5373\u662f networkProcessor \u7a7a\u95f2\u7684\u5360\u6bd4\u3002 io-wait-ratio \u6307\u6807\u5728 Selector \u4e2d\u8ba1\u7b97\uff0c Meter \u7c7b\u578b","title":"1.2 server \u7f51\u7edc\u6a21\u578b"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#12-server","text":"\u4ece\u4e0a\u4e00\u8282\u6211\u4eec\u5f97\u77e5 kafka \u670d\u52a1\u542f\u52a8\u7684\u52a8\u4f5c\u5bf9\u5e94\u7684\u4ee3\u7801\u662f KafkaServer \u7684 startup() \u51fd\u6570\uff0c\u5728 startup() \u51fd\u6570\u4e2d\uff0c \u521b\u5efa\u5e76\u542f\u52a8\u4e86\u4e00\u7cfb\u5217\u7ec4\u4ef6\uff0c\u5176\u4e2d\u5305\u62ec SocketServer \u3002 // Create and start the socket server acceptor threads so that the bound port is known. // Delay starting processors until the end of the initialization sequence to ensure // that credentials have been loaded before processing authentications. // // Note that we allow the use of KRaft mode controller APIs when forwarding is enabled // so that the Envelope request is exposed. This is only used in testing currently. socketServer = new SocketServer ( config , metrics , time , credentialProvider , apiVersionManager ) socketServer . startup ( startProcessingRequests = false ) \u8fd9\u4e00\u8282\u89e3\u6790 server \u7aef\u7684\u7f51\u7edc\u6a21\u578b\uff0c\u5373 server \u7aef\u63a5\u6536\u8bf7\u6c42\uff0c\u5904\u7406\u5e76\u8fd4\u56de\u54cd\u5e94\u7684\u8fc7\u7a0b\u3002 \u5bf9\u8bf7\u6c42\u7684\u76d1\u542c\u6b63\u662f\u901a\u8fc7 SocketServer \u7ec4\u4ef6\u5b8c\u6210\u3002","title":"1.2 server \u7f51\u7edc\u6a21\u578b"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#socketserver","text":"SocketServer \u652f\u6301 2 \u79cd\u7c7b\u578b\u7684\u8bf7\u6c42\uff1a \u6570\u636e(data plane)\uff1a \u5904\u7406\u6765\u81ea client \u6216\u8005\u540c\u96c6\u7fa4\u5176\u4ed6 broker \u7684\u8bf7\u6c42 \u7ebf\u7a0b\u6a21\u578b\u662f\uff1a \u53ef\u4ee5\u914d\u7f6e\u591a\u4e2a listener\uff0c\u6bcf\u4e2a listener \u5bf9\u5e94 1 \u4e2a Acceptor \u7ebf\u7a0b\uff0c\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42 Acceptor \u6709 N \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea sockets \u7684\u8bf7\u6c42( num.network.threads ) M \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b( num.io.threads ) \u63a7\u5236(control plane)\uff1a \u5904\u7406\u6765\u81ea controller \u7684\u8bf7\u6c42\u3002\u8fd9\u662f\u53ef\u9009\u7684\u5e76\u4e14\u901a\u8fc7\u6307\u5b9a control.plane.listener.name \u8fdb\u884c\u914d\u7f6e\u3002\u5982\u679c\u6ca1\u6709\u914d\u7f6e\uff0ccontroller \u7684\u8bf7\u6c42\u4e5f\u4f1a\u50cf\u6570\u636e\u8bf7\u6c42\u4e00\u6837\u901a\u8fc7 listener \u5904\u7406 \u7ebf\u7a0b\u6a21\u578b\u662f\uff1a 1 \u4e2a Acceptor \u7ebf\u7a0b\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42 Acceptor \u6709 1 \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea socket \u7684\u8bf7\u6c42 1 \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b \u67e5\u770b SocketServer \u4ee3\u7801\uff0c SocketServer \u6784\u9020\u65f6\u4f1a\u521d\u59cb\u4ee5\u4e0b\u5173\u952e\u5c5e\u6027\uff1a // data-plane private val dataPlaneProcessors = new ConcurrentHashMap [ Int , Processor ]() private [ network ] val dataPlaneAcceptors = new ConcurrentHashMap [ EndPoint , Acceptor ]() val dataPlaneRequestChannel = new RequestChannel ( maxQueuedRequests , DataPlaneMetricPrefix , time ) // control-plane private var controlPlaneProcessorOpt : Option [ Processor ] = None private [ network ] var controlPlaneAcceptorOpt : Option [ Acceptor ] = None val controlPlaneRequestChannelOpt : Option [ RequestChannel ] = config . controlPlaneListenerName . map ( _ => new RequestChannel ( 20 , ControlPlaneMetricPrefix , time )) SocketServer \u7684 startup() \u51fd\u6570\u4e2d\u4e3b\u8981\u52a8\u4f5c\u4e3a\uff1a this . synchronized { connectionQuotas = new ConnectionQuotas ( config , time , metrics ) // 1. \u521b\u5efa\u5904\u7406 controller \u8bf7\u6c42\u7684 Acceptor, Processor createControlPlaneAcceptorAndProcessor ( config . controlPlaneListener ) // 2. \u521b\u5efa\u5904\u7406\u6570\u636e\u8bf7\u6c42\u7684 Acceptor, Processor createDataPlaneAcceptorsAndProcessors ( config . numNetworkThreads , config . dataPlaneListeners ) if ( startProcessingRequests ) { // 3. \u542f\u52a8\u7ebf\u7a0b kafkaThread\uff0c\u540e\u53f0\u8fd0\u884c Acceptor \u4e0e Processor this . startProcessingRequests () } } \u4e3b\u8981\u6709 3 \u6b65\uff1a \u521b\u5efa\u5904\u7406 controller \u8bf7\u6c42\u7684 Acceptor, Processor \u521b\u5efa\u5904\u7406\u6570\u636e\u8bf7\u6c42\u7684 Acceptor, Processor \u542f\u52a8\u7ebf\u7a0b kafkaThread\uff0c\u540e\u53f0\u8fd0\u884c Acceptor \u4e0e Processor private def createDataPlaneAcceptorsAndProcessors ( dataProcessorsPerListener : Int , endpoints : Seq [ EndPoint ]): Unit = { // \u5bf9\u6bcf\u4e2a endpoint endpoints . foreach { endpoint => connectionQuotas . addListener ( config , endpoint . listenerName ) // \u521b\u5efa Acceptor val dataPlaneAcceptor = createAcceptor ( endpoint , DataPlaneMetricPrefix ) // 1. \u521b\u5efa dataProcessorsPerListener \u4e2a Processor // 2. \u5c06 Processor \u52a0\u5165\u5230 dataPlaneRequestChannel \u7684 processors \u4e2d // 3. \u5c06 Processor \u52a0\u5165\u5230 SocketServer \u7684 dataPlaneProcessors \u4e2d // 4. \u5c06 Processor \u52a0\u5165\u5230 Acceptor \u7684 processors \u4e2d addDataPlaneProcessors ( dataPlaneAcceptor , endpoint , dataProcessorsPerListener ) // \u5c06 Acceptor \u52a0\u5165\u5230 SocketServer \u7684 dataPlaneAcceptors \u4e2d dataPlaneAcceptors . put ( endpoint , dataPlaneAcceptor ) info ( s\"Created data-plane acceptor and processors for endpoint : ${ endpoint . listenerName } \" ) } } private def createControlPlaneAcceptorAndProcessor ( endpointOpt : Option [ EndPoint ]): Unit = { endpointOpt . foreach { endpoint => connectionQuotas . addListener ( config , endpoint . listenerName ) val controlPlaneAcceptor = createAcceptor ( endpoint , ControlPlaneMetricPrefix ) val controlPlaneProcessor = newProcessor ( nextProcessorId , controlPlaneRequestChannelOpt . get , connectionQuotas , endpoint . listenerName , endpoint . securityProtocol , memoryPool ) controlPlaneAcceptorOpt = Some ( controlPlaneAcceptor ) controlPlaneProcessorOpt = Some ( controlPlaneProcessor ) val listenerProcessors = new ArrayBuffer [ Processor ]() listenerProcessors += controlPlaneProcessor controlPlaneRequestChannelOpt . foreach ( _ . addProcessor ( controlPlaneProcessor )) nextProcessorId += 1 controlPlaneAcceptor . addProcessors ( listenerProcessors , ControlPlaneThreadPrefix ) info ( s\"Created control-plane acceptor and processor for endpoint : ${ endpoint . listenerName } \" ) } } \u8fd9\u91cc\u505a\u7684\u5de5\u4f5c\u4e3a\uff1a \u4e3a\u6bcf\u4e2a endpoint \u521b\u5efa 1 \u4e2a Acceptor \uff0c Acceptor \u8d1f\u8d23\u76d1\u542c\u8bf7\u6c42 \u5bf9\u6bcf\u4e2a endpoint \u521b\u5efa N ( num.network.threads ) \u4e2a Processor \uff0c\u540c\u65f6\uff1a \u5c06 Processor \u8bb0\u5f55\u5230 RequestChannel \u7684 processors hash map \u4e2d \u5c06 Processor \u8bb0\u5f55\u5230 SocketServer \u7684 dataPlaneProcessors hash map \u4e2d \u5c06 Processor \u52a0\u5165\u5230 Acceptor \u7684 processors \u6570\u7ec4\u4e2d \u5c06 Acceptor \u52a0\u5165\u5230 SocketServer \u7684 dataPlaneAcceptors hash map \u4e2d \u8fd9\u91cc\u5b8c\u6210\u4e4b\u540e\uff0c\u5904\u7406\u8bf7\u6c42\u6240\u9700\u7684 SocketServer , Acceptor , Processor , RequestChannel \u5c31\u90fd\u521b\u5efa\u5b8c\u6210\u4e86\uff0c\u4e0b\u9762\u4e3b\u8981\u770b\u4e00\u4e0b Acceptor , Processor , RequestChannel \u5404\u81ea\u7684\u4f5c\u7528\u3002","title":"SocketServer"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#acceptor","text":"Acceptor","title":"Acceptor"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#processor","text":"Processor","title":"Processor"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#requestchannel","text":"RequestChannel","title":"RequestChannel"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#kafkarequesthandler","text":"KafkaRequestHandler","title":"KafkaRequestHandler"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#_1","text":"","title":"\u76d1\u63a7"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#requesthandleravgidlepercent","text":"Meter \u7c7b\u578b\uff0c\u5728 KafkaRequestHandlerPool \u4e2d\u521b\u5efa\uff0c\u521b\u5efa\u6bcf\u4e2a KafkaRequestHandler \u65f6\u4f20\u5165 /* a meter to track the average free capacity of the request handlers */ private val aggregateIdleMeter = newMeter ( requestHandlerAvgIdleMetricName , \"percent\" , TimeUnit . NANOSECONDS ) val runnables = new mutable . ArrayBuffer [ KafkaRequestHandler ]( numThreads ) for ( i <- 0 until numThreads ) { createHandler ( i ) } def createHandler ( id : Int ): Unit = synchronized { runnables += new KafkaRequestHandler ( id , brokerId , aggregateIdleMeter , threadPoolSize , requestChannel , apis , time ) KafkaThread . daemon ( logAndThreadNamePrefix + \"-kafka-request-handler-\" + id , runnables ( id )). start () } KafkaRequestHandler \u4f5c\u4e3a\u540e\u53f0\u7ebf\u7a0b\u542f\u52a8\uff0c\u542f\u52a8\u540e\u4e0d\u540c\u4ece RequestChannel \u4e2d\u83b7\u53d6\u8bf7\u6c42\uff0c Meter.mark() \u65f6\uff0c\u4ee5\u83b7\u53d6\u8bf7\u6c42\u7684\u8017\u65f6\u9664\u4ee5\u7ebf\u7a0b\u603b\u6570\u3002 def run (): Unit = { while ( ! stopped ) { // We use a single meter for aggregate idle percentage for the thread pool. // Since meter is calculated as total_recorded_value / time_window and // time_window is independent of the number of threads, each recorded idle // time should be discounted by # threads. val startSelectTime = time . nanoseconds val req = requestChannel . receiveRequest ( 300 ) val endTime = time . nanoseconds val idleTime = endTime - startSelectTime // NOTE: \u4ece RequestChannel \u4e2d\u83b7\u53d6\u8bf7\u6c42\u7684\u65f6\u95f4\uff0c\u662f KafkaRequestHandler \u7a7a\u95f2\u7684\u65f6\u95f4 aggregateIdleMeter . mark ( idleTime / totalHandlerThreads . get ) // NOTE: \u5904\u7406\u8bf7\u6c42\u7684\u65f6\u95f4\u4e3a KafkaRequestHandler \u975e\u7a7a\u95f2\u7684\u65f6\u95f4 req match { case RequestChannel . ShutdownRequest => debug ( s\"Kafka request handler $ id on broker $ brokerId received shut down command\" ) shutdownComplete . countDown () return case request : RequestChannel . Request => try { request . requestDequeueTimeNanos = endTime trace ( s\"Kafka request handler $ id on broker $ brokerId handling request $ request \" ) apis . handle ( request ) } catch { case e : FatalExitError => shutdownComplete . countDown () Exit . exit ( e . statusCode ) case e : Throwable => error ( \"Exception when handling request\" , e ) } finally { request . releaseBuffer () } case null => // continue } } shutdownComplete . countDown () } requestHandler \u7ebf\u7a0b\u4e0d\u65ad\u5faa\u73af\u505a 2 \u4ef6\u4e8b\uff1a \u4ece channel \u4e2d\u83b7\u53d6\u5f85\u5904\u7406\u8bf7\u6c42 \u5904\u7406\u8bf7\u6c42\uff08\u8bfb/\u5199\u8bf7\u6c42/\u5176\u4ed6\uff09 \u5047\u8bbe\u6bcf\u6b21\u5faa\u73af\uff0c\u7b2c 1 \u4ef6\u4e8b\u8017\u65f6 A\uff0c\u7b2c 2 \u4ef6\u4e8b\u8017\u65f6 B \u7a7a\u95f2\u7387 = A / (A + B) \u6240\u4ee5\u7a7a\u95f2\u7387\u589e\u52a0\uff0c\u53ef\u80fd\u7684\u539f\u56e0\u662f\u8bf7\u6c42\u6570\u51cf\u5c11\uff0c\u5bfc\u81f4 A \u589e\u52a0\uff1b\u6216\u8005\u8bfb/\u5199\u78c1\u76d8\u66f4\u5feb\uff0c\u5904\u7406\u8bf7\u6c42\u8017\u65f6\u51cf\u5c11","title":"requestHandlerAvgIdlePercent"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#networkprocessoravgidlepercent","text":"Gauge \u7c7b\u578b\uff0c\u5728 SocketServer.startup() \u65f6\u5b9a\u4e49 newGauge ( s\" ${ DataPlaneMetricPrefix } NetworkProcessorAvgIdlePercent\" , () => SocketServer . this . synchronized { val ioWaitRatioMetricNames = dataPlaneProcessors . values . asScala . iterator . map { p => metrics . metricName ( \"io-wait-ratio\" , MetricsGroup , p . metricTags ) } ioWaitRatioMetricNames . map { metricName => Option ( metrics . metric ( metricName )). fold ( 0.0 )( m => Math . min ( m . metricValue . asInstanceOf [ Double ], 1.0 )) }. sum / dataPlaneProcessors . size }) \u8ba1\u7b97\u65b9\u6cd5\u662f\u53d6\u5f97\u6bcf\u4e2a networkProcessor \u7684 io-wait-ratio\uff0c\u9664\u4ee5 networkProcessor \u603b\u6570\u3002io-wait \u7684\u6bd4\u4f8b\uff0c\u5373\u662f networkProcessor \u7a7a\u95f2\u7684\u5360\u6bd4\u3002 io-wait-ratio \u6307\u6807\u5728 Selector \u4e2d\u8ba1\u7b97\uff0c Meter \u7c7b\u578b","title":"NetworkProcessorAvgIdlePercent"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/","text":"1.3 controller Kafka \u96c6\u7fa4\u4e2d\u4f1a\u6709\u4e00\u4e2a broker \u4f5c\u4e3a\u96c6\u7fa4 controller\uff0c\u8d1f\u8d23\u4e00\u4e9b\u7ba1\u7406\u548c\u534f\u8c03\u7684\u5de5\u4f5c\uff1a \u76d1\u542c /brokers/topics \uff0c\u89e6\u53d1 topic \u521b\u5efa\u52a8\u4f5c \u76d1\u542c /admin/delete_topics \uff0c\u89e6\u53d1 topic \u5220\u9664\u52a8\u4f5c \u76d1\u542c /admin/reassign_partitions \uff0c\u89e6\u53d1 topic \u91cd\u5206\u533a\u64cd\u4f5c \u76d1\u542c /admin/preferred_replica_election \uff0c\u89e6\u53d1\u6700\u4f18 leader \u9009\u4e3e\u64cd\u4f5c \u76d1\u542c /brokers/topics/<topic> \uff0c\u89e6\u53d1 topic partition \u6269\u5bb9\u64cd\u4f5c \u76d1\u542c /brokers/ids \uff0c\u89e6\u53d1 broker \u4e0a\u7ebf/\u4e0b\u7ebf \u64cd\u4f5c \u5f53\u96c6\u7fa4\u5143\u4fe1\u606f\u53d8\u52a8\u65f6\uff0ccontroller \u901a\u8fc7 UpdateMetadataRequest \u8bf7\u6c42\u5e7f\u64ad\u4fe1\u606f\u5230\u96c6\u7fa4\u7684\u6240\u6709 broker \u4e0a\uff0c\u901a\u8fc7 LeaderAndIsrRequest , StopReplicaRequest \u53d1\u9001\u8bf7\u6c42\u5230\u76f8\u5173 broker broker \u81ea\u884c\u505c\u6b62\u65f6\uff0c\u5411 controller \u53d1\u9001 ControlledShudownRequest \u8bf7\u6c42\uff0ccontroller \u8d1f\u8d23\u6536\u5c3e\u5de5\u4f5c \u76d1\u542c /controller \uff0c\u5982\u679c\u8be5\u8282\u70b9\u6d88\u5931\uff0c\u6240\u6709 broker \u62a2\u5360 /controller \uff0c\u62a2\u5360\u6210\u529f\u5219\u6210\u4e3a\u65b0\u7684 controller \u53c2\u8003\uff1a - https://cwiki.apache.org/confluence/display/kafka/kafka+controller+redesign - https://jaceklaskowski.gitbooks.io/apache-kafka/content/kafka-controller-ControllerEventManager.html - https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment","title":"1.3 KafkaController"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/#13-controller","text":"Kafka \u96c6\u7fa4\u4e2d\u4f1a\u6709\u4e00\u4e2a broker \u4f5c\u4e3a\u96c6\u7fa4 controller\uff0c\u8d1f\u8d23\u4e00\u4e9b\u7ba1\u7406\u548c\u534f\u8c03\u7684\u5de5\u4f5c\uff1a \u76d1\u542c /brokers/topics \uff0c\u89e6\u53d1 topic \u521b\u5efa\u52a8\u4f5c \u76d1\u542c /admin/delete_topics \uff0c\u89e6\u53d1 topic \u5220\u9664\u52a8\u4f5c \u76d1\u542c /admin/reassign_partitions \uff0c\u89e6\u53d1 topic \u91cd\u5206\u533a\u64cd\u4f5c \u76d1\u542c /admin/preferred_replica_election \uff0c\u89e6\u53d1\u6700\u4f18 leader \u9009\u4e3e\u64cd\u4f5c \u76d1\u542c /brokers/topics/<topic> \uff0c\u89e6\u53d1 topic partition \u6269\u5bb9\u64cd\u4f5c \u76d1\u542c /brokers/ids \uff0c\u89e6\u53d1 broker \u4e0a\u7ebf/\u4e0b\u7ebf \u64cd\u4f5c \u5f53\u96c6\u7fa4\u5143\u4fe1\u606f\u53d8\u52a8\u65f6\uff0ccontroller \u901a\u8fc7 UpdateMetadataRequest \u8bf7\u6c42\u5e7f\u64ad\u4fe1\u606f\u5230\u96c6\u7fa4\u7684\u6240\u6709 broker \u4e0a\uff0c\u901a\u8fc7 LeaderAndIsrRequest , StopReplicaRequest \u53d1\u9001\u8bf7\u6c42\u5230\u76f8\u5173 broker broker \u81ea\u884c\u505c\u6b62\u65f6\uff0c\u5411 controller \u53d1\u9001 ControlledShudownRequest \u8bf7\u6c42\uff0ccontroller \u8d1f\u8d23\u6536\u5c3e\u5de5\u4f5c \u76d1\u542c /controller \uff0c\u5982\u679c\u8be5\u8282\u70b9\u6d88\u5931\uff0c\u6240\u6709 broker \u62a2\u5360 /controller \uff0c\u62a2\u5360\u6210\u529f\u5219\u6210\u4e3a\u65b0\u7684 controller \u53c2\u8003\uff1a - https://cwiki.apache.org/confluence/display/kafka/kafka+controller+redesign - https://jaceklaskowski.gitbooks.io/apache-kafka/content/kafka-controller-ControllerEventManager.html - https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment","title":"1.3 controller"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/1-controller%E5%90%AF%E5%8A%A8/","text":"1.3.1 controller \u542f\u52a8 controller \u5728 kafkaServer.startup() \u4e2d\u521d\u59cb\u5316\u5e76\u542f\u52a8\uff1a /** * Start up API for bringing up a single instance of the Kafka server. * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers */ def startup (): Unit = { try { info ( \"starting\" ) if ( isShuttingDown . get ) throw new IllegalStateException ( \"Kafka server is still shutting down, cannot re-start!\" ) if ( startupComplete . get ) return val canStartup = isStartingUp . compareAndSet ( false , true ) if ( canStartup ) { // ... /* start kafka controller */ kafkaController = new KafkaController ( config , zkClient , time , metrics , brokerInfo , brokerEpoch , tokenManager , brokerFeatures , featureCache , threadNamePrefix ) kafkaController . startup () // ... KafkaController.startup() \u65b9\u6cd5\u4f1a\u5411 zookeeper \u6ce8\u518c StateChangeHandler\uff0c\u5e76\u542f\u52a8 eventManager /** * Invoked when the controller module of a Kafka server is started up. This does not assume that the current broker * is the controller. It merely registers the session expiration listener and starts the controller leader * elector */ def startup () = { zkClient . registerStateChangeHandler ( new StateChangeHandler { override val name : String = StateChangeHandlers . ControllerHandler override def afterInitializingSession (): Unit = { eventManager . put ( RegisterBrokerAndReelect ) } override def beforeInitializingSession (): Unit = { val queuedEvent = eventManager . clearAndPut ( Expire ) // Block initialization of the new session until the expiration event is being handled, // which ensures that all pending events have been processed before creating the new session queuedEvent . awaitProcessing () } }) eventManager . put ( Startup ) eventManager . start () } eventManager \u662f ControllerEventManager \u7684\u5b9e\u4f8b\uff0c\u5176\u7ef4\u62a4\u4e86 QueuedEvent (\u5bf9 ControllerEvent \u7684\u5305\u88c5) \u7684\u961f\u5217\uff0c\u5e76\u542f\u52a8\u7ebf\u7a0b\uff1a \u4ece queue \u4e2d\u53d6\u4e8b\u4ef6 QueuedEvent \u901a\u8fc7 processor \u5bf9\u4e8b\u4ef6\u8fdb\u884c\u5904\u7406 \u8fd9\u91cc\u7684 processor \u662f KafkaController \uff0c\u5176\u5b9e\u73b0\u4e86 ControllerEventProcessor \u5b9a\u4e49\u7684\u65b9\u6cd5 process() \u65b9\u6cd5 \u5728\u542f\u52a8 eventManager \u4e4b\u524d\uff0c\u5411 eventManager \u589e\u52a0\u4e86 Startup \u4e8b\u4ef6\uff0c\u53ef\u4ee5\u5728 KafkaController.process() \u65b9\u6cd5\u4e2d\u770b\u5230\u5bf9\u6240\u6709\u4e8b\u4ef6\u7684\u5904\u7406 override def process ( event : ControllerEvent ): Unit = { try { event match { case event : MockEvent => // Used only in test cases event . process () case ShutdownEventThread => error ( \"Received a ShutdownEventThread event. This type of event is supposed to be handle by ControllerEventThread\" ) // ... case Startup => processStartup () } } catch { case e : ControllerMovedException => info ( s\"Controller moved to another broker when processing $ event .\" , e ) maybeResign () case e : Throwable => error ( s\"Error processing event $ event \" , e ) } finally { updateMetrics () } } \u53ef\u4ee5\u770b\u5230\uff0c\u5bf9\u4e8e Startup \u4e8b\u4ef6\uff0c\u8fd9\u91cc\u5b9e\u9645\u901a\u8fc7 processStartup() \u8fdb\u884c\u5904\u7406\u3002 private def processStartup (): Unit = { zkClient . registerZNodeChangeHandlerAndCheckExistence ( controllerChangeHandler ) elect () } \u8fd9\u91cc\u9996\u5148\u5411 zookeeper \u6ce8\u518c controllerChangeHandler \u7528\u6765\u76d1\u542c /controller \uff0c\u8d1f\u8d23 controller \u81ea\u8eab\u7684\u9009\u4e3e\u3001\u7ba1\u7406\u64cd\u4f5c\uff1a class ControllerChangeHandler ( eventManager : ControllerEventManager ) extends ZNodeChangeHandler { override val path : String = ControllerZNode . path override def handleCreation (): Unit = eventManager . put ( ControllerChange ) override def handleDeletion (): Unit = eventManager . put ( Reelect ) override def handleDataChange (): Unit = eventManager . put ( ControllerChange ) } /controller zookeeper \u8282\u70b9\u521b\u5efa\u65f6\u5411 eventManager \u7684 queue \u589e\u52a0 ControllerChange \u4e8b\u4ef6 /controller zookeeper \u8282\u70b9\u5220\u9664\u65f6\u5411 eventManager \u7684 queue \u589e\u52a0 Reelect \u4e8b\u4ef6 /controller zookeeper \u8282\u70b9\u5185\u5bb9\u6539\u53d8\u65f6\u5411 eventManager \u7684 queue \u589e\u52a0 ControllerChange \u4e8b\u4ef6 \u7136\u540e\uff0c\u6267\u884c elect() \uff0c\u5c1d\u8bd5\u6210\u4e3a\u5f53\u524d\u96c6\u7fa4\u7684 controller private def elect (): Unit = { activeControllerId = zkClient . getControllerId . getOrElse ( - 1 ) /* * We can get here during the initial startup and the handleDeleted ZK callback. Because of the potential race condition, * it's possible that the controller has already been elected when we get here. This check will prevent the following * createEphemeralPath method from getting into an infinite loop if this broker is already the controller. */ if ( activeControllerId != - 1 ) { debug ( s\"Broker $ activeControllerId has been elected as the controller, so stopping the election process.\" ) return } try { val ( epoch , epochZkVersion ) = zkClient . registerControllerAndIncrementControllerEpoch ( config . brokerId ) controllerContext . epoch = epoch controllerContext . epochZkVersion = epochZkVersion activeControllerId = config . brokerId info ( s\" ${ config . brokerId } successfully elected as the controller. Epoch incremented to ${ controllerContext . epoch } \" + s\"and epoch zk version is now ${ controllerContext . epochZkVersion } \" ) onControllerFailover () } catch { case e : ControllerMovedException => maybeResign () if ( activeControllerId != - 1 ) debug ( s\"Broker $ activeControllerId was elected as controller instead of broker ${ config . brokerId } \" , e ) else warn ( \"A controller has been elected but just resigned, this will result in another round of election\" , e ) case t : Throwable => error ( s\"Error while electing or becoming controller on broker ${ config . brokerId } . \" + s\"Trigger controller movement immediately\" , t ) triggerControllerMove () } } \u9996\u5148\u83b7\u53d6\u5f53\u524d\u662f\u5426\u6709 /controller \uff0c\u5982\u679c\u6709\uff0c\u5373 activeControllerId != -1 \uff0c\u5219\u8fd4\u56de\uff0c\u5426\u5219\u7ee7\u7eed \u5c1d\u8bd5\u4ee5\u81ea\u8eab brokerId \u521b\u5efa /controller \uff0c\u5982\u679c\u521b\u5efa\u6210\u529f\uff0c\u5219\u5f53\u524d broker \u4e3a\u96c6\u7fa4 controller\uff0c\u6267\u884c onControllerFailover() \u65b9\u6cd5\uff0c\u5982\u4e0b\uff1a \u521d\u59cb\u5316 controller context \u7f13\u5b58\u6240\u6709 topics\uff0clive brokers\uff0cleaders for all existing partitions \u5f00\u542f controller channel manager \u5f00\u542f replica state machine \u5f00\u542f partition state machine /** * This callback is invoked by the zookeeper leader elector on electing the current broker as the new controller. * It does the following things on the become-controller state change - * 1. Initializes the controller's context object that holds cache objects for current topics, live brokers and * leaders for all existing partitions. * 2. Starts the controller's channel manager * 3. Starts the replica state machine * 4. Starts the partition state machine * If it encounters any unexpected exception/error while becoming controller, it resigns as the current controller. * This ensures another controller election will be triggered and there will always be an actively serving controller */ private def onControllerFailover (): Unit = { maybeSetupFeatureVersioning () info ( \"Registering handlers\" ) // before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks val childChangeHandlers = Seq ( brokerChangeHandler , topicChangeHandler , topicDeletionHandler , logDirEventNotificationHandler , isrChangeNotificationHandler ) childChangeHandlers . foreach ( zkClient . registerZNodeChildChangeHandler ) val nodeChangeHandlers = Seq ( preferredReplicaElectionHandler , partitionReassignmentHandler ) nodeChangeHandlers . foreach ( zkClient . registerZNodeChangeHandlerAndCheckExistence ) info ( \"Deleting log dir event notifications\" ) zkClient . deleteLogDirEventNotifications ( controllerContext . epochZkVersion ) info ( \"Deleting isr change notifications\" ) zkClient . deleteIsrChangeNotifications ( controllerContext . epochZkVersion ) info ( \"Initializing controller context\" ) initializeControllerContext () info ( \"Fetching topic deletions in progress\" ) val ( topicsToBeDeleted , topicsIneligibleForDeletion ) = fetchTopicDeletionsInProgress () info ( \"Initializing topic deletion manager\" ) topicDeletionManager . init ( topicsToBeDeleted , topicsIneligibleForDeletion ) // We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines // are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before // they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and // partitionStateMachine.startup(). info ( \"Sending update metadata request\" ) sendUpdateMetadataRequest ( controllerContext . liveOrShuttingDownBrokerIds . toSeq , Set . empty ) replicaStateMachine . startup () partitionStateMachine . startup () info ( s\"Ready to serve as the new controller with epoch $ epoch \" ) initializePartitionReassignments () topicDeletionManager . tryTopicDeletion () val pendingPreferredReplicaElections = fetchPendingPreferredReplicaElections () onReplicaElection ( pendingPreferredReplicaElections , ElectionType . PREFERRED , ZkTriggered ) info ( \"Starting the controller scheduler\" ) kafkaScheduler . startup () if ( config . autoLeaderRebalanceEnable ) { scheduleAutoLeaderRebalanceTask ( delay = 5 , unit = TimeUnit . SECONDS ) } if ( config . tokenAuthEnabled ) { info ( \"starting the token expiry check scheduler\" ) tokenCleanScheduler . startup () tokenCleanScheduler . schedule ( name = \"delete-expired-tokens\" , fun = () => tokenManager . expireTokens (), period = config . delegationTokenExpiryCheckIntervalMs , unit = TimeUnit . MILLISECONDS ) } } \u6ce8\u518c\u4ee5\u4e0b handler\uff0c\u76d1\u542c zookeeper\uff0c\u505a\u5bf9\u5e94\u5904\u7406\uff1a brokerChangeHandler \uff0c\u76d1\u542c /brokers/ids \uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 BrokerChange \u4e8b\u4ef6 topicChangeHandler \uff0c\u76d1\u542c /brokers/topics \uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 TopicChange \u4e8b\u4ef6 topicDeletionHandler \uff0c\u76d1\u542c /admin/delete_topics \uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 TopicDeletion \u4e8b\u4ef6 logDirEventNotificationHandler \uff0c\u76d1\u542c /log_dir_event_notification \uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 LogDirEventNotification \u4e8b\u4ef6 isrChangeNotificationHandler \uff0c\u76d1\u542c /isr_change_notification \uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 IsrChangeNotification \u4e8b\u4ef6 preferredReplicaElectionHandler \uff0c\u76d1\u542c /admin/preferred_replica_election \uff0c\u5982\u679c\u65b0\u5efa\uff0c\u5411 eventManager \u589e\u52a0 ReplicaLeaderElection \u4e8b\u4ef6 partitionReassignmentHandler \uff0c\u76d1\u542c /admin/reassign_partitions \uff0c\u5982\u679c\u65b0\u5efa\uff0c\u5411 eventManager \u589e\u52a0 ZkPartitionReassignment \u4e8b\u4ef6 \u521d\u59cb\u5316 controllerContext \u83b7\u53d6 /admin/delete_topics/ \u8282\u70b9\uff0c\u67e5\u770b\u662f\u5426\u6709\u5f85\u5220\u9664\u7684 topic\uff0c\u521d\u59cb TopicDeletionManager \u5411\u6240\u6709 broker \u53d1\u9001 UpdateMetadataRequest \u5f00\u542f replicaStateMachine \u5f00\u542f partitionStateMachine \u5c1d\u8bd5\u8fdb\u884c\u672a\u5b8c\u6210\u7684 partitionReassignments \u5c1d\u8bd5\u8fdb\u884c\u672a\u5b8c\u6210\u7684 preferredReplicaElections \u542f\u52a8 kafkaScheduler \u5982\u679c\u914d\u7f6e\u4e86 auto leader rebalance\uff0c\u5219\u5468\u671f\u8fdb\u884c leader rebalance \u5982\u679c\u5f00\u542f token auth\uff0c\u5219\u5468\u671f\u5220\u9664\u8fc7\u671f token \u81f3\u6b64\uff0ccontroller \u5df2\u7ecf\u5b8c\u6210\u542f\u52a8","title":"1.3.1 KafkaController \u542f\u52a8"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/1-controller%E5%90%AF%E5%8A%A8/#131-controller","text":"controller \u5728 kafkaServer.startup() \u4e2d\u521d\u59cb\u5316\u5e76\u542f\u52a8\uff1a /** * Start up API for bringing up a single instance of the Kafka server. * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers */ def startup (): Unit = { try { info ( \"starting\" ) if ( isShuttingDown . get ) throw new IllegalStateException ( \"Kafka server is still shutting down, cannot re-start!\" ) if ( startupComplete . get ) return val canStartup = isStartingUp . compareAndSet ( false , true ) if ( canStartup ) { // ... /* start kafka controller */ kafkaController = new KafkaController ( config , zkClient , time , metrics , brokerInfo , brokerEpoch , tokenManager , brokerFeatures , featureCache , threadNamePrefix ) kafkaController . startup () // ... KafkaController.startup() \u65b9\u6cd5\u4f1a\u5411 zookeeper \u6ce8\u518c StateChangeHandler\uff0c\u5e76\u542f\u52a8 eventManager /** * Invoked when the controller module of a Kafka server is started up. This does not assume that the current broker * is the controller. It merely registers the session expiration listener and starts the controller leader * elector */ def startup () = { zkClient . registerStateChangeHandler ( new StateChangeHandler { override val name : String = StateChangeHandlers . ControllerHandler override def afterInitializingSession (): Unit = { eventManager . put ( RegisterBrokerAndReelect ) } override def beforeInitializingSession (): Unit = { val queuedEvent = eventManager . clearAndPut ( Expire ) // Block initialization of the new session until the expiration event is being handled, // which ensures that all pending events have been processed before creating the new session queuedEvent . awaitProcessing () } }) eventManager . put ( Startup ) eventManager . start () } eventManager \u662f ControllerEventManager \u7684\u5b9e\u4f8b\uff0c\u5176\u7ef4\u62a4\u4e86 QueuedEvent (\u5bf9 ControllerEvent \u7684\u5305\u88c5) \u7684\u961f\u5217\uff0c\u5e76\u542f\u52a8\u7ebf\u7a0b\uff1a \u4ece queue \u4e2d\u53d6\u4e8b\u4ef6 QueuedEvent \u901a\u8fc7 processor \u5bf9\u4e8b\u4ef6\u8fdb\u884c\u5904\u7406 \u8fd9\u91cc\u7684 processor \u662f KafkaController \uff0c\u5176\u5b9e\u73b0\u4e86 ControllerEventProcessor \u5b9a\u4e49\u7684\u65b9\u6cd5 process() \u65b9\u6cd5 \u5728\u542f\u52a8 eventManager \u4e4b\u524d\uff0c\u5411 eventManager \u589e\u52a0\u4e86 Startup \u4e8b\u4ef6\uff0c\u53ef\u4ee5\u5728 KafkaController.process() \u65b9\u6cd5\u4e2d\u770b\u5230\u5bf9\u6240\u6709\u4e8b\u4ef6\u7684\u5904\u7406 override def process ( event : ControllerEvent ): Unit = { try { event match { case event : MockEvent => // Used only in test cases event . process () case ShutdownEventThread => error ( \"Received a ShutdownEventThread event. This type of event is supposed to be handle by ControllerEventThread\" ) // ... case Startup => processStartup () } } catch { case e : ControllerMovedException => info ( s\"Controller moved to another broker when processing $ event .\" , e ) maybeResign () case e : Throwable => error ( s\"Error processing event $ event \" , e ) } finally { updateMetrics () } } \u53ef\u4ee5\u770b\u5230\uff0c\u5bf9\u4e8e Startup \u4e8b\u4ef6\uff0c\u8fd9\u91cc\u5b9e\u9645\u901a\u8fc7 processStartup() \u8fdb\u884c\u5904\u7406\u3002 private def processStartup (): Unit = { zkClient . registerZNodeChangeHandlerAndCheckExistence ( controllerChangeHandler ) elect () } \u8fd9\u91cc\u9996\u5148\u5411 zookeeper \u6ce8\u518c controllerChangeHandler \u7528\u6765\u76d1\u542c /controller \uff0c\u8d1f\u8d23 controller \u81ea\u8eab\u7684\u9009\u4e3e\u3001\u7ba1\u7406\u64cd\u4f5c\uff1a class ControllerChangeHandler ( eventManager : ControllerEventManager ) extends ZNodeChangeHandler { override val path : String = ControllerZNode . path override def handleCreation (): Unit = eventManager . put ( ControllerChange ) override def handleDeletion (): Unit = eventManager . put ( Reelect ) override def handleDataChange (): Unit = eventManager . put ( ControllerChange ) } /controller zookeeper \u8282\u70b9\u521b\u5efa\u65f6\u5411 eventManager \u7684 queue \u589e\u52a0 ControllerChange \u4e8b\u4ef6 /controller zookeeper \u8282\u70b9\u5220\u9664\u65f6\u5411 eventManager \u7684 queue \u589e\u52a0 Reelect \u4e8b\u4ef6 /controller zookeeper \u8282\u70b9\u5185\u5bb9\u6539\u53d8\u65f6\u5411 eventManager \u7684 queue \u589e\u52a0 ControllerChange \u4e8b\u4ef6 \u7136\u540e\uff0c\u6267\u884c elect() \uff0c\u5c1d\u8bd5\u6210\u4e3a\u5f53\u524d\u96c6\u7fa4\u7684 controller private def elect (): Unit = { activeControllerId = zkClient . getControllerId . getOrElse ( - 1 ) /* * We can get here during the initial startup and the handleDeleted ZK callback. Because of the potential race condition, * it's possible that the controller has already been elected when we get here. This check will prevent the following * createEphemeralPath method from getting into an infinite loop if this broker is already the controller. */ if ( activeControllerId != - 1 ) { debug ( s\"Broker $ activeControllerId has been elected as the controller, so stopping the election process.\" ) return } try { val ( epoch , epochZkVersion ) = zkClient . registerControllerAndIncrementControllerEpoch ( config . brokerId ) controllerContext . epoch = epoch controllerContext . epochZkVersion = epochZkVersion activeControllerId = config . brokerId info ( s\" ${ config . brokerId } successfully elected as the controller. Epoch incremented to ${ controllerContext . epoch } \" + s\"and epoch zk version is now ${ controllerContext . epochZkVersion } \" ) onControllerFailover () } catch { case e : ControllerMovedException => maybeResign () if ( activeControllerId != - 1 ) debug ( s\"Broker $ activeControllerId was elected as controller instead of broker ${ config . brokerId } \" , e ) else warn ( \"A controller has been elected but just resigned, this will result in another round of election\" , e ) case t : Throwable => error ( s\"Error while electing or becoming controller on broker ${ config . brokerId } . \" + s\"Trigger controller movement immediately\" , t ) triggerControllerMove () } } \u9996\u5148\u83b7\u53d6\u5f53\u524d\u662f\u5426\u6709 /controller \uff0c\u5982\u679c\u6709\uff0c\u5373 activeControllerId != -1 \uff0c\u5219\u8fd4\u56de\uff0c\u5426\u5219\u7ee7\u7eed \u5c1d\u8bd5\u4ee5\u81ea\u8eab brokerId \u521b\u5efa /controller \uff0c\u5982\u679c\u521b\u5efa\u6210\u529f\uff0c\u5219\u5f53\u524d broker \u4e3a\u96c6\u7fa4 controller\uff0c\u6267\u884c onControllerFailover() \u65b9\u6cd5\uff0c\u5982\u4e0b\uff1a \u521d\u59cb\u5316 controller context \u7f13\u5b58\u6240\u6709 topics\uff0clive brokers\uff0cleaders for all existing partitions \u5f00\u542f controller channel manager \u5f00\u542f replica state machine \u5f00\u542f partition state machine /** * This callback is invoked by the zookeeper leader elector on electing the current broker as the new controller. * It does the following things on the become-controller state change - * 1. Initializes the controller's context object that holds cache objects for current topics, live brokers and * leaders for all existing partitions. * 2. Starts the controller's channel manager * 3. Starts the replica state machine * 4. Starts the partition state machine * If it encounters any unexpected exception/error while becoming controller, it resigns as the current controller. * This ensures another controller election will be triggered and there will always be an actively serving controller */ private def onControllerFailover (): Unit = { maybeSetupFeatureVersioning () info ( \"Registering handlers\" ) // before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks val childChangeHandlers = Seq ( brokerChangeHandler , topicChangeHandler , topicDeletionHandler , logDirEventNotificationHandler , isrChangeNotificationHandler ) childChangeHandlers . foreach ( zkClient . registerZNodeChildChangeHandler ) val nodeChangeHandlers = Seq ( preferredReplicaElectionHandler , partitionReassignmentHandler ) nodeChangeHandlers . foreach ( zkClient . registerZNodeChangeHandlerAndCheckExistence ) info ( \"Deleting log dir event notifications\" ) zkClient . deleteLogDirEventNotifications ( controllerContext . epochZkVersion ) info ( \"Deleting isr change notifications\" ) zkClient . deleteIsrChangeNotifications ( controllerContext . epochZkVersion ) info ( \"Initializing controller context\" ) initializeControllerContext () info ( \"Fetching topic deletions in progress\" ) val ( topicsToBeDeleted , topicsIneligibleForDeletion ) = fetchTopicDeletionsInProgress () info ( \"Initializing topic deletion manager\" ) topicDeletionManager . init ( topicsToBeDeleted , topicsIneligibleForDeletion ) // We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines // are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before // they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and // partitionStateMachine.startup(). info ( \"Sending update metadata request\" ) sendUpdateMetadataRequest ( controllerContext . liveOrShuttingDownBrokerIds . toSeq , Set . empty ) replicaStateMachine . startup () partitionStateMachine . startup () info ( s\"Ready to serve as the new controller with epoch $ epoch \" ) initializePartitionReassignments () topicDeletionManager . tryTopicDeletion () val pendingPreferredReplicaElections = fetchPendingPreferredReplicaElections () onReplicaElection ( pendingPreferredReplicaElections , ElectionType . PREFERRED , ZkTriggered ) info ( \"Starting the controller scheduler\" ) kafkaScheduler . startup () if ( config . autoLeaderRebalanceEnable ) { scheduleAutoLeaderRebalanceTask ( delay = 5 , unit = TimeUnit . SECONDS ) } if ( config . tokenAuthEnabled ) { info ( \"starting the token expiry check scheduler\" ) tokenCleanScheduler . startup () tokenCleanScheduler . schedule ( name = \"delete-expired-tokens\" , fun = () => tokenManager . expireTokens (), period = config . delegationTokenExpiryCheckIntervalMs , unit = TimeUnit . MILLISECONDS ) } } \u6ce8\u518c\u4ee5\u4e0b handler\uff0c\u76d1\u542c zookeeper\uff0c\u505a\u5bf9\u5e94\u5904\u7406\uff1a brokerChangeHandler \uff0c\u76d1\u542c /brokers/ids \uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 BrokerChange \u4e8b\u4ef6 topicChangeHandler \uff0c\u76d1\u542c /brokers/topics \uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 TopicChange \u4e8b\u4ef6 topicDeletionHandler \uff0c\u76d1\u542c /admin/delete_topics \uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 TopicDeletion \u4e8b\u4ef6 logDirEventNotificationHandler \uff0c\u76d1\u542c /log_dir_event_notification \uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 LogDirEventNotification \u4e8b\u4ef6 isrChangeNotificationHandler \uff0c\u76d1\u542c /isr_change_notification \uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 IsrChangeNotification \u4e8b\u4ef6 preferredReplicaElectionHandler \uff0c\u76d1\u542c /admin/preferred_replica_election \uff0c\u5982\u679c\u65b0\u5efa\uff0c\u5411 eventManager \u589e\u52a0 ReplicaLeaderElection \u4e8b\u4ef6 partitionReassignmentHandler \uff0c\u76d1\u542c /admin/reassign_partitions \uff0c\u5982\u679c\u65b0\u5efa\uff0c\u5411 eventManager \u589e\u52a0 ZkPartitionReassignment \u4e8b\u4ef6 \u521d\u59cb\u5316 controllerContext \u83b7\u53d6 /admin/delete_topics/ \u8282\u70b9\uff0c\u67e5\u770b\u662f\u5426\u6709\u5f85\u5220\u9664\u7684 topic\uff0c\u521d\u59cb TopicDeletionManager \u5411\u6240\u6709 broker \u53d1\u9001 UpdateMetadataRequest \u5f00\u542f replicaStateMachine \u5f00\u542f partitionStateMachine \u5c1d\u8bd5\u8fdb\u884c\u672a\u5b8c\u6210\u7684 partitionReassignments \u5c1d\u8bd5\u8fdb\u884c\u672a\u5b8c\u6210\u7684 preferredReplicaElections \u542f\u52a8 kafkaScheduler \u5982\u679c\u914d\u7f6e\u4e86 auto leader rebalance\uff0c\u5219\u5468\u671f\u8fdb\u884c leader rebalance \u5982\u679c\u5f00\u542f token auth\uff0c\u5219\u5468\u671f\u5220\u9664\u8fc7\u671f token \u81f3\u6b64\uff0ccontroller \u5df2\u7ecf\u5b8c\u6210\u542f\u52a8","title":"1.3.1 controller \u542f\u52a8"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/3-reassign-partitions/","text":"1.3.3 controller \u5728 controller \u542f\u52a8\u8fc7\u7a0b\u4e2d\uff0c\u5728 KafkaController.onControllerFailover() \u65b9\u6cd5\u4e2d\u5411 zkClient \u6ce8\u518c\u4e86 PartitionReassignmentHandler PartitionReassignmentHandler \u5982\u4e0b\uff1a class PartitionReassignmentHandler ( eventManager : ControllerEventManager ) extends ZNodeChangeHandler { override val path : String = ReassignPartitionsZNode . path // Note that the event is also enqueued when the znode is deleted, but we do it explicitly instead of relying on // handleDeletion(). This approach is more robust as it doesn't depend on the watcher being re-registered after // it's consumed during data changes (we ensure re-registration when the znode is deleted). override def handleCreation (): Unit = eventManager . put ( ZkPartitionReassignment ) } \u8fd9\u91cc\u5b9e\u9645\u4e0a\u662f\u76d1\u542c zookeeper /admin/reassign_partitions \u8282\u70b9\uff0c\u5982\u679c\u6709\u65b0\u8282\u70b9\u521b\u5efa\uff0c\u5219\u5411 eventManager \u4e8b\u4ef6\u961f\u5217 queue \u589e\u52a0 ZkPartitionReassignment eventManager \u4f1a\u542f\u52a8\u5355\u72ec\u7684\u7ebf\u7a0b\u4e0d\u65ad\u4ece\u4e8b\u4ef6\u961f\u5217\u4e2d\u53d6\u51fa\u4e8b\u4ef6\uff0c\u901a\u8fc7 processor \u8fdb\u884c\u5904\u7406 eventManager \u7684 processor \u662f KafkaController \u7684\u5b9e\u4f8b\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728 KafkaController.process() \u65b9\u6cd5\u91cc\u627e\u5230\u5bf9 ZkPartitionReassignment \u7684\u5904\u7406 override def process ( event : ControllerEvent ): Unit = { try { event match { case event : MockEvent => // Used only in test cases event . process () // ... case ApiPartitionReassignment ( reassignments , callback ) => processApiPartitionReassignment ( reassignments , callback ) case ZkPartitionReassignment => processZkPartitionReassignment () case ListPartitionReassignments ( partitions , callback ) => processListPartitionReassignments ( partitions , callback ) case PartitionReassignmentIsrChange ( partition ) => processPartitionReassignmentIsrChange ( partition ) // ... } } catch { case e : ControllerMovedException => info ( s\"Controller moved to another broker when processing $ event .\" , e ) maybeResign () case e : Throwable => error ( s\"Error processing event $ event \" , e ) } finally { updateMetrics () } } \u5bf9 ZkPartitionReassignment \u901a\u8fc7 processZkPartitionReassignment() \u8fdb\u884c\u5904\u7406\uff1a private def processZkPartitionReassignment (): Set [ TopicPartition ] = { // We need to register the watcher if the path doesn't exist in order to detect future // reassignments and we get the `path exists` check for free if ( isActive && zkClient . registerZNodeChangeHandlerAndCheckExistence ( partitionReassignmentHandler )) { val reassignmentResults = mutable . Map . empty [ TopicPartition , ApiError ] val partitionsToReassign = mutable . Map . empty [ TopicPartition , ReplicaAssignment ] // \u8fd9\u91cc\u7ec4\u5408\u4e86\u73b0\u5728\u7684 replica \u548c targetReplica zkClient . getPartitionReassignment . forKeyValue { ( tp , targetReplicas ) => maybeBuildReassignment ( tp , Some ( targetReplicas )) match { case Some ( context ) => partitionsToReassign . put ( tp , context ) case None => reassignmentResults . put ( tp , new ApiError ( Errors . NO_REASSIGNMENT_IN_PROGRESS )) } } // \u89e6\u53d1 partition reassignment reassignmentResults ++= maybeTriggerPartitionReassignment ( partitionsToReassign ) val ( partitionsReassigned , partitionsFailed ) = reassignmentResults . partition ( _ . _2 . error == Errors . NONE ) if ( partitionsFailed . nonEmpty ) { warn ( s\"Failed reassignment through zk with the following errors: $ partitionsFailed \" ) maybeRemoveFromZkReassignment (( tp , _ ) => partitionsFailed . contains ( tp )) } partitionsReassigned . keySet } else { Set . empty } } /** * Trigger a partition reassignment provided that the topic exists and is not being deleted. * * This is called when a reassignment is initially received either through Zookeeper or through the * AlterPartitionReassignments API * * The `partitionsBeingReassigned` field in the controller context will be updated by this * call after the reassignment completes validation and is successfully stored in the topic * assignment zNode. * * @param reassignments The reassignments to begin processing * @return A map of any errors in the reassignment. If the error is NONE for a given partition, * then the reassignment was submitted successfully. */ private def maybeTriggerPartitionReassignment ( reassignments : Map [ TopicPartition , ReplicaAssignment ]): Map [ TopicPartition , ApiError ] = { reassignments . map { case ( tp , reassignment ) => val topic = tp . topic val apiError = if ( topicDeletionManager . isTopicQueuedUpForDeletion ( topic )) { info ( s\"Skipping reassignment of $ tp since the topic is currently being deleted\" ) new ApiError ( Errors . UNKNOWN_TOPIC_OR_PARTITION , \"The partition does not exist.\" ) } else { val assignedReplicas = controllerContext . partitionReplicaAssignment ( tp ) if ( assignedReplicas . nonEmpty ) { try { onPartitionReassignment ( tp , reassignment ) ApiError . NONE } catch { case e : ControllerMovedException => info ( s\"Failed completing reassignment of partition $ tp because controller has moved to another broker\" ) throw e case e : Throwable => error ( s\"Error completing reassignment of partition $ tp \" , e ) new ApiError ( Errors . UNKNOWN_SERVER_ERROR ) } } else { new ApiError ( Errors . UNKNOWN_TOPIC_OR_PARTITION , \"The partition does not exist.\" ) } } tp -> apiError } } \u5b9a\u4e49\u4ee5\u4e0b\u6982\u5ff5\uff1a - RS: \u5f53\u524d\u526f\u672c\u96c6\u5408 - ORS: \u539f\u59cb\u526f\u672c\u96c6\u5408 - TRS: \u76ee\u6807\u526f\u672c\u96c6\u5408 - AR: \u5728\u8fd9\u6b21\u91cd\u5206\u914d\u4e2d\u8981\u589e\u52a0\u7684\u526f\u672c - RR: \u5728\u8fd9\u6b21\u91cd\u5206\u914d\u4e2d\u8981\u79fb\u9664\u7684\u526f\u672c \u5047\u8bbe\u4e00\u4e2a topic \u5f53\u524d\u526f\u672c\u96c6\u4e3a 1,2,3 \uff0c\u5219\u5176 RS \u548c ORS \u4e3a 1,2,3 \u5982\u679c\u8981\u5c06\u5176\u91cd\u5206\u914d\u4e3a 4,5,6 \uff0c\u5219\u5176 TRS \u4e3a 4,5,6 , AR \u4e3a 4,5,6 , RR \u4e3a 1,2,3 RS AR RR leader isr step 1,2,3 1 1,2,3 initial state 4,5,6,1,2,3 4,5,6 1,2,3 1 1,2,3 step A2 4,5,6,1,2,3 4,5,6 1,2,3 1 1,2,3,4,5,6 phase B 4,5,6,1,2,3 4,5,6 1,2,3 4 1,2,3,4,5,6 step B3 4,5,6,1,2,3 4,5,6 1,2,3 4 4,5,6 step B4 4,5,6 4 4,5,6 step B6 \u5c06\u6574\u4e2a\u91cd\u5206\u914d\u8fc7\u7a0b\u5206\u4e3a 3 \u4e2a\u9636\u6bb5\uff0c\u6bcf\u4e2a\u9636\u6bb5\u6709\u81ea\u5df1\u7684\u6b65\u9aa4 1. Phase U (Assignment update) 1. U1: \u66f4\u65b0 Zk \u4f7f RS = ORS + TRS, AR = TRS - ORS, RR = ORS - TRS 2. U2: \u66f4\u65b0 memory \u4f7f RS = ORS + TRS, AR = TRS - ORS, RR = ORS - TRS 3. U3: \u5982\u679c\u6b63\u5728\u53d6\u6d88\u5f53\u524d\u7684\u91cd\u5206\u914d\u6216\u8005\u66ff\u6362\u5f53\u524d\u7684\u91cd\u5206\u914d\uff0c\u5411\u6240\u6709\u4e0d\u5728\u65b0\u5206\u914d\u7684 TRS \u7684 AR \u53d1\u9001 StopReplica \u8bf7\u6c42 2. Phase A (when TRS != ISR) 1. A1: bump the leader epoch for the partition and send LeaderAndIsr updates to RS 2. A2: \u901a\u8fc7\u5c06 AR \u4e2d\u7684 replicas \u7f6e\u4e3a NewReplica \u72b6\u6001\u542f\u52a8\u65b0\u7684 replicas 3. Phase B (when TRS == ISR) 1. B1: \u5c06\u6240\u6709 AR \u4e2d\u7684 replicas \u7f6e\u4e3a OnlineReplica \u72b6\u6001 2. B2: \u66f4\u65b0 memory \u4f7f RS = TRS, AR = [], RR = [] 3. B3: \u53d1\u9001\u8868\u793a RS = TRS \u7684 LeaderAndIsr \u8bf7\u6c42\u3002This will prevent the leader from adding any replica in TRS - ORS back in the isr. \u5982\u679c\u5f53\u524d leader \u4e0d\u5728 TRS \u91cc\u6216\u8005\u4e0d\u662f\u5b58\u6d3b\u7684\uff0cwe move the leader to a new replica in TRS. We may send the LeaderAndIsr to more than the TRS replicas due to the way the partition state machine works (it reads replicas from ZK) 4. B4: \u5c06\u6240\u6709 RR \u4e2d\u7684 replicas \u7f6e\u4e3a OfflineReplica \u72b6\u6001\u3002As part of OfflineReplica state change, we shrink the isr to remove RR in ZooKeeper and send a LeaderAndIsr ONLY to the Leader to notify it of the shrunk isr. After that, we send a StopReplica (delete = false) to the replicas in RR. 5. B5: \u5c06\u6240\u6709 RR \u4e2d\u7684 replicas \u7f6e\u4e3a NonExistentReplica \u72b6\u6001\u3002This will send a StopReplica (delete = true) to the replicas in RR to physically delete the replicas on disk. 6. B6: \u66f4\u65b0 Zk \u4f7f RS = TRS, AR = [], RR = [] 7. B7: \u79fb\u9664 ISR reassign listener\uff0c\u66f4\u65b0 Zk \u7684 /admin/reassign_partitions \u79fb\u9664\u8fd9\u4e2a partition 8. B8: \u7ecf\u8fc7 leader \u9009\u4e3e\uff0creplicas \u548c isr \u4fe1\u606f\u6709\u53d8\u66f4\uff0c\u5411\u6240\u6709 broker \u53d1 metadata request /** * This callback is invoked: * 1. By the AlterPartitionReassignments API * 2. By the reassigned partitions listener which is triggered when the /admin/reassign/partitions znode is created * 3. When an ongoing reassignment finishes - this is detected by a change in the partition's ISR znode * 4. Whenever a new broker comes up which is part of an ongoing reassignment * 5. On controller startup/failover * * Reassigning replicas for a partition goes through a few steps listed in the code. * RS = current assigned replica set * ORS = Original replica set for partition * TRS = Reassigned (target) replica set * AR = The replicas we are adding as part of this reassignment * RR = The replicas we are removing as part of this reassignment * * A reassignment may have up to three phases, each with its own steps: * Phase U (Assignment update): Regardless of the trigger, the first step is in the reassignment process * is to update the existing assignment state. We always update the state in Zookeeper before * we update memory so that it can be resumed upon controller fail-over. * * U1. Update ZK with RS = ORS + TRS, AR = TRS - ORS, RR = ORS - TRS. * U2. Update memory with RS = ORS + TRS, AR = TRS - ORS and RR = ORS - TRS * U3. If we are cancelling or replacing an existing reassignment, send StopReplica to all members * of AR in the original reassignment if they are not in TRS from the new assignment * * To complete the reassignment, we need to bring the new replicas into sync, so depending on the state * of the ISR, we will execute one of the following steps. * * Phase A (when TRS != ISR): The reassignment is not yet complete * * A1. Bump the leader epoch for the partition and send LeaderAndIsr updates to RS. * A2. Start new replicas AR by moving replicas in AR to NewReplica state. * * Phase B (when TRS = ISR): The reassignment is complete * * B1. Move all replicas in AR to OnlineReplica state. * B2. Set RS = TRS, AR = [], RR = [] in memory. * B3. Send a LeaderAndIsr request with RS = TRS. This will prevent the leader from adding any replica in TRS - ORS back in the isr. * If the current leader is not in TRS or isn't alive, we move the leader to a new replica in TRS. * We may send the LeaderAndIsr to more than the TRS replicas due to the * way the partition state machine works (it reads replicas from ZK) * B4. Move all replicas in RR to OfflineReplica state. As part of OfflineReplica state change, we shrink the * isr to remove RR in ZooKeeper and send a LeaderAndIsr ONLY to the Leader to notify it of the shrunk isr. * After that, we send a StopReplica (delete = false) to the replicas in RR. * B5. Move all replicas in RR to NonExistentReplica state. This will send a StopReplica (delete = true) to * the replicas in RR to physically delete the replicas on disk. * B6. Update ZK with RS=TRS, AR=[], RR=[]. * B7. Remove the ISR reassign listener and maybe update the /admin/reassign_partitions path in ZK to remove this partition from it if present. * B8. After electing leader, the replicas and isr information changes. So resend the update metadata request to every broker. * * In general, there are two goals we want to aim for: * 1. Every replica present in the replica set of a LeaderAndIsrRequest gets the request sent to it * 2. Replicas that are removed from a partition's assignment get StopReplica sent to them * * For example, if ORS = {1,2,3} and TRS = {4,5,6}, the values in the topic and leader/isr paths in ZK * may go through the following transitions. * RS AR RR leader isr * {1,2,3} {} {} 1 {1,2,3} (initial state) * {4,5,6,1,2,3} {4,5,6} {1,2,3} 1 {1,2,3} (step A2) * {4,5,6,1,2,3} {4,5,6} {1,2,3} 1 {1,2,3,4,5,6} (phase B) * {4,5,6,1,2,3} {4,5,6} {1,2,3} 4 {1,2,3,4,5,6} (step B3) * {4,5,6,1,2,3} {4,5,6} {1,2,3} 4 {4,5,6} (step B4) * {4,5,6} {} {} 4 {4,5,6} (step B6) * * Note that we have to update RS in ZK with TRS last since it's the only place where we store ORS persistently. * This way, if the controller crashes before that step, we can still recover. */ private def onPartitionReassignment ( topicPartition : TopicPartition , reassignment : ReplicaAssignment ): Unit = { // While a reassignment is in progress, deletion is not allowed topicDeletionManager . markTopicIneligibleForDeletion ( Set ( topicPartition . topic ), reason = \"topic reassignment in progress\" ) updateCurrentReassignment ( topicPartition , reassignment ) val addingReplicas = reassignment . addingReplicas val removingReplicas = reassignment . removingReplicas if ( ! isReassignmentComplete ( topicPartition , reassignment )) { // A1. Send LeaderAndIsr request to every replica in ORS + TRS (with the new RS, AR and RR). updateLeaderEpochAndSendRequest ( topicPartition , reassignment ) // A2. replicas in AR -> NewReplica startNewReplicasForReassignedPartition ( topicPartition , addingReplicas ) } else { // B1. replicas in AR -> OnlineReplica replicaStateMachine . handleStateChanges ( addingReplicas . map ( PartitionAndReplica ( topicPartition , _ )), OnlineReplica ) // B2. Set RS = TRS, AR = [], RR = [] in memory. val completedReassignment = ReplicaAssignment ( reassignment . targetReplicas ) controllerContext . updatePartitionFullReplicaAssignment ( topicPartition , completedReassignment ) // B3. Send LeaderAndIsr request with a potential new leader (if current leader not in TRS) and // a new RS (using TRS) and same isr to every broker in ORS + TRS or TRS moveReassignedPartitionLeaderIfRequired ( topicPartition , completedReassignment ) // B4. replicas in RR -> Offline (force those replicas out of isr) // B5. replicas in RR -> NonExistentReplica (force those replicas to be deleted) stopRemovedReplicasOfReassignedPartition ( topicPartition , removingReplicas ) // B6. Update ZK with RS = TRS, AR = [], RR = []. updateReplicaAssignmentForPartition ( topicPartition , completedReassignment ) // B7. Remove the ISR reassign listener and maybe update the /admin/reassign_partitions path in ZK to remove this partition from it. removePartitionFromReassigningPartitions ( topicPartition , completedReassignment ) // B8. After electing a leader in B3, the replicas and isr information changes, so resend the update metadata request to every broker sendUpdateMetadataRequest ( controllerContext . liveOrShuttingDownBrokerIds . toSeq , Set ( topicPartition )) // signal delete topic thread if reassignment for some partitions belonging to topics being deleted just completed topicDeletionManager . resumeDeletionForTopics ( Set ( topicPartition . topic )) } }","title":"1.3.3 reassign partitions"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/3-reassign-partitions/#133-controller","text":"\u5728 controller \u542f\u52a8\u8fc7\u7a0b\u4e2d\uff0c\u5728 KafkaController.onControllerFailover() \u65b9\u6cd5\u4e2d\u5411 zkClient \u6ce8\u518c\u4e86 PartitionReassignmentHandler PartitionReassignmentHandler \u5982\u4e0b\uff1a class PartitionReassignmentHandler ( eventManager : ControllerEventManager ) extends ZNodeChangeHandler { override val path : String = ReassignPartitionsZNode . path // Note that the event is also enqueued when the znode is deleted, but we do it explicitly instead of relying on // handleDeletion(). This approach is more robust as it doesn't depend on the watcher being re-registered after // it's consumed during data changes (we ensure re-registration when the znode is deleted). override def handleCreation (): Unit = eventManager . put ( ZkPartitionReassignment ) } \u8fd9\u91cc\u5b9e\u9645\u4e0a\u662f\u76d1\u542c zookeeper /admin/reassign_partitions \u8282\u70b9\uff0c\u5982\u679c\u6709\u65b0\u8282\u70b9\u521b\u5efa\uff0c\u5219\u5411 eventManager \u4e8b\u4ef6\u961f\u5217 queue \u589e\u52a0 ZkPartitionReassignment eventManager \u4f1a\u542f\u52a8\u5355\u72ec\u7684\u7ebf\u7a0b\u4e0d\u65ad\u4ece\u4e8b\u4ef6\u961f\u5217\u4e2d\u53d6\u51fa\u4e8b\u4ef6\uff0c\u901a\u8fc7 processor \u8fdb\u884c\u5904\u7406 eventManager \u7684 processor \u662f KafkaController \u7684\u5b9e\u4f8b\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728 KafkaController.process() \u65b9\u6cd5\u91cc\u627e\u5230\u5bf9 ZkPartitionReassignment \u7684\u5904\u7406 override def process ( event : ControllerEvent ): Unit = { try { event match { case event : MockEvent => // Used only in test cases event . process () // ... case ApiPartitionReassignment ( reassignments , callback ) => processApiPartitionReassignment ( reassignments , callback ) case ZkPartitionReassignment => processZkPartitionReassignment () case ListPartitionReassignments ( partitions , callback ) => processListPartitionReassignments ( partitions , callback ) case PartitionReassignmentIsrChange ( partition ) => processPartitionReassignmentIsrChange ( partition ) // ... } } catch { case e : ControllerMovedException => info ( s\"Controller moved to another broker when processing $ event .\" , e ) maybeResign () case e : Throwable => error ( s\"Error processing event $ event \" , e ) } finally { updateMetrics () } } \u5bf9 ZkPartitionReassignment \u901a\u8fc7 processZkPartitionReassignment() \u8fdb\u884c\u5904\u7406\uff1a private def processZkPartitionReassignment (): Set [ TopicPartition ] = { // We need to register the watcher if the path doesn't exist in order to detect future // reassignments and we get the `path exists` check for free if ( isActive && zkClient . registerZNodeChangeHandlerAndCheckExistence ( partitionReassignmentHandler )) { val reassignmentResults = mutable . Map . empty [ TopicPartition , ApiError ] val partitionsToReassign = mutable . Map . empty [ TopicPartition , ReplicaAssignment ] // \u8fd9\u91cc\u7ec4\u5408\u4e86\u73b0\u5728\u7684 replica \u548c targetReplica zkClient . getPartitionReassignment . forKeyValue { ( tp , targetReplicas ) => maybeBuildReassignment ( tp , Some ( targetReplicas )) match { case Some ( context ) => partitionsToReassign . put ( tp , context ) case None => reassignmentResults . put ( tp , new ApiError ( Errors . NO_REASSIGNMENT_IN_PROGRESS )) } } // \u89e6\u53d1 partition reassignment reassignmentResults ++= maybeTriggerPartitionReassignment ( partitionsToReassign ) val ( partitionsReassigned , partitionsFailed ) = reassignmentResults . partition ( _ . _2 . error == Errors . NONE ) if ( partitionsFailed . nonEmpty ) { warn ( s\"Failed reassignment through zk with the following errors: $ partitionsFailed \" ) maybeRemoveFromZkReassignment (( tp , _ ) => partitionsFailed . contains ( tp )) } partitionsReassigned . keySet } else { Set . empty } } /** * Trigger a partition reassignment provided that the topic exists and is not being deleted. * * This is called when a reassignment is initially received either through Zookeeper or through the * AlterPartitionReassignments API * * The `partitionsBeingReassigned` field in the controller context will be updated by this * call after the reassignment completes validation and is successfully stored in the topic * assignment zNode. * * @param reassignments The reassignments to begin processing * @return A map of any errors in the reassignment. If the error is NONE for a given partition, * then the reassignment was submitted successfully. */ private def maybeTriggerPartitionReassignment ( reassignments : Map [ TopicPartition , ReplicaAssignment ]): Map [ TopicPartition , ApiError ] = { reassignments . map { case ( tp , reassignment ) => val topic = tp . topic val apiError = if ( topicDeletionManager . isTopicQueuedUpForDeletion ( topic )) { info ( s\"Skipping reassignment of $ tp since the topic is currently being deleted\" ) new ApiError ( Errors . UNKNOWN_TOPIC_OR_PARTITION , \"The partition does not exist.\" ) } else { val assignedReplicas = controllerContext . partitionReplicaAssignment ( tp ) if ( assignedReplicas . nonEmpty ) { try { onPartitionReassignment ( tp , reassignment ) ApiError . NONE } catch { case e : ControllerMovedException => info ( s\"Failed completing reassignment of partition $ tp because controller has moved to another broker\" ) throw e case e : Throwable => error ( s\"Error completing reassignment of partition $ tp \" , e ) new ApiError ( Errors . UNKNOWN_SERVER_ERROR ) } } else { new ApiError ( Errors . UNKNOWN_TOPIC_OR_PARTITION , \"The partition does not exist.\" ) } } tp -> apiError } } \u5b9a\u4e49\u4ee5\u4e0b\u6982\u5ff5\uff1a - RS: \u5f53\u524d\u526f\u672c\u96c6\u5408 - ORS: \u539f\u59cb\u526f\u672c\u96c6\u5408 - TRS: \u76ee\u6807\u526f\u672c\u96c6\u5408 - AR: \u5728\u8fd9\u6b21\u91cd\u5206\u914d\u4e2d\u8981\u589e\u52a0\u7684\u526f\u672c - RR: \u5728\u8fd9\u6b21\u91cd\u5206\u914d\u4e2d\u8981\u79fb\u9664\u7684\u526f\u672c \u5047\u8bbe\u4e00\u4e2a topic \u5f53\u524d\u526f\u672c\u96c6\u4e3a 1,2,3 \uff0c\u5219\u5176 RS \u548c ORS \u4e3a 1,2,3 \u5982\u679c\u8981\u5c06\u5176\u91cd\u5206\u914d\u4e3a 4,5,6 \uff0c\u5219\u5176 TRS \u4e3a 4,5,6 , AR \u4e3a 4,5,6 , RR \u4e3a 1,2,3 RS AR RR leader isr step 1,2,3 1 1,2,3 initial state 4,5,6,1,2,3 4,5,6 1,2,3 1 1,2,3 step A2 4,5,6,1,2,3 4,5,6 1,2,3 1 1,2,3,4,5,6 phase B 4,5,6,1,2,3 4,5,6 1,2,3 4 1,2,3,4,5,6 step B3 4,5,6,1,2,3 4,5,6 1,2,3 4 4,5,6 step B4 4,5,6 4 4,5,6 step B6 \u5c06\u6574\u4e2a\u91cd\u5206\u914d\u8fc7\u7a0b\u5206\u4e3a 3 \u4e2a\u9636\u6bb5\uff0c\u6bcf\u4e2a\u9636\u6bb5\u6709\u81ea\u5df1\u7684\u6b65\u9aa4 1. Phase U (Assignment update) 1. U1: \u66f4\u65b0 Zk \u4f7f RS = ORS + TRS, AR = TRS - ORS, RR = ORS - TRS 2. U2: \u66f4\u65b0 memory \u4f7f RS = ORS + TRS, AR = TRS - ORS, RR = ORS - TRS 3. U3: \u5982\u679c\u6b63\u5728\u53d6\u6d88\u5f53\u524d\u7684\u91cd\u5206\u914d\u6216\u8005\u66ff\u6362\u5f53\u524d\u7684\u91cd\u5206\u914d\uff0c\u5411\u6240\u6709\u4e0d\u5728\u65b0\u5206\u914d\u7684 TRS \u7684 AR \u53d1\u9001 StopReplica \u8bf7\u6c42 2. Phase A (when TRS != ISR) 1. A1: bump the leader epoch for the partition and send LeaderAndIsr updates to RS 2. A2: \u901a\u8fc7\u5c06 AR \u4e2d\u7684 replicas \u7f6e\u4e3a NewReplica \u72b6\u6001\u542f\u52a8\u65b0\u7684 replicas 3. Phase B (when TRS == ISR) 1. B1: \u5c06\u6240\u6709 AR \u4e2d\u7684 replicas \u7f6e\u4e3a OnlineReplica \u72b6\u6001 2. B2: \u66f4\u65b0 memory \u4f7f RS = TRS, AR = [], RR = [] 3. B3: \u53d1\u9001\u8868\u793a RS = TRS \u7684 LeaderAndIsr \u8bf7\u6c42\u3002This will prevent the leader from adding any replica in TRS - ORS back in the isr. \u5982\u679c\u5f53\u524d leader \u4e0d\u5728 TRS \u91cc\u6216\u8005\u4e0d\u662f\u5b58\u6d3b\u7684\uff0cwe move the leader to a new replica in TRS. We may send the LeaderAndIsr to more than the TRS replicas due to the way the partition state machine works (it reads replicas from ZK) 4. B4: \u5c06\u6240\u6709 RR \u4e2d\u7684 replicas \u7f6e\u4e3a OfflineReplica \u72b6\u6001\u3002As part of OfflineReplica state change, we shrink the isr to remove RR in ZooKeeper and send a LeaderAndIsr ONLY to the Leader to notify it of the shrunk isr. After that, we send a StopReplica (delete = false) to the replicas in RR. 5. B5: \u5c06\u6240\u6709 RR \u4e2d\u7684 replicas \u7f6e\u4e3a NonExistentReplica \u72b6\u6001\u3002This will send a StopReplica (delete = true) to the replicas in RR to physically delete the replicas on disk. 6. B6: \u66f4\u65b0 Zk \u4f7f RS = TRS, AR = [], RR = [] 7. B7: \u79fb\u9664 ISR reassign listener\uff0c\u66f4\u65b0 Zk \u7684 /admin/reassign_partitions \u79fb\u9664\u8fd9\u4e2a partition 8. B8: \u7ecf\u8fc7 leader \u9009\u4e3e\uff0creplicas \u548c isr \u4fe1\u606f\u6709\u53d8\u66f4\uff0c\u5411\u6240\u6709 broker \u53d1 metadata request /** * This callback is invoked: * 1. By the AlterPartitionReassignments API * 2. By the reassigned partitions listener which is triggered when the /admin/reassign/partitions znode is created * 3. When an ongoing reassignment finishes - this is detected by a change in the partition's ISR znode * 4. Whenever a new broker comes up which is part of an ongoing reassignment * 5. On controller startup/failover * * Reassigning replicas for a partition goes through a few steps listed in the code. * RS = current assigned replica set * ORS = Original replica set for partition * TRS = Reassigned (target) replica set * AR = The replicas we are adding as part of this reassignment * RR = The replicas we are removing as part of this reassignment * * A reassignment may have up to three phases, each with its own steps: * Phase U (Assignment update): Regardless of the trigger, the first step is in the reassignment process * is to update the existing assignment state. We always update the state in Zookeeper before * we update memory so that it can be resumed upon controller fail-over. * * U1. Update ZK with RS = ORS + TRS, AR = TRS - ORS, RR = ORS - TRS. * U2. Update memory with RS = ORS + TRS, AR = TRS - ORS and RR = ORS - TRS * U3. If we are cancelling or replacing an existing reassignment, send StopReplica to all members * of AR in the original reassignment if they are not in TRS from the new assignment * * To complete the reassignment, we need to bring the new replicas into sync, so depending on the state * of the ISR, we will execute one of the following steps. * * Phase A (when TRS != ISR): The reassignment is not yet complete * * A1. Bump the leader epoch for the partition and send LeaderAndIsr updates to RS. * A2. Start new replicas AR by moving replicas in AR to NewReplica state. * * Phase B (when TRS = ISR): The reassignment is complete * * B1. Move all replicas in AR to OnlineReplica state. * B2. Set RS = TRS, AR = [], RR = [] in memory. * B3. Send a LeaderAndIsr request with RS = TRS. This will prevent the leader from adding any replica in TRS - ORS back in the isr. * If the current leader is not in TRS or isn't alive, we move the leader to a new replica in TRS. * We may send the LeaderAndIsr to more than the TRS replicas due to the * way the partition state machine works (it reads replicas from ZK) * B4. Move all replicas in RR to OfflineReplica state. As part of OfflineReplica state change, we shrink the * isr to remove RR in ZooKeeper and send a LeaderAndIsr ONLY to the Leader to notify it of the shrunk isr. * After that, we send a StopReplica (delete = false) to the replicas in RR. * B5. Move all replicas in RR to NonExistentReplica state. This will send a StopReplica (delete = true) to * the replicas in RR to physically delete the replicas on disk. * B6. Update ZK with RS=TRS, AR=[], RR=[]. * B7. Remove the ISR reassign listener and maybe update the /admin/reassign_partitions path in ZK to remove this partition from it if present. * B8. After electing leader, the replicas and isr information changes. So resend the update metadata request to every broker. * * In general, there are two goals we want to aim for: * 1. Every replica present in the replica set of a LeaderAndIsrRequest gets the request sent to it * 2. Replicas that are removed from a partition's assignment get StopReplica sent to them * * For example, if ORS = {1,2,3} and TRS = {4,5,6}, the values in the topic and leader/isr paths in ZK * may go through the following transitions. * RS AR RR leader isr * {1,2,3} {} {} 1 {1,2,3} (initial state) * {4,5,6,1,2,3} {4,5,6} {1,2,3} 1 {1,2,3} (step A2) * {4,5,6,1,2,3} {4,5,6} {1,2,3} 1 {1,2,3,4,5,6} (phase B) * {4,5,6,1,2,3} {4,5,6} {1,2,3} 4 {1,2,3,4,5,6} (step B3) * {4,5,6,1,2,3} {4,5,6} {1,2,3} 4 {4,5,6} (step B4) * {4,5,6} {} {} 4 {4,5,6} (step B6) * * Note that we have to update RS in ZK with TRS last since it's the only place where we store ORS persistently. * This way, if the controller crashes before that step, we can still recover. */ private def onPartitionReassignment ( topicPartition : TopicPartition , reassignment : ReplicaAssignment ): Unit = { // While a reassignment is in progress, deletion is not allowed topicDeletionManager . markTopicIneligibleForDeletion ( Set ( topicPartition . topic ), reason = \"topic reassignment in progress\" ) updateCurrentReassignment ( topicPartition , reassignment ) val addingReplicas = reassignment . addingReplicas val removingReplicas = reassignment . removingReplicas if ( ! isReassignmentComplete ( topicPartition , reassignment )) { // A1. Send LeaderAndIsr request to every replica in ORS + TRS (with the new RS, AR and RR). updateLeaderEpochAndSendRequest ( topicPartition , reassignment ) // A2. replicas in AR -> NewReplica startNewReplicasForReassignedPartition ( topicPartition , addingReplicas ) } else { // B1. replicas in AR -> OnlineReplica replicaStateMachine . handleStateChanges ( addingReplicas . map ( PartitionAndReplica ( topicPartition , _ )), OnlineReplica ) // B2. Set RS = TRS, AR = [], RR = [] in memory. val completedReassignment = ReplicaAssignment ( reassignment . targetReplicas ) controllerContext . updatePartitionFullReplicaAssignment ( topicPartition , completedReassignment ) // B3. Send LeaderAndIsr request with a potential new leader (if current leader not in TRS) and // a new RS (using TRS) and same isr to every broker in ORS + TRS or TRS moveReassignedPartitionLeaderIfRequired ( topicPartition , completedReassignment ) // B4. replicas in RR -> Offline (force those replicas out of isr) // B5. replicas in RR -> NonExistentReplica (force those replicas to be deleted) stopRemovedReplicasOfReassignedPartition ( topicPartition , removingReplicas ) // B6. Update ZK with RS = TRS, AR = [], RR = []. updateReplicaAssignmentForPartition ( topicPartition , completedReassignment ) // B7. Remove the ISR reassign listener and maybe update the /admin/reassign_partitions path in ZK to remove this partition from it. removePartitionFromReassigningPartitions ( topicPartition , completedReassignment ) // B8. After electing a leader in B3, the replicas and isr information changes, so resend the update metadata request to every broker sendUpdateMetadataRequest ( controllerContext . liveOrShuttingDownBrokerIds . toSeq , Set ( topicPartition )) // signal delete topic thread if reassignment for some partitions belonging to topics being deleted just completed topicDeletionManager . resumeDeletionForTopics ( Set ( topicPartition . topic )) } }","title":"1.3.3 controller"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/4-delete-topic/","text":"1.3.4 delete topic","title":"1.3.4 delete topic"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/4-delete-topic/#134-delete-topic","text":"","title":"1.3.4 delete topic"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/","text":"1.3.2 controller \u7ec4\u4ef6 ControllerChannelManager \u4e8b\u4ef6\u5904\u7406 ControllerEventProcessor QueuedEvent ControllerEventManager \u7ef4\u6301\u4e00\u4e2a queue \u5b58\u653e\u4e8b\u4ef6\uff1a private val queue = new LinkedBlockingQueue [ QueuedEvent ] \u540c\u65f6\u4f1a\u521b\u5efa\u4e00\u4e2a thread \u5728\u540e\u53f0\u4e0d\u65ad\u4ece queue \u4e2d\u53d6\u4e8b\u4ef6\u5e76\u8fdb\u884c\u5904\u7406 private [ controller ] var thread = new ControllerEventThread ( ControllerEventThreadName ) class ControllerEventThread ( name : String ) extends ShutdownableThread ( name = name , isInterruptible = false ) { logIdent = s\"[ControllerEventThread controllerId= $ controllerId ] \" override def doWork (): Unit = { val dequeued = pollFromEventQueue () dequeued . event match { case ShutdownEventThread => // The shutting down of the thread has been initiated at this point. Ignore this event. case controllerEvent => _state = controllerEvent . state eventQueueTimeHist . update ( time . milliseconds () - dequeued . enqueueTimeMs ) try { def process (): Unit = dequeued . process ( processor ) rateAndTimeMetrics . get ( state ) match { case Some ( timer ) => timer . time { process () } case None => process () } } catch { case e : Throwable => error ( s\"Uncaught error processing event $ controllerEvent \" , e ) } _state = ControllerState . Idle } } } KafkaController \u7ee7\u627f\u81ea ControllerEventProcessor \uff0c\u5b9a\u4e49 process() \u65b9\u6cd5\u5982\u4e0b\uff1a override def process ( event : ControllerEvent ): Unit = { try { event match { case event : MockEvent => // Used only in test cases event . process () case ShutdownEventThread => error ( \"Received a ShutdownEventThread event. This type of event is supposed to be handle by ControllerEventThread\" ) case AutoPreferredReplicaLeaderElection => processAutoPreferredReplicaLeaderElection () case ReplicaLeaderElection ( partitions , electionType , electionTrigger , callback ) => processReplicaLeaderElection ( partitions , electionType , electionTrigger , callback ) case UncleanLeaderElectionEnable => processUncleanLeaderElectionEnable () case TopicUncleanLeaderElectionEnable ( topic ) => processTopicUncleanLeaderElectionEnable ( topic ) case ControlledShutdown ( id , brokerEpoch , callback ) => processControlledShutdown ( id , brokerEpoch , callback ) case LeaderAndIsrResponseReceived ( response , brokerId ) => processLeaderAndIsrResponseReceived ( response , brokerId ) case UpdateMetadataResponseReceived ( response , brokerId ) => processUpdateMetadataResponseReceived ( response , brokerId ) case TopicDeletionStopReplicaResponseReceived ( replicaId , requestError , partitionErrors ) => processTopicDeletionStopReplicaResponseReceived ( replicaId , requestError , partitionErrors ) case BrokerChange => processBrokerChange () case BrokerModifications ( brokerId ) => processBrokerModification ( brokerId ) case ControllerChange => processControllerChange () case Reelect => processReelect () case RegisterBrokerAndReelect => processRegisterBrokerAndReelect () case Expire => processExpire () case TopicChange => processTopicChange () case LogDirEventNotification => processLogDirEventNotification () case PartitionModifications ( topic ) => processPartitionModifications ( topic ) case TopicDeletion => processTopicDeletion () case ApiPartitionReassignment ( reassignments , callback ) => processApiPartitionReassignment ( reassignments , callback ) case ZkPartitionReassignment => processZkPartitionReassignment () case ListPartitionReassignments ( partitions , callback ) => processListPartitionReassignments ( partitions , callback ) case UpdateFeatures ( request , callback ) => processFeatureUpdates ( request , callback ) case PartitionReassignmentIsrChange ( partition ) => processPartitionReassignmentIsrChange ( partition ) case IsrChangeNotification => processIsrChangeNotification () case AlterIsrReceived ( brokerId , brokerEpoch , isrsToAlter , callback ) => processAlterIsr ( brokerId , brokerEpoch , isrsToAlter , callback ) case Startup => processStartup () } } catch { case e : ControllerMovedException => info ( s\"Controller moved to another broker when processing $ event .\" , e ) maybeResign () case e : Throwable => error ( s\"Error processing event $ event \" , e ) } finally { updateMetrics () } } \u8fd9\u91cc\u4f1a\u5bf9\u4e8b\u4ef6\u8fdb\u884c\u5904\u7406","title":"1.3.2 KafkaController \u7ec4\u4ef6"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#132-controller","text":"","title":"1.3.2 controller \u7ec4\u4ef6"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#controllerchannelmanager","text":"","title":"ControllerChannelManager"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#_1","text":"","title":"\u4e8b\u4ef6\u5904\u7406"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#controllereventprocessor","text":"","title":"ControllerEventProcessor"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#queuedevent","text":"","title":"QueuedEvent"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#controllereventmanager","text":"\u7ef4\u6301\u4e00\u4e2a queue \u5b58\u653e\u4e8b\u4ef6\uff1a private val queue = new LinkedBlockingQueue [ QueuedEvent ] \u540c\u65f6\u4f1a\u521b\u5efa\u4e00\u4e2a thread \u5728\u540e\u53f0\u4e0d\u65ad\u4ece queue \u4e2d\u53d6\u4e8b\u4ef6\u5e76\u8fdb\u884c\u5904\u7406 private [ controller ] var thread = new ControllerEventThread ( ControllerEventThreadName ) class ControllerEventThread ( name : String ) extends ShutdownableThread ( name = name , isInterruptible = false ) { logIdent = s\"[ControllerEventThread controllerId= $ controllerId ] \" override def doWork (): Unit = { val dequeued = pollFromEventQueue () dequeued . event match { case ShutdownEventThread => // The shutting down of the thread has been initiated at this point. Ignore this event. case controllerEvent => _state = controllerEvent . state eventQueueTimeHist . update ( time . milliseconds () - dequeued . enqueueTimeMs ) try { def process (): Unit = dequeued . process ( processor ) rateAndTimeMetrics . get ( state ) match { case Some ( timer ) => timer . time { process () } case None => process () } } catch { case e : Throwable => error ( s\"Uncaught error processing event $ controllerEvent \" , e ) } _state = ControllerState . Idle } } } KafkaController \u7ee7\u627f\u81ea ControllerEventProcessor \uff0c\u5b9a\u4e49 process() \u65b9\u6cd5\u5982\u4e0b\uff1a override def process ( event : ControllerEvent ): Unit = { try { event match { case event : MockEvent => // Used only in test cases event . process () case ShutdownEventThread => error ( \"Received a ShutdownEventThread event. This type of event is supposed to be handle by ControllerEventThread\" ) case AutoPreferredReplicaLeaderElection => processAutoPreferredReplicaLeaderElection () case ReplicaLeaderElection ( partitions , electionType , electionTrigger , callback ) => processReplicaLeaderElection ( partitions , electionType , electionTrigger , callback ) case UncleanLeaderElectionEnable => processUncleanLeaderElectionEnable () case TopicUncleanLeaderElectionEnable ( topic ) => processTopicUncleanLeaderElectionEnable ( topic ) case ControlledShutdown ( id , brokerEpoch , callback ) => processControlledShutdown ( id , brokerEpoch , callback ) case LeaderAndIsrResponseReceived ( response , brokerId ) => processLeaderAndIsrResponseReceived ( response , brokerId ) case UpdateMetadataResponseReceived ( response , brokerId ) => processUpdateMetadataResponseReceived ( response , brokerId ) case TopicDeletionStopReplicaResponseReceived ( replicaId , requestError , partitionErrors ) => processTopicDeletionStopReplicaResponseReceived ( replicaId , requestError , partitionErrors ) case BrokerChange => processBrokerChange () case BrokerModifications ( brokerId ) => processBrokerModification ( brokerId ) case ControllerChange => processControllerChange () case Reelect => processReelect () case RegisterBrokerAndReelect => processRegisterBrokerAndReelect () case Expire => processExpire () case TopicChange => processTopicChange () case LogDirEventNotification => processLogDirEventNotification () case PartitionModifications ( topic ) => processPartitionModifications ( topic ) case TopicDeletion => processTopicDeletion () case ApiPartitionReassignment ( reassignments , callback ) => processApiPartitionReassignment ( reassignments , callback ) case ZkPartitionReassignment => processZkPartitionReassignment () case ListPartitionReassignments ( partitions , callback ) => processListPartitionReassignments ( partitions , callback ) case UpdateFeatures ( request , callback ) => processFeatureUpdates ( request , callback ) case PartitionReassignmentIsrChange ( partition ) => processPartitionReassignmentIsrChange ( partition ) case IsrChangeNotification => processIsrChangeNotification () case AlterIsrReceived ( brokerId , brokerEpoch , isrsToAlter , callback ) => processAlterIsr ( brokerId , brokerEpoch , isrsToAlter , callback ) case Startup => processStartup () } } catch { case e : ControllerMovedException => info ( s\"Controller moved to another broker when processing $ event .\" , e ) maybeResign () case e : Throwable => error ( s\"Error processing event $ event \" , e ) } finally { updateMetrics () } } \u8fd9\u91cc\u4f1a\u5bf9\u4e8b\u4ef6\u8fdb\u884c\u5904\u7406","title":"ControllerEventManager"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/1-ControllerContext/","text":"1.3.2.1 ControllerContext ControllerContext \u5305\u542b Controller \u9700\u8981\u7ef4\u62a4\u7684\u4fe1\u606f\uff1a // \u4e0b\u7ebf\u5206\u533a\u6570 var offlinePartitionCount = 0 var preferredReplicaImbalanceCount = 0 val shuttingDownBrokerIds = mutable . Set . empty [ Int ] private val liveBrokers = mutable . Set . empty [ Broker ] private val liveBrokerEpochs = mutable . Map . empty [ Int , Long ] var epoch : Int = KafkaController . InitialControllerEpoch var epochZkVersion : Int = KafkaController . InitialControllerEpochZkVersion val allTopics = mutable . Set . empty [ String ] // \u8bb0\u5f55 topic \u5bf9\u5e94\u7684 partition \u5bf9\u5e94\u7684 replicaAssignments val partitionAssignments = mutable . Map . empty [ String , mutable . Map [ Int , ReplicaAssignment ]] // \u8bb0\u5f55 topic partition \u4e0e\u5176\u5bf9\u5e94\u7684 leader isr private val partitionLeadershipInfo = mutable . Map . empty [ TopicPartition , LeaderIsrAndControllerEpoch ] val partitionsBeingReassigned = mutable . Set . empty [ TopicPartition ] // \u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 \u72b6\u6001 val partitionStates = mutable . Map . empty [ TopicPartition , PartitionState ] // \u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 replica \u72b6\u6001 val replicaStates = mutable . Map . empty [ PartitionAndReplica , ReplicaState ] val replicasOnOfflineDirs = mutable . Map . empty [ Int , Set [ TopicPartition ]] val topicsToBeDeleted = mutable . Set . empty [ String ]","title":"1.3.2.1 ControllerContext"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/1-ControllerContext/#1321-controllercontext","text":"ControllerContext \u5305\u542b Controller \u9700\u8981\u7ef4\u62a4\u7684\u4fe1\u606f\uff1a // \u4e0b\u7ebf\u5206\u533a\u6570 var offlinePartitionCount = 0 var preferredReplicaImbalanceCount = 0 val shuttingDownBrokerIds = mutable . Set . empty [ Int ] private val liveBrokers = mutable . Set . empty [ Broker ] private val liveBrokerEpochs = mutable . Map . empty [ Int , Long ] var epoch : Int = KafkaController . InitialControllerEpoch var epochZkVersion : Int = KafkaController . InitialControllerEpochZkVersion val allTopics = mutable . Set . empty [ String ] // \u8bb0\u5f55 topic \u5bf9\u5e94\u7684 partition \u5bf9\u5e94\u7684 replicaAssignments val partitionAssignments = mutable . Map . empty [ String , mutable . Map [ Int , ReplicaAssignment ]] // \u8bb0\u5f55 topic partition \u4e0e\u5176\u5bf9\u5e94\u7684 leader isr private val partitionLeadershipInfo = mutable . Map . empty [ TopicPartition , LeaderIsrAndControllerEpoch ] val partitionsBeingReassigned = mutable . Set . empty [ TopicPartition ] // \u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 \u72b6\u6001 val partitionStates = mutable . Map . empty [ TopicPartition , PartitionState ] // \u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 replica \u72b6\u6001 val replicaStates = mutable . Map . empty [ PartitionAndReplica , ReplicaState ] val replicasOnOfflineDirs = mutable . Map . empty [ Int , Set [ TopicPartition ]] val topicsToBeDeleted = mutable . Set . empty [ String ]","title":"1.3.2.1 ControllerContext"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/2-ReplicaStateMachine/","text":"1.3.2.2 ReplicaStateMachine ReplicaStateMachine \u63d0\u4f9b\u63a5\u53e3\uff0c\u8d1f\u8d23\u5904\u7406 replica \u72b6\u6001\u53d8\u66f4\u3002replica \u72b6\u6001\u7f13\u5b58\u5728 ControllerContext partitionAssignments \u4e2d\uff0c\u72b6\u6001\u53d8\u66f4\u8fc7\u7a0b\u9700\u8981\u4e0e zookeeper \u4ee5\u53ca broker \u4ea4\u4e92\u3002 replica \u6240\u6709\u72b6\u6001\uff1a NewReplica : controller \u5728 partition reassignment \u671f\u95f4\u53ef\u4ee5\u521b\u5efa\u65b0\u7684 replicas\u3002\u5728\u6b64\u72b6\u6001\u4e0b\uff0creplica \u53ea\u80fd\u6210\u4e3a follower\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f NonExistentReplica OnlineReplica : \u5f53 replica \u6210\u4e3a\u5176 partition \u7684 assigned replicas \u7684\u4e00\u90e8\u5206\uff0creplica \u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u5728\u6b64\u72b6\u6001\u4e0b\uff0creplica \u53ef\u4ee5\u6210\u4e3a leader \u6216\u8005 follower\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u6709 NewReplica , OnlineReplica , OfflineReplica OfflineReplica : \u5982\u679c replica \u6240\u5904 broker \u4e0b\u7ebf\uff0creplica \u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f NewReplica , OnlineReplica ReplicaDeletionStarted : \u5982\u679c replica \u5f00\u59cb\u5220\u9664\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f OfflineReplica ReplicaDeletionSuccessful : \u5982\u679c delete replica \u8bf7\u6c42\u7684\u54cd\u5e94\u6ca1\u6709\u9519\u8bef\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f ReplicaDeletionStarted ReplicaDeletionIneligible : \u5982\u679c replica \u5220\u9664\u5931\u8d25\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f ReplicaDeletionStarted , OfflineReplica NonExistentReplica : \u5982\u679c replica \u5220\u9664\u6210\u529f\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f ReplicaDeletionSuccessful sealed trait ReplicaState { def state : Byte def validPreviousStates : Set [ ReplicaState ] } case object NewReplica extends ReplicaState { val state : Byte = 1 val validPreviousStates : Set [ ReplicaState ] = Set ( NonExistentReplica ) } case object OnlineReplica extends ReplicaState { val state : Byte = 2 val validPreviousStates : Set [ ReplicaState ] = Set ( NewReplica , OnlineReplica , OfflineReplica , ReplicaDeletionIneligible ) } case object OfflineReplica extends ReplicaState { val state : Byte = 3 val validPreviousStates : Set [ ReplicaState ] = Set ( NewReplica , OnlineReplica , OfflineReplica , ReplicaDeletionIneligible ) } case object ReplicaDeletionStarted extends ReplicaState { val state : Byte = 4 val validPreviousStates : Set [ ReplicaState ] = Set ( OfflineReplica ) } case object ReplicaDeletionSuccessful extends ReplicaState { val state : Byte = 5 val validPreviousStates : Set [ ReplicaState ] = Set ( ReplicaDeletionStarted ) } case object ReplicaDeletionIneligible extends ReplicaState { val state : Byte = 6 val validPreviousStates : Set [ ReplicaState ] = Set ( OfflineReplica , ReplicaDeletionStarted ) } case object NonExistentReplica extends ReplicaState { val state : Byte = 7 val validPreviousStates : Set [ ReplicaState ] = Set ( ReplicaDeletionSuccessful ) }","title":"1.3.2.2 ReplicaStateMachine"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/2-ReplicaStateMachine/#1322-replicastatemachine","text":"ReplicaStateMachine \u63d0\u4f9b\u63a5\u53e3\uff0c\u8d1f\u8d23\u5904\u7406 replica \u72b6\u6001\u53d8\u66f4\u3002replica \u72b6\u6001\u7f13\u5b58\u5728 ControllerContext partitionAssignments \u4e2d\uff0c\u72b6\u6001\u53d8\u66f4\u8fc7\u7a0b\u9700\u8981\u4e0e zookeeper \u4ee5\u53ca broker \u4ea4\u4e92\u3002 replica \u6240\u6709\u72b6\u6001\uff1a NewReplica : controller \u5728 partition reassignment \u671f\u95f4\u53ef\u4ee5\u521b\u5efa\u65b0\u7684 replicas\u3002\u5728\u6b64\u72b6\u6001\u4e0b\uff0creplica \u53ea\u80fd\u6210\u4e3a follower\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f NonExistentReplica OnlineReplica : \u5f53 replica \u6210\u4e3a\u5176 partition \u7684 assigned replicas \u7684\u4e00\u90e8\u5206\uff0creplica \u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u5728\u6b64\u72b6\u6001\u4e0b\uff0creplica \u53ef\u4ee5\u6210\u4e3a leader \u6216\u8005 follower\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u6709 NewReplica , OnlineReplica , OfflineReplica OfflineReplica : \u5982\u679c replica \u6240\u5904 broker \u4e0b\u7ebf\uff0creplica \u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f NewReplica , OnlineReplica ReplicaDeletionStarted : \u5982\u679c replica \u5f00\u59cb\u5220\u9664\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f OfflineReplica ReplicaDeletionSuccessful : \u5982\u679c delete replica \u8bf7\u6c42\u7684\u54cd\u5e94\u6ca1\u6709\u9519\u8bef\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f ReplicaDeletionStarted ReplicaDeletionIneligible : \u5982\u679c replica \u5220\u9664\u5931\u8d25\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f ReplicaDeletionStarted , OfflineReplica NonExistentReplica : \u5982\u679c replica \u5220\u9664\u6210\u529f\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f ReplicaDeletionSuccessful sealed trait ReplicaState { def state : Byte def validPreviousStates : Set [ ReplicaState ] } case object NewReplica extends ReplicaState { val state : Byte = 1 val validPreviousStates : Set [ ReplicaState ] = Set ( NonExistentReplica ) } case object OnlineReplica extends ReplicaState { val state : Byte = 2 val validPreviousStates : Set [ ReplicaState ] = Set ( NewReplica , OnlineReplica , OfflineReplica , ReplicaDeletionIneligible ) } case object OfflineReplica extends ReplicaState { val state : Byte = 3 val validPreviousStates : Set [ ReplicaState ] = Set ( NewReplica , OnlineReplica , OfflineReplica , ReplicaDeletionIneligible ) } case object ReplicaDeletionStarted extends ReplicaState { val state : Byte = 4 val validPreviousStates : Set [ ReplicaState ] = Set ( OfflineReplica ) } case object ReplicaDeletionSuccessful extends ReplicaState { val state : Byte = 5 val validPreviousStates : Set [ ReplicaState ] = Set ( ReplicaDeletionStarted ) } case object ReplicaDeletionIneligible extends ReplicaState { val state : Byte = 6 val validPreviousStates : Set [ ReplicaState ] = Set ( OfflineReplica , ReplicaDeletionStarted ) } case object NonExistentReplica extends ReplicaState { val state : Byte = 7 val validPreviousStates : Set [ ReplicaState ] = Set ( ReplicaDeletionSuccessful ) }","title":"1.3.2.2 ReplicaStateMachine"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/3-PartitionStateMachine/","text":"1.3.2.3 PartitionStateMachine partition \u6240\u6709\u72b6\u6001\uff1a NonExistentPartition : \u8fd9\u4e2a\u72b6\u6001\u8868\u793a parition \u6ca1\u6709\u88ab\u521b\u5efa\uff0c\u6216\u8005\u66fe\u521b\u5efa\u8fc7\u4f46\u88ab\u5220\u9664\u4e86\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f OfflinePartition NewPartition : \u521b\u5efa\u4e4b\u540e\uff0cpartition \u5904\u4e8e NewPartition \u72b6\u6001\u3002\u5728\u6b64\u72b6\u6001\u4e0b\uff0cpartition \u5e94\u8be5\u6709 assigned replicas\uff0c\u4f46\u662f\u6ca1\u6709 leader \u548c isr\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f NonExistentPartition OnlinePartition : \u5f53 partition \u7684 leader \u9009\u4e3e\u51fa\u6765\uff0c\u5176\u5904\u4e8e OnlinePartition \u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f NewPartition , OfflinePartition OfflinePartition : \u5982\u679c\u7ecf\u8fc7\u6210\u529f\u7684 leader \u9009\u4e3e\u4e4b\u540e\uff0cleader \u56e0\u4e3a\u4e00\u4e9b\u539f\u56e0\u4e0b\u7ebf\u4e86\uff0cpartition \u6b64\u65f6\u5904\u4e8e OfflinePartition \u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f NewPartition , OnlinePartition sealed trait PartitionState { def state : Byte def validPreviousStates : Set [ PartitionState ] } case object NewPartition extends PartitionState { val state : Byte = 0 val validPreviousStates : Set [ PartitionState ] = Set ( NonExistentPartition ) } case object OnlinePartition extends PartitionState { val state : Byte = 1 val validPreviousStates : Set [ PartitionState ] = Set ( NewPartition , OnlinePartition , OfflinePartition ) } case object OfflinePartition extends PartitionState { val state : Byte = 2 val validPreviousStates : Set [ PartitionState ] = Set ( NewPartition , OnlinePartition , OfflinePartition ) } case object NonExistentPartition extends PartitionState { val state : Byte = 3 val validPreviousStates : Set [ PartitionState ] = Set ( OfflinePartition ) }","title":"1.3.2.3 PartitionStateMachine"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/3-PartitionStateMachine/#1323-partitionstatemachine","text":"partition \u6240\u6709\u72b6\u6001\uff1a NonExistentPartition : \u8fd9\u4e2a\u72b6\u6001\u8868\u793a parition \u6ca1\u6709\u88ab\u521b\u5efa\uff0c\u6216\u8005\u66fe\u521b\u5efa\u8fc7\u4f46\u88ab\u5220\u9664\u4e86\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f OfflinePartition NewPartition : \u521b\u5efa\u4e4b\u540e\uff0cpartition \u5904\u4e8e NewPartition \u72b6\u6001\u3002\u5728\u6b64\u72b6\u6001\u4e0b\uff0cpartition \u5e94\u8be5\u6709 assigned replicas\uff0c\u4f46\u662f\u6ca1\u6709 leader \u548c isr\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f NonExistentPartition OnlinePartition : \u5f53 partition \u7684 leader \u9009\u4e3e\u51fa\u6765\uff0c\u5176\u5904\u4e8e OnlinePartition \u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f NewPartition , OfflinePartition OfflinePartition : \u5982\u679c\u7ecf\u8fc7\u6210\u529f\u7684 leader \u9009\u4e3e\u4e4b\u540e\uff0cleader \u56e0\u4e3a\u4e00\u4e9b\u539f\u56e0\u4e0b\u7ebf\u4e86\uff0cpartition \u6b64\u65f6\u5904\u4e8e OfflinePartition \u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f NewPartition , OnlinePartition sealed trait PartitionState { def state : Byte def validPreviousStates : Set [ PartitionState ] } case object NewPartition extends PartitionState { val state : Byte = 0 val validPreviousStates : Set [ PartitionState ] = Set ( NonExistentPartition ) } case object OnlinePartition extends PartitionState { val state : Byte = 1 val validPreviousStates : Set [ PartitionState ] = Set ( NewPartition , OnlinePartition , OfflinePartition ) } case object OfflinePartition extends PartitionState { val state : Byte = 2 val validPreviousStates : Set [ PartitionState ] = Set ( NewPartition , OnlinePartition , OfflinePartition ) } case object NonExistentPartition extends PartitionState { val state : Byte = 3 val validPreviousStates : Set [ PartitionState ] = Set ( OfflinePartition ) }","title":"1.3.2.3 PartitionStateMachine"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/4-ControllerChannelManager/","text":"1.3.2.4 ControllerChannelManager Controller \u4f1a\u5411 broker \u53d1\u9001 3 \u79cd\u7c7b\u578b\u7684\u8bf7\u6c42\uff1a LeaderAndIsrRequest UpdateMetadataRequest StopReplicaRequest ControllerChannelManager \u7ef4\u62a4\u4e0e broker \u4e4b\u95f4\u7684\u8fde\u63a5 protected val brokerStateInfo = new HashMap [ Int , ControllerBrokerStateInfo ] private val brokerLock = new Object ControllerBrokerStateInfo \u5b9a\u4e49\u5982\u4e0b\uff1a case class ControllerBrokerStateInfo ( networkClient : NetworkClient , brokerNode : Node , messageQueue : BlockingQueue [ QueueItem ], requestSendThread : RequestSendThread , queueSizeGauge : Gauge [ Int ], requestRateAndTimeMetrics : Timer , reconfigurableChannelBuilder : Option [ Reconfigurable ])","title":"1.3.2.4 ControllerChannelManager"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/4-ControllerChannelManager/#1324-controllerchannelmanager","text":"Controller \u4f1a\u5411 broker \u53d1\u9001 3 \u79cd\u7c7b\u578b\u7684\u8bf7\u6c42\uff1a LeaderAndIsrRequest UpdateMetadataRequest StopReplicaRequest ControllerChannelManager \u7ef4\u62a4\u4e0e broker \u4e4b\u95f4\u7684\u8fde\u63a5 protected val brokerStateInfo = new HashMap [ Int , ControllerBrokerStateInfo ] private val brokerLock = new Object ControllerBrokerStateInfo \u5b9a\u4e49\u5982\u4e0b\uff1a case class ControllerBrokerStateInfo ( networkClient : NetworkClient , brokerNode : Node , messageQueue : BlockingQueue [ QueueItem ], requestSendThread : RequestSendThread , queueSizeGauge : Gauge [ Int ], requestRateAndTimeMetrics : Timer , reconfigurableChannelBuilder : Option [ Reconfigurable ])","title":"1.3.2.4 ControllerChannelManager"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/","text":"1.4 ReplicaManager \u542f\u52a8 ReplicaFetcherManager \u8d1f\u8d23\u5b9a\u65f6\u68c0\u67e5\u526f\u672c\u662f\u5426\u843d\u540e\uff0c\u5982\u679c\u843d\u540e\uff0c\u9700\u8981\u901a\u77e5 controller \u5c06\u5176\u79fb\u51fa isr \u63d0\u4f9b fetchMessages() \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea consumer \u6216\u8005 follower \u7684 FetchRequest \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 fetchMessages() \u65b9\u6cd5\uff0c fetchMessages() \u4f1a\u66f4\u65b0\u526f\u672c\u72b6\u6001\uff0c\u5fc5\u8981\u65f6\u901a\u77e5 controller \u5c06\u526f\u672c\u52a0\u5165 isr \u63d0\u4f9b stopReplicas() \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea controller \u7684 StopReplicaRequest \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 stopReplicas() \u65b9\u6cd5 \u63d0\u4f9b becomeLeaderOrFollower() \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea controller \u7684 LeaderAndIsrRequest \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 becomeLeaderOrFollower() \u65b9\u6cd5 \u5bf9\u4e8e\u6210\u4e3a leader \u7684\u672c\u5730 replica\uff0c\u8c03\u7528 makeLeaders() \u5bf9\u4e8e\u6210\u4e3a follower \u7684\u672c\u5730 replica\uff0c\u8c03\u7528 makeFollowers() \uff0c\u8fd9\u91cc\u4f1a\u521b\u5efa\u5e76\u542f\u52a8 fetcherThread \u63d0\u4f9b maybeUpdateMetadataCache() \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea controller \u7684 UpdateMetadataRequest \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 maybeUpdateMetadataCache() \u65b9\u6cd5 \u63d0\u4f9b appendRecords() \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea producer \u7684 ProduceRequest \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 appendRecords() \u65b9\u6cd5 ListOffsetRequest handleListOffsetRequestV0: \u63d0\u4f9b legacyFetchOffsetsForTimestamp \u65b9\u6cd5\uff0cbroker \u5904\u7406 ListOffsetRequestV0 \u8bf7\u6c42\u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 legacyFetchOffsetsForTimestamp \u65b9\u6cd5 handleListOffsetRequestV1AndAbove: \u63d0\u4f9b fetchOffsetForTimestamp \u65b9\u6cd5\uff0cbroker \u5904\u7406 ListOffsetRequestV1 \u8bf7\u6c42\u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 fetchOffsetForTimestamp \u65b9\u6cd5","title":"1.4 ReplicaManager"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/#14-replicamanager","text":"\u542f\u52a8 ReplicaFetcherManager \u8d1f\u8d23\u5b9a\u65f6\u68c0\u67e5\u526f\u672c\u662f\u5426\u843d\u540e\uff0c\u5982\u679c\u843d\u540e\uff0c\u9700\u8981\u901a\u77e5 controller \u5c06\u5176\u79fb\u51fa isr \u63d0\u4f9b fetchMessages() \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea consumer \u6216\u8005 follower \u7684 FetchRequest \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 fetchMessages() \u65b9\u6cd5\uff0c fetchMessages() \u4f1a\u66f4\u65b0\u526f\u672c\u72b6\u6001\uff0c\u5fc5\u8981\u65f6\u901a\u77e5 controller \u5c06\u526f\u672c\u52a0\u5165 isr \u63d0\u4f9b stopReplicas() \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea controller \u7684 StopReplicaRequest \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 stopReplicas() \u65b9\u6cd5 \u63d0\u4f9b becomeLeaderOrFollower() \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea controller \u7684 LeaderAndIsrRequest \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 becomeLeaderOrFollower() \u65b9\u6cd5 \u5bf9\u4e8e\u6210\u4e3a leader \u7684\u672c\u5730 replica\uff0c\u8c03\u7528 makeLeaders() \u5bf9\u4e8e\u6210\u4e3a follower \u7684\u672c\u5730 replica\uff0c\u8c03\u7528 makeFollowers() \uff0c\u8fd9\u91cc\u4f1a\u521b\u5efa\u5e76\u542f\u52a8 fetcherThread \u63d0\u4f9b maybeUpdateMetadataCache() \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea controller \u7684 UpdateMetadataRequest \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 maybeUpdateMetadataCache() \u65b9\u6cd5 \u63d0\u4f9b appendRecords() \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea producer \u7684 ProduceRequest \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 appendRecords() \u65b9\u6cd5 ListOffsetRequest handleListOffsetRequestV0: \u63d0\u4f9b legacyFetchOffsetsForTimestamp \u65b9\u6cd5\uff0cbroker \u5904\u7406 ListOffsetRequestV0 \u8bf7\u6c42\u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 legacyFetchOffsetsForTimestamp \u65b9\u6cd5 handleListOffsetRequestV1AndAbove: \u63d0\u4f9b fetchOffsetForTimestamp \u65b9\u6cd5\uff0cbroker \u5904\u7406 ListOffsetRequestV1 \u8bf7\u6c42\u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 fetchOffsetForTimestamp \u65b9\u6cd5","title":"1.4 ReplicaManager"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/1-replica%20manager%20%E5%90%AF%E5%8A%A8/","text":"1.4.1 replica manager \u542f\u52a8 isr shrik \u548c isr expand \u90fd\u662f leader \u53d1\u8d77\u7684\u3002 leader \u6709\u5b9a\u65f6\u4efb\u52a1\u68c0\u67e5 isr \u6bcf\u4e2a replica \u662f\u5426\u53d1\u751f\u843d\u540e\uff0creplica fetch \u8bf7\u6c42\u5230\u6765\u65f6\uff0c\u68c0\u67e5\u662f\u5426\u8981\u5c06\u8be5 replica \u52a0\u5165 isr\u3002 zk \u4e0a\u8be5 partition \u7684 state \u4fe1\u606f\u4e5f\u7531 leader \u66f4\u65b0\u3002 replica manager \u5728 kafkaServer.startup() \u4e2d\u521d\u59cb\u5316\u5e76\u542f\u52a8\uff1a /** * Start up API for bringing up a single instance of the Kafka server. * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers */ def startup (): Unit = { try { info ( \"starting\" ) if ( isShuttingDown . get ) throw new IllegalStateException ( \"Kafka server is still shutting down, cannot re-start!\" ) if ( startupComplete . get ) return val canStartup = isStartingUp . compareAndSet ( false , true ) if ( canStartup ) { // ... /* start replica manager */ brokerToControllerChannelManager = new BrokerToControllerChannelManagerImpl ( metadataCache , time , metrics , config , threadNamePrefix ) replicaManager = createReplicaManager ( isShuttingDown ) replicaManager . startup () brokerToControllerChannelManager . start () // ... protected def createReplicaManager ( isShuttingDown : AtomicBoolean ): ReplicaManager = { val alterIsrManager = new AlterIsrManagerImpl ( brokerToControllerChannelManager , kafkaScheduler , time , config . brokerId , () => kafkaController . brokerEpoch ) new ReplicaManager ( config , metrics , time , zkClient , kafkaScheduler , logManager , isShuttingDown , quotaManagers , brokerTopicStats , metadataCache , logDirFailureChannel , alterIsrManager ) } ReplicaManager.startup() \u65b9\u6cd5\u4f1a\u542f\u52a8\u4ee5\u4e0b\u5b9a\u65f6\u4efb\u52a1\uff1a 1. isr-expiration\uff1a\u5b9a\u65f6\u68c0\u67e5 isr \u5217\u8868\u4e2d\u662f\u5426\u6709 replica \u9700\u8981\u88ab\u79fb\u9664\uff0c\u8fd9\u91cc\u4f1a\u5bf9 isr \u53d8\u5316\u7684 topicPartition \u8fdb\u884c\u8bb0\u5f55\uff0c\u5206\u4e3a 2 \u79cd\u60c5\u51b5\uff1a 1. \u5982\u679c api version >= KAFKA_2_7_IV2\uff0c\u8bb0\u5f55\u5728 alterIsrManager \u7684 unsentIsrUpdates \u4e2d 2. \u5426\u5219\u8bb0\u5f55\u5728\u81ea\u8eab\u7684 isrChangeSet \u4e2d 2. isr \u7684\u53d8\u5316\u9700\u8981\u901a\u77e5\u5230 controller\uff0c\u4e0a\u4e00\u6b65\u7684\u5b9a\u65f6\u4efb\u52a1\u53ea\u662f\u8bb0\u5f55\uff0c\u8fd9\u91cc\u4f1a\u6839\u636e\u96c6\u7fa4\u652f\u6301\u7684 API version \u5206\u522b\u901a\u8fc7 AlterIsrRequest \u8bf7\u6c42\u6216\u8005 zookeeper \u5411 controller \u8fdb\u884c\u901a\u77e5: 1. send-alter-isr: \u5982\u679c api version >= KAFKA_2_7_IV2\uff0c\u542f\u52a8 alterIsrManager\uff0calterIsrManager \u540c\u6837\u542f\u52a8\u5b9a\u65f6\u4efb\u52a1\uff0c\u5b9a\u671f\u5411 controller \u53d1\u9001\u672a\u5b8c\u6210\u7684 AlterIsrRequest \u8bf7\u6c42 2. isr-change-propagation: \u5426\u5219\u5b9a\u65f6\u68c0\u67e5 isr \u662f\u5426\u9700\u8981\u53d8\u52a8\uff0c\u521b\u5efa /isr_change_notification/isr_change_<> zookeeper \u8282\u70b9\uff0c\u89e6\u53d1 controller \u52a8\u4f5c 3. shutdown-idle-replica-alter-log-dirs-thread def startup (): Unit = { // start ISR expiration thread // A follower can lag behind leader for up to config.replicaLagTimeMaxMs x 1.5 before it is removed from ISR scheduler . schedule ( \"isr-expiration\" , maybeShrinkIsr _ , period = config . replicaLagTimeMaxMs / 2 , unit = TimeUnit . MILLISECONDS ) // If using AlterIsr, we don't need the znode ISR propagation if ( ! config . interBrokerProtocolVersion . isAlterIsrSupported ) { scheduler . schedule ( \"isr-change-propagation\" , maybePropagateIsrChanges _ , period = isrChangeNotificationConfig . checkIntervalMs , unit = TimeUnit . MILLISECONDS ) } else { alterIsrManager . start () } scheduler . schedule ( \"shutdown-idle-replica-alter-log-dirs-thread\" , shutdownIdleReplicaAlterLogDirsThread _ , period = 10000L , unit = TimeUnit . MILLISECONDS ) // If inter-broker protocol (IBP) < 1.0, the controller will send LeaderAndIsrRequest V0 which does not include isNew field. // In this case, the broker receiving the request cannot determine whether it is safe to create a partition if a log directory has failed. // Thus, we choose to halt the broker on any log diretory failure if IBP < 1.0 val haltBrokerOnFailure = config . interBrokerProtocolVersion < KAFKA_1_0_IV0 logDirFailureHandler = new LogDirFailureHandler ( \"LogDirFailureHandler\" , haltBrokerOnFailure ) logDirFailureHandler . start () } isr shrink \u5b9a\u65f6\u4efb\u52a1 maybeShrinkIsr \u4f1a\u904d\u5386 topic partition\uff0c\u5bf9 partition \u5b9e\u4f8b\u8c03\u7528\u5176 maybeShrinkIsr() \u65b9\u6cd5 private def maybeShrinkIsr (): Unit = { trace ( \"Evaluating ISR list of partitions to see which replicas can be removed from the ISR\" ) // Shrink ISRs for non offline partitions allPartitions . keys . foreach { topicPartition => nonOfflinePartition ( topicPartition ). foreach ( _ . maybeShrinkIsr ()) } } Partition.maybeShrinkIsr() \u5b9a\u4e49\u5982\u4e0b def maybeShrinkIsr (): Unit = { val needsIsrUpdate = ! isrState . isInflight && inReadLock ( leaderIsrUpdateLock ) { needsShrinkIsr () } val leaderHWIncremented = needsIsrUpdate && inWriteLock ( leaderIsrUpdateLock ) { leaderLogIfLocal . exists { leaderLog => val outOfSyncReplicaIds = getOutOfSyncReplicas ( replicaLagTimeMaxMs ) if ( outOfSyncReplicaIds . nonEmpty ) { val outOfSyncReplicaLog = outOfSyncReplicaIds . map { replicaId => s\"(brokerId: $ replicaId , endOffset: ${ getReplicaOrException ( replicaId ). logEndOffset } )\" }. mkString ( \" \" ) val newIsrLog = ( isrState . isr -- outOfSyncReplicaIds ). mkString ( \",\" ) info ( s\"Shrinking ISR from ${ isrState . isr . mkString ( \",\" ) } to $ newIsrLog . \" + s\"Leader: (highWatermark: ${ leaderLog . highWatermark } , endOffset: ${ leaderLog . logEndOffset } ). \" + s\"Out of sync replicas: $ outOfSyncReplicaLog .\" ) shrinkIsr ( outOfSyncReplicaIds ) // we may need to increment high watermark since ISR could be down to 1 maybeIncrementLeaderHW ( leaderLog ) } else { false } } } // some delayed operations may be unblocked after HW changed if ( leaderHWIncremented ) tryCompleteDelayedRequests () } Partition.shrinkIsr() private [ cluster ] def shrinkIsr ( outOfSyncReplicas : Set [ Int ]): Unit = { if ( useAlterIsr ) { shrinkIsrWithAlterIsr ( outOfSyncReplicas ) } else { shrinkIsrWithZk ( isrState . isr -- outOfSyncReplicas ) } } private def shrinkIsrWithAlterIsr ( outOfSyncReplicas : Set [ Int ]): Unit = { // This is called from maybeShrinkIsr which holds the ISR write lock if ( ! isrState . isInflight ) { // When shrinking the ISR, we cannot assume that the update will succeed as this could erroneously advance the HW // We update pendingInSyncReplicaIds here simply to prevent any further ISR updates from occurring until we get // the next LeaderAndIsr sendAlterIsrRequest ( PendingShrinkIsr ( isrState . isr , outOfSyncReplicas )) } else { trace ( s\"ISR update in-flight, not removing out-of-sync replicas $ outOfSyncReplicas \" ) } } private def shrinkIsrWithZk ( newIsr : Set [ Int ]): Unit = { val newLeaderAndIsr = new LeaderAndIsr ( localBrokerId , leaderEpoch , newIsr . toList , zkVersion ) val zkVersionOpt = stateStore . shrinkIsr ( controllerEpoch , newLeaderAndIsr ) if ( zkVersionOpt . isDefined ) { isrChangeListener . markShrink () } maybeUpdateIsrAndVersionWithZk ( newIsr , zkVersionOpt ) } \u8fd9\u91cc\u4e5f\u5206 2 \u79cd\u60c5\u51b5\uff1a 1. \u5982\u679c api version >= KAFKA_2_7_IV2\uff0c\u5373 useAlterIsr \uff0c\u5411 alterIsrManager \u589e\u52a0 AlterIsrRequest \u8bb0\u5f55 2. \u5426\u5219\uff0c\u5728 ReplicaManager \u7684 isrChangeSet \u4e2d\u8bb0\u5f55 isr \u53d8\u5316\u7684 topicPartition isr expand / fetchMessages topic partition leader \u6240\u5728\u8282\u70b9\uff0c\u9700\u8981\u8bb0\u5f55\u6bcf\u4e2a replica \u7684 logEndOffset\uff0c\u6bcf\u4e2a replica \u90fd\u8981\u8bb0\u5f55\u81ea\u8eab\u7684 HW /** * Fetch messages from a replica, and wait until enough data can be fetched and return; * the callback function will be triggered either when timeout or required fetch info is satisfied. * Consumers may fetch from any replica, but followers can only fetch from the leader. */ def fetchMessages ( timeout : Long , replicaId : Int , fetchMinBytes : Int , fetchMaxBytes : Int , hardMaxBytesLimit : Boolean , fetchInfos : Seq [( TopicPartition , PartitionData )], quota : ReplicaQuota , responseCallback : Seq [( TopicPartition , FetchPartitionData )] => Unit , isolationLevel : IsolationLevel , clientMetadata : Option [ ClientMetadata ]): Unit = { val isFromFollower = Request . isValidBrokerId ( replicaId ) val isFromConsumer = ! ( isFromFollower || replicaId == Request . FutureLocalReplicaId ) val fetchIsolation = if ( ! isFromConsumer ) FetchLogEnd else if ( isolationLevel == IsolationLevel . READ_COMMITTED ) FetchTxnCommitted else FetchHighWatermark // Restrict fetching to leader if request is from follower or from a client with older version (no ClientMetadata) val fetchOnlyFromLeader = isFromFollower || ( isFromConsumer && clientMetadata . isEmpty ) def readFromLog (): Seq [( TopicPartition , LogReadResult )] = { val result = readFromLocalLog ( replicaId = replicaId , fetchOnlyFromLeader = fetchOnlyFromLeader , fetchIsolation = fetchIsolation , fetchMaxBytes = fetchMaxBytes , hardMaxBytesLimit = hardMaxBytesLimit , readPartitionInfo = fetchInfos , quota = quota , clientMetadata = clientMetadata ) if ( isFromFollower ) updateFollowerFetchState ( replicaId , result ) else result } val logReadResults = readFromLog () // check if this fetch request can be satisfied right away var bytesReadable : Long = 0 var errorReadingData = false var hasDivergingEpoch = false val logReadResultMap = new mutable . HashMap [ TopicPartition , LogReadResult ] logReadResults . foreach { case ( topicPartition , logReadResult ) => brokerTopicStats . topicStats ( topicPartition . topic ). totalFetchRequestRate . mark () brokerTopicStats . allTopicsStats . totalFetchRequestRate . mark () if ( logReadResult . error != Errors . NONE ) errorReadingData = true if ( logReadResult . divergingEpoch . nonEmpty ) hasDivergingEpoch = true bytesReadable = bytesReadable + logReadResult . info . records . sizeInBytes logReadResultMap . put ( topicPartition , logReadResult ) } // respond immediately if 1) fetch request does not want to wait // 2) fetch request does not require any data // 3) has enough data to respond // 4) some error happens while reading data // 5) we found a diverging epoch if ( timeout <= 0 || fetchInfos . isEmpty || bytesReadable >= fetchMinBytes || errorReadingData || hasDivergingEpoch ) { val fetchPartitionData = logReadResults . map { case ( tp , result ) => val isReassignmentFetch = isFromFollower && isAddingReplica ( tp , replicaId ) tp -> FetchPartitionData ( result . error , result . highWatermark , result . leaderLogStartOffset , result . info . records , result . divergingEpoch , result . lastStableOffset , result . info . abortedTransactions , result . preferredReadReplica , isReassignmentFetch ) } responseCallback ( fetchPartitionData ) } else { // construct the fetch results from the read results val fetchPartitionStatus = new mutable . ArrayBuffer [( TopicPartition , FetchPartitionStatus )] fetchInfos . foreach { case ( topicPartition , partitionData ) => logReadResultMap . get ( topicPartition ). foreach ( logReadResult => { val logOffsetMetadata = logReadResult . info . fetchOffsetMetadata fetchPartitionStatus += ( topicPartition -> FetchPartitionStatus ( logOffsetMetadata , partitionData )) }) } val fetchMetadata : SFetchMetadata = SFetchMetadata ( fetchMinBytes , fetchMaxBytes , hardMaxBytesLimit , fetchOnlyFromLeader , fetchIsolation , isFromFollower , replicaId , fetchPartitionStatus ) val delayedFetch = new DelayedFetch ( timeout , fetchMetadata , this , quota , clientMetadata , responseCallback ) // create a list of (topic, partition) pairs to use as keys for this delayed fetch operation val delayedFetchKeys = fetchPartitionStatus . map { case ( tp , _ ) => TopicPartitionOperationKey ( tp ) } // try to complete the request immediately, otherwise put it into the purgatory; // this is because while the delayed fetch operation is being created, new requests // may arrive and hence make this operation completable. delayedFetchPurgatory . tryCompleteElseWatch ( delayedFetch , delayedFetchKeys ) } } updateFollowerFetchState /** * Update the follower's fetch state on the leader based on the last fetch request and update `readResult`. * If the follower replica is not recognized to be one of the assigned replicas, do not update * `readResult` so that log start/end offset and high watermark is consistent with * records in fetch response. Log start/end offset and high watermark may change not only due to * this fetch request, e.g., rolling new log segment and removing old log segment may move log * start offset further than the last offset in the fetched records. The followers will get the * updated leader's state in the next fetch response. */ private def updateFollowerFetchState ( followerId : Int , readResults : Seq [( TopicPartition , LogReadResult )]): Seq [( TopicPartition , LogReadResult )] = { readResults . map { case ( topicPartition , readResult ) => val updatedReadResult = if ( readResult . error != Errors . NONE ) { debug ( s\"Skipping update of fetch state for follower $ followerId since the \" + s\"log read returned error ${ readResult . error } \" ) readResult } else { nonOfflinePartition ( topicPartition ) match { case Some ( partition ) => if ( partition . updateFollowerFetchState ( followerId , followerFetchOffsetMetadata = readResult . info . fetchOffsetMetadata , followerStartOffset = readResult . followerLogStartOffset , followerFetchTimeMs = readResult . fetchTimeMs , leaderEndOffset = readResult . leaderLogEndOffset )) { readResult } else { warn ( s\"Leader $ localBrokerId failed to record follower $ followerId 's position \" + s\" ${ readResult . info . fetchOffsetMetadata . messageOffset } , and last sent HW since the replica \" + s\"is not recognized to be one of the assigned replicas ${ partition . assignmentState . replicas . mkString ( \",\" ) } \" + s\"for partition $ topicPartition . Empty records will be returned for this partition.\" ) readResult . withEmptyFetchInfo } case None => warn ( s\"While recording the replica LEO, the partition $ topicPartition hasn't been created.\" ) readResult } } topicPartition -> updatedReadResult } }","title":"1.4.1 ReplicaManager \u542f\u52a8"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/1-replica%20manager%20%E5%90%AF%E5%8A%A8/#141-replica-manager","text":"isr shrik \u548c isr expand \u90fd\u662f leader \u53d1\u8d77\u7684\u3002 leader \u6709\u5b9a\u65f6\u4efb\u52a1\u68c0\u67e5 isr \u6bcf\u4e2a replica \u662f\u5426\u53d1\u751f\u843d\u540e\uff0creplica fetch \u8bf7\u6c42\u5230\u6765\u65f6\uff0c\u68c0\u67e5\u662f\u5426\u8981\u5c06\u8be5 replica \u52a0\u5165 isr\u3002 zk \u4e0a\u8be5 partition \u7684 state \u4fe1\u606f\u4e5f\u7531 leader \u66f4\u65b0\u3002 replica manager \u5728 kafkaServer.startup() \u4e2d\u521d\u59cb\u5316\u5e76\u542f\u52a8\uff1a /** * Start up API for bringing up a single instance of the Kafka server. * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers */ def startup (): Unit = { try { info ( \"starting\" ) if ( isShuttingDown . get ) throw new IllegalStateException ( \"Kafka server is still shutting down, cannot re-start!\" ) if ( startupComplete . get ) return val canStartup = isStartingUp . compareAndSet ( false , true ) if ( canStartup ) { // ... /* start replica manager */ brokerToControllerChannelManager = new BrokerToControllerChannelManagerImpl ( metadataCache , time , metrics , config , threadNamePrefix ) replicaManager = createReplicaManager ( isShuttingDown ) replicaManager . startup () brokerToControllerChannelManager . start () // ... protected def createReplicaManager ( isShuttingDown : AtomicBoolean ): ReplicaManager = { val alterIsrManager = new AlterIsrManagerImpl ( brokerToControllerChannelManager , kafkaScheduler , time , config . brokerId , () => kafkaController . brokerEpoch ) new ReplicaManager ( config , metrics , time , zkClient , kafkaScheduler , logManager , isShuttingDown , quotaManagers , brokerTopicStats , metadataCache , logDirFailureChannel , alterIsrManager ) } ReplicaManager.startup() \u65b9\u6cd5\u4f1a\u542f\u52a8\u4ee5\u4e0b\u5b9a\u65f6\u4efb\u52a1\uff1a 1. isr-expiration\uff1a\u5b9a\u65f6\u68c0\u67e5 isr \u5217\u8868\u4e2d\u662f\u5426\u6709 replica \u9700\u8981\u88ab\u79fb\u9664\uff0c\u8fd9\u91cc\u4f1a\u5bf9 isr \u53d8\u5316\u7684 topicPartition \u8fdb\u884c\u8bb0\u5f55\uff0c\u5206\u4e3a 2 \u79cd\u60c5\u51b5\uff1a 1. \u5982\u679c api version >= KAFKA_2_7_IV2\uff0c\u8bb0\u5f55\u5728 alterIsrManager \u7684 unsentIsrUpdates \u4e2d 2. \u5426\u5219\u8bb0\u5f55\u5728\u81ea\u8eab\u7684 isrChangeSet \u4e2d 2. isr \u7684\u53d8\u5316\u9700\u8981\u901a\u77e5\u5230 controller\uff0c\u4e0a\u4e00\u6b65\u7684\u5b9a\u65f6\u4efb\u52a1\u53ea\u662f\u8bb0\u5f55\uff0c\u8fd9\u91cc\u4f1a\u6839\u636e\u96c6\u7fa4\u652f\u6301\u7684 API version \u5206\u522b\u901a\u8fc7 AlterIsrRequest \u8bf7\u6c42\u6216\u8005 zookeeper \u5411 controller \u8fdb\u884c\u901a\u77e5: 1. send-alter-isr: \u5982\u679c api version >= KAFKA_2_7_IV2\uff0c\u542f\u52a8 alterIsrManager\uff0calterIsrManager \u540c\u6837\u542f\u52a8\u5b9a\u65f6\u4efb\u52a1\uff0c\u5b9a\u671f\u5411 controller \u53d1\u9001\u672a\u5b8c\u6210\u7684 AlterIsrRequest \u8bf7\u6c42 2. isr-change-propagation: \u5426\u5219\u5b9a\u65f6\u68c0\u67e5 isr \u662f\u5426\u9700\u8981\u53d8\u52a8\uff0c\u521b\u5efa /isr_change_notification/isr_change_<> zookeeper \u8282\u70b9\uff0c\u89e6\u53d1 controller \u52a8\u4f5c 3. shutdown-idle-replica-alter-log-dirs-thread def startup (): Unit = { // start ISR expiration thread // A follower can lag behind leader for up to config.replicaLagTimeMaxMs x 1.5 before it is removed from ISR scheduler . schedule ( \"isr-expiration\" , maybeShrinkIsr _ , period = config . replicaLagTimeMaxMs / 2 , unit = TimeUnit . MILLISECONDS ) // If using AlterIsr, we don't need the znode ISR propagation if ( ! config . interBrokerProtocolVersion . isAlterIsrSupported ) { scheduler . schedule ( \"isr-change-propagation\" , maybePropagateIsrChanges _ , period = isrChangeNotificationConfig . checkIntervalMs , unit = TimeUnit . MILLISECONDS ) } else { alterIsrManager . start () } scheduler . schedule ( \"shutdown-idle-replica-alter-log-dirs-thread\" , shutdownIdleReplicaAlterLogDirsThread _ , period = 10000L , unit = TimeUnit . MILLISECONDS ) // If inter-broker protocol (IBP) < 1.0, the controller will send LeaderAndIsrRequest V0 which does not include isNew field. // In this case, the broker receiving the request cannot determine whether it is safe to create a partition if a log directory has failed. // Thus, we choose to halt the broker on any log diretory failure if IBP < 1.0 val haltBrokerOnFailure = config . interBrokerProtocolVersion < KAFKA_1_0_IV0 logDirFailureHandler = new LogDirFailureHandler ( \"LogDirFailureHandler\" , haltBrokerOnFailure ) logDirFailureHandler . start () }","title":"1.4.1 replica manager \u542f\u52a8"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/1-replica%20manager%20%E5%90%AF%E5%8A%A8/#isr-shrink","text":"\u5b9a\u65f6\u4efb\u52a1 maybeShrinkIsr \u4f1a\u904d\u5386 topic partition\uff0c\u5bf9 partition \u5b9e\u4f8b\u8c03\u7528\u5176 maybeShrinkIsr() \u65b9\u6cd5 private def maybeShrinkIsr (): Unit = { trace ( \"Evaluating ISR list of partitions to see which replicas can be removed from the ISR\" ) // Shrink ISRs for non offline partitions allPartitions . keys . foreach { topicPartition => nonOfflinePartition ( topicPartition ). foreach ( _ . maybeShrinkIsr ()) } } Partition.maybeShrinkIsr() \u5b9a\u4e49\u5982\u4e0b def maybeShrinkIsr (): Unit = { val needsIsrUpdate = ! isrState . isInflight && inReadLock ( leaderIsrUpdateLock ) { needsShrinkIsr () } val leaderHWIncremented = needsIsrUpdate && inWriteLock ( leaderIsrUpdateLock ) { leaderLogIfLocal . exists { leaderLog => val outOfSyncReplicaIds = getOutOfSyncReplicas ( replicaLagTimeMaxMs ) if ( outOfSyncReplicaIds . nonEmpty ) { val outOfSyncReplicaLog = outOfSyncReplicaIds . map { replicaId => s\"(brokerId: $ replicaId , endOffset: ${ getReplicaOrException ( replicaId ). logEndOffset } )\" }. mkString ( \" \" ) val newIsrLog = ( isrState . isr -- outOfSyncReplicaIds ). mkString ( \",\" ) info ( s\"Shrinking ISR from ${ isrState . isr . mkString ( \",\" ) } to $ newIsrLog . \" + s\"Leader: (highWatermark: ${ leaderLog . highWatermark } , endOffset: ${ leaderLog . logEndOffset } ). \" + s\"Out of sync replicas: $ outOfSyncReplicaLog .\" ) shrinkIsr ( outOfSyncReplicaIds ) // we may need to increment high watermark since ISR could be down to 1 maybeIncrementLeaderHW ( leaderLog ) } else { false } } } // some delayed operations may be unblocked after HW changed if ( leaderHWIncremented ) tryCompleteDelayedRequests () } Partition.shrinkIsr() private [ cluster ] def shrinkIsr ( outOfSyncReplicas : Set [ Int ]): Unit = { if ( useAlterIsr ) { shrinkIsrWithAlterIsr ( outOfSyncReplicas ) } else { shrinkIsrWithZk ( isrState . isr -- outOfSyncReplicas ) } } private def shrinkIsrWithAlterIsr ( outOfSyncReplicas : Set [ Int ]): Unit = { // This is called from maybeShrinkIsr which holds the ISR write lock if ( ! isrState . isInflight ) { // When shrinking the ISR, we cannot assume that the update will succeed as this could erroneously advance the HW // We update pendingInSyncReplicaIds here simply to prevent any further ISR updates from occurring until we get // the next LeaderAndIsr sendAlterIsrRequest ( PendingShrinkIsr ( isrState . isr , outOfSyncReplicas )) } else { trace ( s\"ISR update in-flight, not removing out-of-sync replicas $ outOfSyncReplicas \" ) } } private def shrinkIsrWithZk ( newIsr : Set [ Int ]): Unit = { val newLeaderAndIsr = new LeaderAndIsr ( localBrokerId , leaderEpoch , newIsr . toList , zkVersion ) val zkVersionOpt = stateStore . shrinkIsr ( controllerEpoch , newLeaderAndIsr ) if ( zkVersionOpt . isDefined ) { isrChangeListener . markShrink () } maybeUpdateIsrAndVersionWithZk ( newIsr , zkVersionOpt ) } \u8fd9\u91cc\u4e5f\u5206 2 \u79cd\u60c5\u51b5\uff1a 1. \u5982\u679c api version >= KAFKA_2_7_IV2\uff0c\u5373 useAlterIsr \uff0c\u5411 alterIsrManager \u589e\u52a0 AlterIsrRequest \u8bb0\u5f55 2. \u5426\u5219\uff0c\u5728 ReplicaManager \u7684 isrChangeSet \u4e2d\u8bb0\u5f55 isr \u53d8\u5316\u7684 topicPartition","title":"isr shrink"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/1-replica%20manager%20%E5%90%AF%E5%8A%A8/#isr-expand-fetchmessages","text":"topic partition leader \u6240\u5728\u8282\u70b9\uff0c\u9700\u8981\u8bb0\u5f55\u6bcf\u4e2a replica \u7684 logEndOffset\uff0c\u6bcf\u4e2a replica \u90fd\u8981\u8bb0\u5f55\u81ea\u8eab\u7684 HW /** * Fetch messages from a replica, and wait until enough data can be fetched and return; * the callback function will be triggered either when timeout or required fetch info is satisfied. * Consumers may fetch from any replica, but followers can only fetch from the leader. */ def fetchMessages ( timeout : Long , replicaId : Int , fetchMinBytes : Int , fetchMaxBytes : Int , hardMaxBytesLimit : Boolean , fetchInfos : Seq [( TopicPartition , PartitionData )], quota : ReplicaQuota , responseCallback : Seq [( TopicPartition , FetchPartitionData )] => Unit , isolationLevel : IsolationLevel , clientMetadata : Option [ ClientMetadata ]): Unit = { val isFromFollower = Request . isValidBrokerId ( replicaId ) val isFromConsumer = ! ( isFromFollower || replicaId == Request . FutureLocalReplicaId ) val fetchIsolation = if ( ! isFromConsumer ) FetchLogEnd else if ( isolationLevel == IsolationLevel . READ_COMMITTED ) FetchTxnCommitted else FetchHighWatermark // Restrict fetching to leader if request is from follower or from a client with older version (no ClientMetadata) val fetchOnlyFromLeader = isFromFollower || ( isFromConsumer && clientMetadata . isEmpty ) def readFromLog (): Seq [( TopicPartition , LogReadResult )] = { val result = readFromLocalLog ( replicaId = replicaId , fetchOnlyFromLeader = fetchOnlyFromLeader , fetchIsolation = fetchIsolation , fetchMaxBytes = fetchMaxBytes , hardMaxBytesLimit = hardMaxBytesLimit , readPartitionInfo = fetchInfos , quota = quota , clientMetadata = clientMetadata ) if ( isFromFollower ) updateFollowerFetchState ( replicaId , result ) else result } val logReadResults = readFromLog () // check if this fetch request can be satisfied right away var bytesReadable : Long = 0 var errorReadingData = false var hasDivergingEpoch = false val logReadResultMap = new mutable . HashMap [ TopicPartition , LogReadResult ] logReadResults . foreach { case ( topicPartition , logReadResult ) => brokerTopicStats . topicStats ( topicPartition . topic ). totalFetchRequestRate . mark () brokerTopicStats . allTopicsStats . totalFetchRequestRate . mark () if ( logReadResult . error != Errors . NONE ) errorReadingData = true if ( logReadResult . divergingEpoch . nonEmpty ) hasDivergingEpoch = true bytesReadable = bytesReadable + logReadResult . info . records . sizeInBytes logReadResultMap . put ( topicPartition , logReadResult ) } // respond immediately if 1) fetch request does not want to wait // 2) fetch request does not require any data // 3) has enough data to respond // 4) some error happens while reading data // 5) we found a diverging epoch if ( timeout <= 0 || fetchInfos . isEmpty || bytesReadable >= fetchMinBytes || errorReadingData || hasDivergingEpoch ) { val fetchPartitionData = logReadResults . map { case ( tp , result ) => val isReassignmentFetch = isFromFollower && isAddingReplica ( tp , replicaId ) tp -> FetchPartitionData ( result . error , result . highWatermark , result . leaderLogStartOffset , result . info . records , result . divergingEpoch , result . lastStableOffset , result . info . abortedTransactions , result . preferredReadReplica , isReassignmentFetch ) } responseCallback ( fetchPartitionData ) } else { // construct the fetch results from the read results val fetchPartitionStatus = new mutable . ArrayBuffer [( TopicPartition , FetchPartitionStatus )] fetchInfos . foreach { case ( topicPartition , partitionData ) => logReadResultMap . get ( topicPartition ). foreach ( logReadResult => { val logOffsetMetadata = logReadResult . info . fetchOffsetMetadata fetchPartitionStatus += ( topicPartition -> FetchPartitionStatus ( logOffsetMetadata , partitionData )) }) } val fetchMetadata : SFetchMetadata = SFetchMetadata ( fetchMinBytes , fetchMaxBytes , hardMaxBytesLimit , fetchOnlyFromLeader , fetchIsolation , isFromFollower , replicaId , fetchPartitionStatus ) val delayedFetch = new DelayedFetch ( timeout , fetchMetadata , this , quota , clientMetadata , responseCallback ) // create a list of (topic, partition) pairs to use as keys for this delayed fetch operation val delayedFetchKeys = fetchPartitionStatus . map { case ( tp , _ ) => TopicPartitionOperationKey ( tp ) } // try to complete the request immediately, otherwise put it into the purgatory; // this is because while the delayed fetch operation is being created, new requests // may arrive and hence make this operation completable. delayedFetchPurgatory . tryCompleteElseWatch ( delayedFetch , delayedFetchKeys ) } } updateFollowerFetchState /** * Update the follower's fetch state on the leader based on the last fetch request and update `readResult`. * If the follower replica is not recognized to be one of the assigned replicas, do not update * `readResult` so that log start/end offset and high watermark is consistent with * records in fetch response. Log start/end offset and high watermark may change not only due to * this fetch request, e.g., rolling new log segment and removing old log segment may move log * start offset further than the last offset in the fetched records. The followers will get the * updated leader's state in the next fetch response. */ private def updateFollowerFetchState ( followerId : Int , readResults : Seq [( TopicPartition , LogReadResult )]): Seq [( TopicPartition , LogReadResult )] = { readResults . map { case ( topicPartition , readResult ) => val updatedReadResult = if ( readResult . error != Errors . NONE ) { debug ( s\"Skipping update of fetch state for follower $ followerId since the \" + s\"log read returned error ${ readResult . error } \" ) readResult } else { nonOfflinePartition ( topicPartition ) match { case Some ( partition ) => if ( partition . updateFollowerFetchState ( followerId , followerFetchOffsetMetadata = readResult . info . fetchOffsetMetadata , followerStartOffset = readResult . followerLogStartOffset , followerFetchTimeMs = readResult . fetchTimeMs , leaderEndOffset = readResult . leaderLogEndOffset )) { readResult } else { warn ( s\"Leader $ localBrokerId failed to record follower $ followerId 's position \" + s\" ${ readResult . info . fetchOffsetMetadata . messageOffset } , and last sent HW since the replica \" + s\"is not recognized to be one of the assigned replicas ${ partition . assignmentState . replicas . mkString ( \",\" ) } \" + s\"for partition $ topicPartition . Empty records will be returned for this partition.\" ) readResult . withEmptyFetchInfo } case None => warn ( s\"While recording the replica LEO, the partition $ topicPartition hasn't been created.\" ) readResult } } topicPartition -> updatedReadResult } }","title":"isr expand / fetchMessages"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/2-%E5%89%AF%E6%9C%AC%E8%90%BD%E5%90%8E/","text":"isr shrink \u526f\u672c\u6536\u7f29\u662f leader \u53d1\u8d77\u7684 def maybeShrinkIsr (): Unit = { val needsIsrUpdate = ! isrState . isInflight && inReadLock ( leaderIsrUpdateLock ) { needsShrinkIsr () } val leaderHWIncremented = needsIsrUpdate && inWriteLock ( leaderIsrUpdateLock ) { leaderLogIfLocal . exists { leaderLog => val outOfSyncReplicaIds = getOutOfSyncReplicas ( replicaLagTimeMaxMs ) if ( outOfSyncReplicaIds . nonEmpty ) { val outOfSyncReplicaLog = outOfSyncReplicaIds . map { replicaId => s\"(brokerId: $ replicaId , endOffset: ${ getReplicaOrException ( replicaId ). logEndOffset } )\" }. mkString ( \" \" ) val newIsrLog = ( isrState . isr -- outOfSyncReplicaIds ). mkString ( \",\" ) info ( s\"Shrinking ISR from ${ isrState . isr . mkString ( \",\" ) } to $ newIsrLog . \" + s\"Leader: (highWatermark: ${ leaderLog . highWatermark } , endOffset: ${ leaderLog . logEndOffset } ). \" + s\"Out of sync replicas: $ outOfSyncReplicaLog .\" ) shrinkIsr ( outOfSyncReplicaIds ) // we may need to increment high watermark since ISR could be down to 1 maybeIncrementLeaderHW ( leaderLog ) } else { false } } } // some delayed operations may be unblocked after HW changed if ( leaderHWIncremented ) tryCompleteDelayedRequests () } \u5f53\u524d\u8282\u70b9\u7684 partition \u662f leader\uff0c\u83b7\u53d6 partition \u5f53\u524d\u7684 isr follower\uff0c\u8fd4\u56de\u5df2\u7ecf\u843d\u540e\u7684 follower /** * If the follower already has the same leo as the leader, it will not be considered as out-of-sync, * otherwise there are two cases that will be handled here - * 1. Stuck followers: If the leo of the replica hasn't been updated for maxLagMs ms, * the follower is stuck and should be removed from the ISR * 2. Slow followers: If the replica has not read up to the leo within the last maxLagMs ms, * then the follower is lagging and should be removed from the ISR * Both these cases are handled by checking the lastCaughtUpTimeMs which represents * the last time when the replica was fully caught up. If either of the above conditions * is violated, that replica is considered to be out of sync * * If an ISR update is in-flight, we will return an empty set here **/ def getOutOfSyncReplicas ( maxLagMs : Long ): Set [ Int ] = { val current = isrState if ( ! current . isInflight ) { val candidateReplicaIds = current . isr - localBrokerId val currentTimeMs = time . milliseconds () val leaderEndOffset = localLogOrException . logEndOffset candidateReplicaIds . filter ( replicaId => isFollowerOutOfSync ( replicaId , leaderEndOffset , currentTimeMs , maxLagMs )) } else { Set . empty } } \u5982\u679c follower \u7684 leo \u4e0d\u7b49\u4e8e leader \u7684 leo\uff0c\u4e14 follower \u7684 lastCaughtUpTimeMs \u8ddd\u5f53\u524d\u65f6\u95f4\u8d85\u8fc7 replica.lag.time.max.ms \uff0c\u5219\u8ba4\u4e3a\u5176\u843d\u540e private def isFollowerOutOfSync ( replicaId : Int , leaderEndOffset : Long , currentTimeMs : Long , maxLagMs : Long ): Boolean = { val followerReplica = getReplicaOrException ( replicaId ) followerReplica . logEndOffset != leaderEndOffset && ( currentTimeMs - followerReplica . lastCaughtUpTimeMs ) > maxLagMs }","title":"isr shrink"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/2-%E5%89%AF%E6%9C%AC%E8%90%BD%E5%90%8E/#isr-shrink","text":"\u526f\u672c\u6536\u7f29\u662f leader \u53d1\u8d77\u7684 def maybeShrinkIsr (): Unit = { val needsIsrUpdate = ! isrState . isInflight && inReadLock ( leaderIsrUpdateLock ) { needsShrinkIsr () } val leaderHWIncremented = needsIsrUpdate && inWriteLock ( leaderIsrUpdateLock ) { leaderLogIfLocal . exists { leaderLog => val outOfSyncReplicaIds = getOutOfSyncReplicas ( replicaLagTimeMaxMs ) if ( outOfSyncReplicaIds . nonEmpty ) { val outOfSyncReplicaLog = outOfSyncReplicaIds . map { replicaId => s\"(brokerId: $ replicaId , endOffset: ${ getReplicaOrException ( replicaId ). logEndOffset } )\" }. mkString ( \" \" ) val newIsrLog = ( isrState . isr -- outOfSyncReplicaIds ). mkString ( \",\" ) info ( s\"Shrinking ISR from ${ isrState . isr . mkString ( \",\" ) } to $ newIsrLog . \" + s\"Leader: (highWatermark: ${ leaderLog . highWatermark } , endOffset: ${ leaderLog . logEndOffset } ). \" + s\"Out of sync replicas: $ outOfSyncReplicaLog .\" ) shrinkIsr ( outOfSyncReplicaIds ) // we may need to increment high watermark since ISR could be down to 1 maybeIncrementLeaderHW ( leaderLog ) } else { false } } } // some delayed operations may be unblocked after HW changed if ( leaderHWIncremented ) tryCompleteDelayedRequests () } \u5f53\u524d\u8282\u70b9\u7684 partition \u662f leader\uff0c\u83b7\u53d6 partition \u5f53\u524d\u7684 isr follower\uff0c\u8fd4\u56de\u5df2\u7ecf\u843d\u540e\u7684 follower /** * If the follower already has the same leo as the leader, it will not be considered as out-of-sync, * otherwise there are two cases that will be handled here - * 1. Stuck followers: If the leo of the replica hasn't been updated for maxLagMs ms, * the follower is stuck and should be removed from the ISR * 2. Slow followers: If the replica has not read up to the leo within the last maxLagMs ms, * then the follower is lagging and should be removed from the ISR * Both these cases are handled by checking the lastCaughtUpTimeMs which represents * the last time when the replica was fully caught up. If either of the above conditions * is violated, that replica is considered to be out of sync * * If an ISR update is in-flight, we will return an empty set here **/ def getOutOfSyncReplicas ( maxLagMs : Long ): Set [ Int ] = { val current = isrState if ( ! current . isInflight ) { val candidateReplicaIds = current . isr - localBrokerId val currentTimeMs = time . milliseconds () val leaderEndOffset = localLogOrException . logEndOffset candidateReplicaIds . filter ( replicaId => isFollowerOutOfSync ( replicaId , leaderEndOffset , currentTimeMs , maxLagMs )) } else { Set . empty } } \u5982\u679c follower \u7684 leo \u4e0d\u7b49\u4e8e leader \u7684 leo\uff0c\u4e14 follower \u7684 lastCaughtUpTimeMs \u8ddd\u5f53\u524d\u65f6\u95f4\u8d85\u8fc7 replica.lag.time.max.ms \uff0c\u5219\u8ba4\u4e3a\u5176\u843d\u540e private def isFollowerOutOfSync ( replicaId : Int , leaderEndOffset : Long , currentTimeMs : Long , maxLagMs : Long ): Boolean = { val followerReplica = getReplicaOrException ( replicaId ) followerReplica . logEndOffset != leaderEndOffset && ( currentTimeMs - followerReplica . lastCaughtUpTimeMs ) > maxLagMs }","title":"isr shrink"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/5-GroupCoordinator/","text":"1.5 GroupCoordinator kafka \u96c6\u7fa4\u6bcf\u4e2a broker \u90fd\u4f1a\u542f\u52a8\u4e00\u4e2a GroupCoordinator\uff0c\u8d1f\u8d23 consumer group member \u4e0e\u8ba2\u9605 topic partition \u7684\u5206\u914d\u4e0e offset \u7ba1\u7406\u3002 \u6bcf\u4e2a GroupCoordinator \u90fd\u4f1a\u8d1f\u8d23\u4e00\u7ec4 group\uff0c\u7531 group id \u51b3\u5b9a group \u8be5\u7531\u90a3\u4e2a GroupCoordinator \u8d1f\u8d23\uff0c\u5177\u4f53\u5206\u914d\u65b9\u6cd5\u53ef\u4ee5\u67e5\u770b handleFindCoordinatorRequest() \u65b9\u6cd5\u3002 \u5b9e\u9645\u662f\u901a\u8fc7 group id \u627e\u5230 __consumer_offsets \u8fd9\u4e2a topic \u5bf9\u5e94\u7684 partition\uff0c\u4ee5 partition leader \u6240\u5728\u8282\u70b9\u4f5c\u4e3a\u8be5 group id \u7684 GroupCoordinator\u3002 def partitionFor ( groupId : String ): Int = Utils . abs ( groupId . hashCode ) % groupMetadataTopicPartitionCount public static int abs ( int n ) { return ( n == Integer . MIN_VALUE ) ? 0 : Math . abs ( n ); } groupMetadataTopicPartitionCount \u662f offsets.topic.num.partitions \u53c2\u6570\u7684\u503c\uff0c\u662f __consumer_offsets partition \u7684\u6570\u91cf\uff0c\u9ed8\u8ba4\u662f 50","title":"1.5 GroupCoordinator"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/5-GroupCoordinator/#15-groupcoordinator","text":"kafka \u96c6\u7fa4\u6bcf\u4e2a broker \u90fd\u4f1a\u542f\u52a8\u4e00\u4e2a GroupCoordinator\uff0c\u8d1f\u8d23 consumer group member \u4e0e\u8ba2\u9605 topic partition \u7684\u5206\u914d\u4e0e offset \u7ba1\u7406\u3002 \u6bcf\u4e2a GroupCoordinator \u90fd\u4f1a\u8d1f\u8d23\u4e00\u7ec4 group\uff0c\u7531 group id \u51b3\u5b9a group \u8be5\u7531\u90a3\u4e2a GroupCoordinator \u8d1f\u8d23\uff0c\u5177\u4f53\u5206\u914d\u65b9\u6cd5\u53ef\u4ee5\u67e5\u770b handleFindCoordinatorRequest() \u65b9\u6cd5\u3002 \u5b9e\u9645\u662f\u901a\u8fc7 group id \u627e\u5230 __consumer_offsets \u8fd9\u4e2a topic \u5bf9\u5e94\u7684 partition\uff0c\u4ee5 partition leader \u6240\u5728\u8282\u70b9\u4f5c\u4e3a\u8be5 group id \u7684 GroupCoordinator\u3002 def partitionFor ( groupId : String ): Int = Utils . abs ( groupId . hashCode ) % groupMetadataTopicPartitionCount public static int abs ( int n ) { return ( n == Integer . MIN_VALUE ) ? 0 : Math . abs ( n ); } groupMetadataTopicPartitionCount \u662f offsets.topic.num.partitions \u53c2\u6570\u7684\u503c\uff0c\u662f __consumer_offsets partition \u7684\u6570\u91cf\uff0c\u9ed8\u8ba4\u662f 50","title":"1.5 GroupCoordinator"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/5-GroupCoordinator/1-coordinatorq%E5%90%AF%E5%8A%A8/","text":"1.5.1 coordinator \u542f\u52a8 coordinator \u5728 kafkaServer.startup() \u4e2d\u542f\u52a8\uff1a /* start group coordinator */ // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue groupCoordinator = GroupCoordinator ( config , zkClient , replicaManager , Time . SYSTEM , metrics ) groupCoordinator . startup () groupCoordinator \u7684 startup() \u65b9\u6cd5\u5982\u4e0b\uff1a /** * Startup logic executed at the same time when the server starts up. */ def startup ( enableMetadataExpiration : Boolean = true ): Unit = { info ( \"Starting up.\" ) groupManager . startup ( enableMetadataExpiration ) isActive . set ( true ) info ( \"Startup complete.\" ) } groupManager \u7684 startup() \u65b9\u6cd5\u5982\u4e0b\uff1a def startup ( enableMetadataExpiration : Boolean ): Unit = { scheduler . startup () if ( enableMetadataExpiration ) { scheduler . schedule ( name = \"delete-expired-group-metadata\" , fun = () => cleanupGroupMetadata (), period = config . offsetsRetentionCheckIntervalMs , unit = TimeUnit . MILLISECONDS ) } } \u5b9e\u9645\u4e0a\u542f\u52a8\u4e86\u4e00\u4e2a\u540e\u53f0\u7ebf\u7a0b\uff0c\u5e76\u4e14\u5b9a\u65f6\u4efb\u52a1\uff0c\u6bcf\u9694 offsets.retention.check.interval.ms \u6beb\u79d2\u6e05\u7406\u8d85\u8fc7 offsets.retention.minutes \u7684 offset cache","title":"1.5.1 coordinator \u542f\u52a8"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/5-GroupCoordinator/1-coordinatorq%E5%90%AF%E5%8A%A8/#151-coordinator","text":"coordinator \u5728 kafkaServer.startup() \u4e2d\u542f\u52a8\uff1a /* start group coordinator */ // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue groupCoordinator = GroupCoordinator ( config , zkClient , replicaManager , Time . SYSTEM , metrics ) groupCoordinator . startup () groupCoordinator \u7684 startup() \u65b9\u6cd5\u5982\u4e0b\uff1a /** * Startup logic executed at the same time when the server starts up. */ def startup ( enableMetadataExpiration : Boolean = true ): Unit = { info ( \"Starting up.\" ) groupManager . startup ( enableMetadataExpiration ) isActive . set ( true ) info ( \"Startup complete.\" ) } groupManager \u7684 startup() \u65b9\u6cd5\u5982\u4e0b\uff1a def startup ( enableMetadataExpiration : Boolean ): Unit = { scheduler . startup () if ( enableMetadataExpiration ) { scheduler . schedule ( name = \"delete-expired-group-metadata\" , fun = () => cleanupGroupMetadata (), period = config . offsetsRetentionCheckIntervalMs , unit = TimeUnit . MILLISECONDS ) } } \u5b9e\u9645\u4e0a\u542f\u52a8\u4e86\u4e00\u4e2a\u540e\u53f0\u7ebf\u7a0b\uff0c\u5e76\u4e14\u5b9a\u65f6\u4efb\u52a1\uff0c\u6bcf\u9694 offsets.retention.check.interval.ms \u6beb\u79d2\u6e05\u7406\u8d85\u8fc7 offsets.retention.minutes \u7684 offset cache","title":"1.5.1 coordinator \u542f\u52a8"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/5-GroupCoordinator/2-%E6%9B%B4%E6%96%B0%E6%95%B0%E6%8D%AE/","text":"1.5.2 \u66f4\u65b0\u6570\u636e GroupCoordinator \u6709 onElection() , onResignation() \u65b9\u6cd5 /** * Load cached state from the given partition and begin handling requests for groups which map to it. * * @param offsetTopicPartitionId The partition we are now leading */ def onElection ( offsetTopicPartitionId : Int ): Unit = { groupManager . scheduleLoadGroupAndOffsets ( offsetTopicPartitionId , onGroupLoaded ) } /** * Unload cached state for the given partition and stop handling requests for groups which map to it. * * @param offsetTopicPartitionId The partition we are no longer leading */ def onResignation ( offsetTopicPartitionId : Int ): Unit = { groupManager . removeGroupsForPartition ( offsetTopicPartitionId , onGroupUnloaded ) } \u4f1a\u8c03\u7528 GroupMetadataManager \u7684 scheduleLoadGroupAndOffsets() \u7684\u65b9\u6cd5\uff0c\u5f00\u59cb\u8c03\u5ea6 loadGroupsAndOffsets \u65b9\u6cd5\u3002 /** * Asynchronously read the partition from the offsets topic and populate the cache */ def scheduleLoadGroupAndOffsets ( offsetsPartition : Int , onGroupLoaded : GroupMetadata => Unit ): Unit = { val topicPartition = new TopicPartition ( Topic . GROUP_METADATA_TOPIC_NAME , offsetsPartition ) if ( addLoadingPartition ( offsetsPartition )) { info ( s\"Scheduling loading of offsets and group metadata from $ topicPartition \" ) val startTimeMs = time . milliseconds () scheduler . schedule ( topicPartition . toString , () => loadGroupsAndOffsets ( topicPartition , onGroupLoaded , startTimeMs )) } else { info ( s\"Already loading offsets and group metadata from $ topicPartition \" ) } } KafkaApis \u7684 handleLeaderAndIsrRequest \u65b9\u6cd5\u5728\u5904\u7406\u65f6\uff0c\u4f1a\u5728 leader \u5173\u7cfb\u53d8\u5316\u65f6\u5224\u65ad topic \u662f\u5426\u4e3a GROUP_METADATA_TOPIC_NAME\uff0c\u5982\u679c\u6210\u4e3a leader \u5219\u8c03\u7528 onElection \u65b9\u6cd5\uff0c\u5982\u679c\u6210\u4e3a follower \u5219\u8c03\u7528 onResignation \u65b9\u6cd5 def handleLeaderAndIsrRequest ( request : RequestChannel . Request ): Unit = { // ensureTopicExists is only for client facing requests // We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they // stop serving data to clients for the topic being deleted val correlationId = request . header . correlationId val leaderAndIsrRequest = request . body [ LeaderAndIsrRequest ] def onLeadershipChange ( updatedLeaders : Iterable [ Partition ], updatedFollowers : Iterable [ Partition ]): Unit = { // for each new leader or follower, call coordinator to handle consumer group migration. // this callback is invoked under the replica state change lock to ensure proper order of // leadership changes updatedLeaders . foreach { partition => if ( partition . topic == GROUP_METADATA_TOPIC_NAME ) groupCoordinator . onElection ( partition . partitionId ) else if ( partition . topic == TRANSACTION_STATE_TOPIC_NAME ) txnCoordinator . onElection ( partition . partitionId , partition . getLeaderEpoch ) } updatedFollowers . foreach { partition => if ( partition . topic == GROUP_METADATA_TOPIC_NAME ) groupCoordinator . onResignation ( partition . partitionId ) else if ( partition . topic == TRANSACTION_STATE_TOPIC_NAME ) txnCoordinator . onResignation ( partition . partitionId , Some ( partition . getLeaderEpoch )) } } authorizeClusterOperation ( request , CLUSTER_ACTION ) if ( isBrokerEpochStale ( leaderAndIsrRequest . brokerEpoch )) { // When the broker restarts very quickly, it is possible for this broker to receive request intended // for its previous generation so the broker should skip the stale request. info ( \"Received LeaderAndIsr request with broker epoch \" + s\" ${ leaderAndIsrRequest . brokerEpoch } smaller than the current broker epoch ${ controller . brokerEpoch } \" ) sendResponseExemptThrottle ( request , leaderAndIsrRequest . getErrorResponse ( 0 , Errors . STALE_BROKER_EPOCH . exception )) } else { val response = replicaManager . becomeLeaderOrFollower ( correlationId , leaderAndIsrRequest , onLeadershipChange ) sendResponseExemptThrottle ( request , response ) } }","title":"1.5.2 \u66f4\u65b0\u6570\u636e"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/5-GroupCoordinator/2-%E6%9B%B4%E6%96%B0%E6%95%B0%E6%8D%AE/#152","text":"GroupCoordinator \u6709 onElection() , onResignation() \u65b9\u6cd5 /** * Load cached state from the given partition and begin handling requests for groups which map to it. * * @param offsetTopicPartitionId The partition we are now leading */ def onElection ( offsetTopicPartitionId : Int ): Unit = { groupManager . scheduleLoadGroupAndOffsets ( offsetTopicPartitionId , onGroupLoaded ) } /** * Unload cached state for the given partition and stop handling requests for groups which map to it. * * @param offsetTopicPartitionId The partition we are no longer leading */ def onResignation ( offsetTopicPartitionId : Int ): Unit = { groupManager . removeGroupsForPartition ( offsetTopicPartitionId , onGroupUnloaded ) } \u4f1a\u8c03\u7528 GroupMetadataManager \u7684 scheduleLoadGroupAndOffsets() \u7684\u65b9\u6cd5\uff0c\u5f00\u59cb\u8c03\u5ea6 loadGroupsAndOffsets \u65b9\u6cd5\u3002 /** * Asynchronously read the partition from the offsets topic and populate the cache */ def scheduleLoadGroupAndOffsets ( offsetsPartition : Int , onGroupLoaded : GroupMetadata => Unit ): Unit = { val topicPartition = new TopicPartition ( Topic . GROUP_METADATA_TOPIC_NAME , offsetsPartition ) if ( addLoadingPartition ( offsetsPartition )) { info ( s\"Scheduling loading of offsets and group metadata from $ topicPartition \" ) val startTimeMs = time . milliseconds () scheduler . schedule ( topicPartition . toString , () => loadGroupsAndOffsets ( topicPartition , onGroupLoaded , startTimeMs )) } else { info ( s\"Already loading offsets and group metadata from $ topicPartition \" ) } } KafkaApis \u7684 handleLeaderAndIsrRequest \u65b9\u6cd5\u5728\u5904\u7406\u65f6\uff0c\u4f1a\u5728 leader \u5173\u7cfb\u53d8\u5316\u65f6\u5224\u65ad topic \u662f\u5426\u4e3a GROUP_METADATA_TOPIC_NAME\uff0c\u5982\u679c\u6210\u4e3a leader \u5219\u8c03\u7528 onElection \u65b9\u6cd5\uff0c\u5982\u679c\u6210\u4e3a follower \u5219\u8c03\u7528 onResignation \u65b9\u6cd5 def handleLeaderAndIsrRequest ( request : RequestChannel . Request ): Unit = { // ensureTopicExists is only for client facing requests // We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they // stop serving data to clients for the topic being deleted val correlationId = request . header . correlationId val leaderAndIsrRequest = request . body [ LeaderAndIsrRequest ] def onLeadershipChange ( updatedLeaders : Iterable [ Partition ], updatedFollowers : Iterable [ Partition ]): Unit = { // for each new leader or follower, call coordinator to handle consumer group migration. // this callback is invoked under the replica state change lock to ensure proper order of // leadership changes updatedLeaders . foreach { partition => if ( partition . topic == GROUP_METADATA_TOPIC_NAME ) groupCoordinator . onElection ( partition . partitionId ) else if ( partition . topic == TRANSACTION_STATE_TOPIC_NAME ) txnCoordinator . onElection ( partition . partitionId , partition . getLeaderEpoch ) } updatedFollowers . foreach { partition => if ( partition . topic == GROUP_METADATA_TOPIC_NAME ) groupCoordinator . onResignation ( partition . partitionId ) else if ( partition . topic == TRANSACTION_STATE_TOPIC_NAME ) txnCoordinator . onResignation ( partition . partitionId , Some ( partition . getLeaderEpoch )) } } authorizeClusterOperation ( request , CLUSTER_ACTION ) if ( isBrokerEpochStale ( leaderAndIsrRequest . brokerEpoch )) { // When the broker restarts very quickly, it is possible for this broker to receive request intended // for its previous generation so the broker should skip the stale request. info ( \"Received LeaderAndIsr request with broker epoch \" + s\" ${ leaderAndIsrRequest . brokerEpoch } smaller than the current broker epoch ${ controller . brokerEpoch } \" ) sendResponseExemptThrottle ( request , leaderAndIsrRequest . getErrorResponse ( 0 , Errors . STALE_BROKER_EPOCH . exception )) } else { val response = replicaManager . becomeLeaderOrFollower ( correlationId , leaderAndIsrRequest , onLeadershipChange ) sendResponseExemptThrottle ( request , response ) } }","title":"1.5.2 \u66f4\u65b0\u6570\u636e"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/","text":"1.6 LogManager","title":"1.6 LogManager"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/#16-logmanager","text":"","title":"1.6 LogManager"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/1-LogManager%20%E5%90%AF%E5%8A%A8/","text":"1.6.1 LogManager \u542f\u52a8 \u5728 kafkaServer.startup() \u4e2d\uff1a /* start log manager */ logManager = LogManager ( config , initialOfflineDirs , zkClient , brokerState , kafkaScheduler , time , brokerTopicStats , logDirFailureChannel ) logManager . startup () \u5b9e\u9645\u8c03\u7528 LogManager.apply() \u521b\u5efa LogManager \u5bf9\u8c61\uff1a object LogManager { val RecoveryPointCheckpointFile = \"recovery-point-offset-checkpoint\" val LogStartOffsetCheckpointFile = \"log-start-offset-checkpoint\" val ProducerIdExpirationCheckIntervalMs = 10 * 60 * 1000 def apply ( config : KafkaConfig , initialOfflineDirs : Seq [ String ], zkClient : KafkaZkClient , brokerState : BrokerState , kafkaScheduler : KafkaScheduler , time : Time , brokerTopicStats : BrokerTopicStats , logDirFailureChannel : LogDirFailureChannel ): LogManager = { val defaultProps = KafkaServer . copyKafkaConfigToLog ( config ) val defaultLogConfig = LogConfig ( defaultProps ) // read the log configurations from zookeeper val ( topicConfigs , failed ) = zkClient . getLogConfigs ( zkClient . getAllTopicsInCluster , defaultProps ) if ( ! failed . isEmpty ) throw failed . head . _2 val cleanerConfig = LogCleaner . cleanerConfig ( config ) new LogManager ( logDirs = config . logDirs . map ( new File ( _ ). getAbsoluteFile ), initialOfflineDirs = initialOfflineDirs . map ( new File ( _ ). getAbsoluteFile ), topicConfigs = topicConfigs , initialDefaultConfig = defaultLogConfig , cleanerConfig = cleanerConfig , recoveryThreadsPerDataDir = config . numRecoveryThreadsPerDataDir , flushCheckMs = config . logFlushSchedulerIntervalMs , flushRecoveryOffsetCheckpointMs = config . logFlushOffsetCheckpointIntervalMs , flushStartOffsetCheckpointMs = config . logFlushStartOffsetCheckpointIntervalMs , retentionCheckMs = config . logCleanupIntervalMs , maxPidExpirationMs = config . transactionIdExpirationMs , scheduler = kafkaScheduler , brokerState = brokerState , brokerTopicStats = brokerTopicStats , logDirFailureChannel = logDirFailureChannel , time = time ) } } LogManager \u8d1f\u8d23\uff1a * creation * retrieval * cleaning \u6240\u6709 read \u548c write \u64cd\u4f5c\u90fd\u4f1a\u7531\u5355\u72ec\u7684 Log \u5bf9\u8c61\u8d1f\u8d23 LogManager \u7ef4\u62a4\u4e00\u4e2a\u6216\u591a\u4e2a\u76ee\u5f55\uff0c\u65b0\u7684 log \u4f1a\u5728 log \u6700\u5c11\u7684\u76ee\u5f55\u4e2d\u521b\u5efa\uff0c\u6ca1\u6709\u5c1d\u8bd5\u79fb\u52a8\u5206\u533a\uff0c\u8003\u8651 size \u6216\u8005 I/O rate \u5747\u8861\u3002 \u4e00\u4e2a\u540e\u53f0\u7ebf\u7a0b\u901a\u8fc7\u5468\u671f\u56de\u6536 log segment \u5904\u7406 log retention LogManager.startup() \u542f\u52a8\u5b9e\u9645\u7684\u540e\u53f0\u7ebf\u7a0b\uff1a /** * Start the background threads to flush logs and do log cleanup */ def startup () { /* Schedule the cleanup task to delete old logs */ if ( scheduler != null ) { info ( \"Starting log cleanup with a period of %d ms.\" . format ( retentionCheckMs )) scheduler . schedule ( \"kafka-log-retention\" , cleanupLogs _ , delay = InitialTaskDelayMs , period = retentionCheckMs , TimeUnit . MILLISECONDS ) info ( \"Starting log flusher with a default period of %d ms.\" . format ( flushCheckMs )) scheduler . schedule ( \"kafka-log-flusher\" , flushDirtyLogs _ , delay = InitialTaskDelayMs , period = flushCheckMs , TimeUnit . MILLISECONDS ) scheduler . schedule ( \"kafka-recovery-point-checkpoint\" , checkpointLogRecoveryOffsets _ , delay = InitialTaskDelayMs , period = flushRecoveryOffsetCheckpointMs , TimeUnit . MILLISECONDS ) scheduler . schedule ( \"kafka-log-start-offset-checkpoint\" , checkpointLogStartOffsets _ , delay = InitialTaskDelayMs , period = flushStartOffsetCheckpointMs , TimeUnit . MILLISECONDS ) scheduler . schedule ( \"kafka-delete-logs\" , // will be rescheduled after each delete logs with a dynamic period deleteLogs _ , delay = InitialTaskDelayMs , unit = TimeUnit . MILLISECONDS ) } // \u5982\u679c\u8bbe\u7f6e\u4e3a true\uff0c\u81ea\u52a8\u6e05\u7406 compaction \u7c7b\u578b\u7684 topic if ( cleanerConfig . enableCleaner ) cleaner . startup () } loadLogs LogManager \u5b9e\u4f8b\u5316\u65f6\u4f1a\u901a\u8fc7 loadLogs() \u65b9\u6cd5\u5728\u914d\u7f6e\u7684 log.dirs \u76ee\u5f55\u4e2d\u52a0\u8f7d\u6240\u6709\u7684 log num.recovery.threads.per.data.dir","title":"1.6.1 LogManager \u542f\u52a8"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/1-LogManager%20%E5%90%AF%E5%8A%A8/#161-logmanager","text":"\u5728 kafkaServer.startup() \u4e2d\uff1a /* start log manager */ logManager = LogManager ( config , initialOfflineDirs , zkClient , brokerState , kafkaScheduler , time , brokerTopicStats , logDirFailureChannel ) logManager . startup () \u5b9e\u9645\u8c03\u7528 LogManager.apply() \u521b\u5efa LogManager \u5bf9\u8c61\uff1a object LogManager { val RecoveryPointCheckpointFile = \"recovery-point-offset-checkpoint\" val LogStartOffsetCheckpointFile = \"log-start-offset-checkpoint\" val ProducerIdExpirationCheckIntervalMs = 10 * 60 * 1000 def apply ( config : KafkaConfig , initialOfflineDirs : Seq [ String ], zkClient : KafkaZkClient , brokerState : BrokerState , kafkaScheduler : KafkaScheduler , time : Time , brokerTopicStats : BrokerTopicStats , logDirFailureChannel : LogDirFailureChannel ): LogManager = { val defaultProps = KafkaServer . copyKafkaConfigToLog ( config ) val defaultLogConfig = LogConfig ( defaultProps ) // read the log configurations from zookeeper val ( topicConfigs , failed ) = zkClient . getLogConfigs ( zkClient . getAllTopicsInCluster , defaultProps ) if ( ! failed . isEmpty ) throw failed . head . _2 val cleanerConfig = LogCleaner . cleanerConfig ( config ) new LogManager ( logDirs = config . logDirs . map ( new File ( _ ). getAbsoluteFile ), initialOfflineDirs = initialOfflineDirs . map ( new File ( _ ). getAbsoluteFile ), topicConfigs = topicConfigs , initialDefaultConfig = defaultLogConfig , cleanerConfig = cleanerConfig , recoveryThreadsPerDataDir = config . numRecoveryThreadsPerDataDir , flushCheckMs = config . logFlushSchedulerIntervalMs , flushRecoveryOffsetCheckpointMs = config . logFlushOffsetCheckpointIntervalMs , flushStartOffsetCheckpointMs = config . logFlushStartOffsetCheckpointIntervalMs , retentionCheckMs = config . logCleanupIntervalMs , maxPidExpirationMs = config . transactionIdExpirationMs , scheduler = kafkaScheduler , brokerState = brokerState , brokerTopicStats = brokerTopicStats , logDirFailureChannel = logDirFailureChannel , time = time ) } } LogManager \u8d1f\u8d23\uff1a * creation * retrieval * cleaning \u6240\u6709 read \u548c write \u64cd\u4f5c\u90fd\u4f1a\u7531\u5355\u72ec\u7684 Log \u5bf9\u8c61\u8d1f\u8d23 LogManager \u7ef4\u62a4\u4e00\u4e2a\u6216\u591a\u4e2a\u76ee\u5f55\uff0c\u65b0\u7684 log \u4f1a\u5728 log \u6700\u5c11\u7684\u76ee\u5f55\u4e2d\u521b\u5efa\uff0c\u6ca1\u6709\u5c1d\u8bd5\u79fb\u52a8\u5206\u533a\uff0c\u8003\u8651 size \u6216\u8005 I/O rate \u5747\u8861\u3002 \u4e00\u4e2a\u540e\u53f0\u7ebf\u7a0b\u901a\u8fc7\u5468\u671f\u56de\u6536 log segment \u5904\u7406 log retention LogManager.startup() \u542f\u52a8\u5b9e\u9645\u7684\u540e\u53f0\u7ebf\u7a0b\uff1a /** * Start the background threads to flush logs and do log cleanup */ def startup () { /* Schedule the cleanup task to delete old logs */ if ( scheduler != null ) { info ( \"Starting log cleanup with a period of %d ms.\" . format ( retentionCheckMs )) scheduler . schedule ( \"kafka-log-retention\" , cleanupLogs _ , delay = InitialTaskDelayMs , period = retentionCheckMs , TimeUnit . MILLISECONDS ) info ( \"Starting log flusher with a default period of %d ms.\" . format ( flushCheckMs )) scheduler . schedule ( \"kafka-log-flusher\" , flushDirtyLogs _ , delay = InitialTaskDelayMs , period = flushCheckMs , TimeUnit . MILLISECONDS ) scheduler . schedule ( \"kafka-recovery-point-checkpoint\" , checkpointLogRecoveryOffsets _ , delay = InitialTaskDelayMs , period = flushRecoveryOffsetCheckpointMs , TimeUnit . MILLISECONDS ) scheduler . schedule ( \"kafka-log-start-offset-checkpoint\" , checkpointLogStartOffsets _ , delay = InitialTaskDelayMs , period = flushStartOffsetCheckpointMs , TimeUnit . MILLISECONDS ) scheduler . schedule ( \"kafka-delete-logs\" , // will be rescheduled after each delete logs with a dynamic period deleteLogs _ , delay = InitialTaskDelayMs , unit = TimeUnit . MILLISECONDS ) } // \u5982\u679c\u8bbe\u7f6e\u4e3a true\uff0c\u81ea\u52a8\u6e05\u7406 compaction \u7c7b\u578b\u7684 topic if ( cleanerConfig . enableCleaner ) cleaner . startup () }","title":"1.6.1 LogManager \u542f\u52a8"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/1-LogManager%20%E5%90%AF%E5%8A%A8/#loadlogs","text":"LogManager \u5b9e\u4f8b\u5316\u65f6\u4f1a\u901a\u8fc7 loadLogs() \u65b9\u6cd5\u5728\u914d\u7f6e\u7684 log.dirs \u76ee\u5f55\u4e2d\u52a0\u8f7d\u6240\u6709\u7684 log num.recovery.threads.per.data.dir","title":"loadLogs"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/","text":"1.6.2 \u65e5\u5fd7\u6e05\u7406 \u65e5\u5fd7\u6e05\u7406\u7684\u7ebf\u7a0b\u5728 LogManager.startup() \u4e2d\u542f\u52a8\uff0c\u5177\u4f53\u4e3a\uff1a def startup () { /* Schedule the cleanup task to delete old logs */ if ( scheduler != null ) { info ( \"Starting log cleanup with a period of %d ms.\" . format ( retentionCheckMs )) scheduler . schedule ( \"kafka-log-retention\" , cleanupLogs _ , delay = InitialTaskDelayMs , period = retentionCheckMs , TimeUnit . MILLISECONDS ) // .... } } \u5468\u671f\u6027\u8fd0\u884c LogManager.cleanupLogs() \u65b9\u6cd5\uff1a /** * Delete any eligible logs. Return the number of segments deleted. * Only consider logs that are not compacted. */ def cleanupLogs (): Unit = { debug ( \"Beginning log cleanup...\" ) var total = 0 val startMs = time . milliseconds // clean current logs. val deletableLogs = { if ( cleaner != null ) { // prevent cleaner from working on same partitions when changing cleanup policy cleaner . pauseCleaningForNonCompactedPartitions () } else { currentLogs . filter { case ( _ , log ) => ! log . config . compact } } } try { deletableLogs . foreach { case ( topicPartition , log ) => debug ( s\"Garbage collecting ' ${ log . name } '\" ) total += log . deleteOldSegments () val futureLog = futureLogs . get ( topicPartition ) if ( futureLog != null ) { // clean future logs debug ( s\"Garbage collecting future log ' ${ futureLog . name } '\" ) total += futureLog . deleteOldSegments () } } } finally { if ( cleaner != null ) { cleaner . resumeCleaning ( deletableLogs . map ( _ . _1 )) } } debug ( s\"Log cleanup completed. $ total files deleted in \" + ( time . milliseconds - startMs ) / 1000 + \" seconds\" ) } \u8c03\u7528\u6bcf\u4e00\u4e2a log \u5b9e\u4f8b\u7684 Log.deleteOldSegments() \u65b9\u6cd5\uff1a /** * Delete any log segments that have either expired due to time based retention * or because the log size is > retentionSize */ def deleteOldSegments (): Int = { if ( ! config . delete ) return 0 deleteRetentionMsBreachedSegments () + deleteRetentionSizeBreachedSegments () + deleteLogStartOffsetBreachedSegments () } \u8fd9\u91cc\u5b9e\u9645\u8c03\u7528\u4e86 3 \u4e2a\u65b9\u6cd5\uff0c\u5206\u522b\u662f\uff1a * deleteRetentionMsBreachedSegments\uff0c\u6309\u4fdd\u7559\u65f6\u95f4\u6e05\u7406 * deleteRetentionSizeBreachedSegments\uff0c\u6309\u4fdd\u7559\u5927\u5c0f\u6e05\u7406 * deleteLogStartOffsetBreachedSegments deleteRetentionMsBreachedSegments Log.scala private def deleteRetentionMsBreachedSegments (): Int = { if ( config . retentionMs < 0 ) return 0 val startMs = time . milliseconds deleteOldSegments (( segment , _ ) => startMs - segment . largestTimestamp > config . retentionMs , reason = s\"retention time ${ config . retentionMs } ms breach\" ) } \u8fd9\u91cc\u5224\u65ad\u7684\u4f9d\u636e\u662f startMs - segment.largestTimestamp > config.retentionMs \u770b\u4e00\u4e0b largestTimestamp \u662f\u600e\u4e48\u5b9a\u4e49\u7684\uff1a LogSegment.scala /** * The last modified time of this log segment as a unix time stamp */ def lastModified = log . file . lastModified /** * The largest timestamp this segment contains, if maxTimestampSoFar >= 0, otherwise None. */ def largestRecordTimestamp : Option [ Long ] = if ( maxTimestampSoFar >= 0 ) Some ( maxTimestampSoFar ) else None /** * The largest timestamp this segment contains. */ def largestTimestamp = if ( maxTimestampSoFar >= 0 ) maxTimestampSoFar else lastModified deleteRetentionSizeBreachedSegments deleteLogStartOffsetBreachedSegments \u6536\u675f\u5230 deleteOldSegments \u4e0a\u8ff0 3 \u79cd\u60c5\u51b5\uff0c\u90fd\u4f1a\u6536\u675f\u5230 Log.deleteOldSegments() \uff0c\u4e0d\u540c\u7684\u60c5\u51b5\u53ea\u662f\u4f20\u4e0d\u540c\u7684 predicate \u5224\u65ad\u65b9\u6cd5\u3002 /** * Delete any log segments matching the given predicate function, * starting with the oldest segment and moving forward until a segment doesn't match. * * @param predicate A function that takes in a candidate log segment and the next higher segment * (if there is one) and returns true iff it is deletable * @return The number of segments deleted */ private def deleteOldSegments ( predicate : ( LogSegment , Option [ LogSegment ]) => Boolean , reason : String ): Int = { lock synchronized { val deletable = deletableSegments ( predicate ) if ( deletable . nonEmpty ) info ( s\"Found deletable segments with base offsets [ ${ deletable . map ( _ . baseOffset ). mkString ( \",\" ) } ] due to $ reason \" ) deleteSegments ( deletable ) } } \u904d\u5386\u6240\u6709 segment\uff0c\u7528 predicate \u5224\u65ad\u662f\u5426\u53ef\u4ee5\u5220\u9664\uff0c\u8fd4\u56de\u53ef\u4ee5\u5220\u9664 segment \u7684\u96c6\u5408\uff1a /** * Find segments starting from the oldest until the user-supplied predicate is false or the segment * containing the current high watermark is reached. We do not delete segments with offsets at or beyond * the high watermark to ensure that the log start offset can never exceed it. If the high watermark * has not yet been initialized, no segments are eligible for deletion. * * A final segment that is empty will never be returned (since we would just end up re-creating it). * * @param predicate A function that takes in a candidate log segment and the next higher segment * (if there is one) and returns true iff it is deletable * @return the segments ready to be deleted */ private def deletableSegments ( predicate : ( LogSegment , Option [ LogSegment ]) => Boolean ): Iterable [ LogSegment ] = { if ( segments . isEmpty || replicaHighWatermark . isEmpty ) { Seq . empty } else { val highWatermark = replicaHighWatermark . get val deletable = ArrayBuffer . empty [ LogSegment ] var segmentEntry = segments . firstEntry while ( segmentEntry != null ) { val segment = segmentEntry . getValue val nextSegmentEntry = segments . higherEntry ( segmentEntry . getKey ) val ( nextSegment , upperBoundOffset , isLastSegmentAndEmpty ) = if ( nextSegmentEntry != null ) ( nextSegmentEntry . getValue , nextSegmentEntry . getValue . baseOffset , false ) else ( null , logEndOffset , segment . size == 0 ) if ( highWatermark >= upperBoundOffset && predicate ( segment , Option ( nextSegment )) && ! isLastSegmentAndEmpty ) { deletable += segment segmentEntry = nextSegmentEntry } else { segmentEntry = null } } deletable } } private def deleteSegments ( deletable : Iterable [ LogSegment ]): Int = { maybeHandleIOException ( s\"Error while deleting segments for $ topicPartition in dir ${ dir . getParent } \" ) { val numToDelete = deletable . size if ( numToDelete > 0 ) { // we must always have at least one segment, so if we are going to delete all the segments, create a new one first if ( segments . size == numToDelete ) roll () lock synchronized { checkIfMemoryMappedBufferClosed () // remove the segments for lookups deletable . foreach ( deleteSegment ) maybeIncrementLogStartOffset ( segments . firstEntry . getValue . baseOffset ) } } numToDelete } } Log.scala /** * If topic deletion is enabled, delete any log segments that have either expired due to time based retention * or because the log size is > retentionSize. * * Whether or not deletion is enabled, delete any log segments that are before the log start offset */ def deleteOldSegments (): Int = { if ( config . delete ) { deleteRetentionMsBreachedSegments () + deleteRetentionSizeBreachedSegments () + deleteLogStartOffsetBreachedSegments () } else { deleteLogStartOffsetBreachedSegments () } } private def deleteRetentionMsBreachedSegments (): Int = { if ( config . retentionMs < 0 ) return 0 val startMs = time . milliseconds def shouldDelete ( segment : LogSegment , nextSegmentOpt : Option [ LogSegment ]): Boolean = { startMs - segment . largestTimestamp > config . retentionMs } deleteOldSegments ( shouldDelete , RetentionMsBreach ) }","title":"1.6.2 \u65e5\u5fd7\u6e05\u7406"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/#162","text":"\u65e5\u5fd7\u6e05\u7406\u7684\u7ebf\u7a0b\u5728 LogManager.startup() \u4e2d\u542f\u52a8\uff0c\u5177\u4f53\u4e3a\uff1a def startup () { /* Schedule the cleanup task to delete old logs */ if ( scheduler != null ) { info ( \"Starting log cleanup with a period of %d ms.\" . format ( retentionCheckMs )) scheduler . schedule ( \"kafka-log-retention\" , cleanupLogs _ , delay = InitialTaskDelayMs , period = retentionCheckMs , TimeUnit . MILLISECONDS ) // .... } } \u5468\u671f\u6027\u8fd0\u884c LogManager.cleanupLogs() \u65b9\u6cd5\uff1a /** * Delete any eligible logs. Return the number of segments deleted. * Only consider logs that are not compacted. */ def cleanupLogs (): Unit = { debug ( \"Beginning log cleanup...\" ) var total = 0 val startMs = time . milliseconds // clean current logs. val deletableLogs = { if ( cleaner != null ) { // prevent cleaner from working on same partitions when changing cleanup policy cleaner . pauseCleaningForNonCompactedPartitions () } else { currentLogs . filter { case ( _ , log ) => ! log . config . compact } } } try { deletableLogs . foreach { case ( topicPartition , log ) => debug ( s\"Garbage collecting ' ${ log . name } '\" ) total += log . deleteOldSegments () val futureLog = futureLogs . get ( topicPartition ) if ( futureLog != null ) { // clean future logs debug ( s\"Garbage collecting future log ' ${ futureLog . name } '\" ) total += futureLog . deleteOldSegments () } } } finally { if ( cleaner != null ) { cleaner . resumeCleaning ( deletableLogs . map ( _ . _1 )) } } debug ( s\"Log cleanup completed. $ total files deleted in \" + ( time . milliseconds - startMs ) / 1000 + \" seconds\" ) } \u8c03\u7528\u6bcf\u4e00\u4e2a log \u5b9e\u4f8b\u7684 Log.deleteOldSegments() \u65b9\u6cd5\uff1a /** * Delete any log segments that have either expired due to time based retention * or because the log size is > retentionSize */ def deleteOldSegments (): Int = { if ( ! config . delete ) return 0 deleteRetentionMsBreachedSegments () + deleteRetentionSizeBreachedSegments () + deleteLogStartOffsetBreachedSegments () } \u8fd9\u91cc\u5b9e\u9645\u8c03\u7528\u4e86 3 \u4e2a\u65b9\u6cd5\uff0c\u5206\u522b\u662f\uff1a * deleteRetentionMsBreachedSegments\uff0c\u6309\u4fdd\u7559\u65f6\u95f4\u6e05\u7406 * deleteRetentionSizeBreachedSegments\uff0c\u6309\u4fdd\u7559\u5927\u5c0f\u6e05\u7406 * deleteLogStartOffsetBreachedSegments","title":"1.6.2 \u65e5\u5fd7\u6e05\u7406"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/#deleteretentionmsbreachedsegments","text":"Log.scala private def deleteRetentionMsBreachedSegments (): Int = { if ( config . retentionMs < 0 ) return 0 val startMs = time . milliseconds deleteOldSegments (( segment , _ ) => startMs - segment . largestTimestamp > config . retentionMs , reason = s\"retention time ${ config . retentionMs } ms breach\" ) } \u8fd9\u91cc\u5224\u65ad\u7684\u4f9d\u636e\u662f startMs - segment.largestTimestamp > config.retentionMs \u770b\u4e00\u4e0b largestTimestamp \u662f\u600e\u4e48\u5b9a\u4e49\u7684\uff1a LogSegment.scala /** * The last modified time of this log segment as a unix time stamp */ def lastModified = log . file . lastModified /** * The largest timestamp this segment contains, if maxTimestampSoFar >= 0, otherwise None. */ def largestRecordTimestamp : Option [ Long ] = if ( maxTimestampSoFar >= 0 ) Some ( maxTimestampSoFar ) else None /** * The largest timestamp this segment contains. */ def largestTimestamp = if ( maxTimestampSoFar >= 0 ) maxTimestampSoFar else lastModified","title":"deleteRetentionMsBreachedSegments"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/#deleteretentionsizebreachedsegments","text":"","title":"deleteRetentionSizeBreachedSegments"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/#deletelogstartoffsetbreachedsegments","text":"","title":"deleteLogStartOffsetBreachedSegments"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/#deleteoldsegments","text":"\u4e0a\u8ff0 3 \u79cd\u60c5\u51b5\uff0c\u90fd\u4f1a\u6536\u675f\u5230 Log.deleteOldSegments() \uff0c\u4e0d\u540c\u7684\u60c5\u51b5\u53ea\u662f\u4f20\u4e0d\u540c\u7684 predicate \u5224\u65ad\u65b9\u6cd5\u3002 /** * Delete any log segments matching the given predicate function, * starting with the oldest segment and moving forward until a segment doesn't match. * * @param predicate A function that takes in a candidate log segment and the next higher segment * (if there is one) and returns true iff it is deletable * @return The number of segments deleted */ private def deleteOldSegments ( predicate : ( LogSegment , Option [ LogSegment ]) => Boolean , reason : String ): Int = { lock synchronized { val deletable = deletableSegments ( predicate ) if ( deletable . nonEmpty ) info ( s\"Found deletable segments with base offsets [ ${ deletable . map ( _ . baseOffset ). mkString ( \",\" ) } ] due to $ reason \" ) deleteSegments ( deletable ) } } \u904d\u5386\u6240\u6709 segment\uff0c\u7528 predicate \u5224\u65ad\u662f\u5426\u53ef\u4ee5\u5220\u9664\uff0c\u8fd4\u56de\u53ef\u4ee5\u5220\u9664 segment \u7684\u96c6\u5408\uff1a /** * Find segments starting from the oldest until the user-supplied predicate is false or the segment * containing the current high watermark is reached. We do not delete segments with offsets at or beyond * the high watermark to ensure that the log start offset can never exceed it. If the high watermark * has not yet been initialized, no segments are eligible for deletion. * * A final segment that is empty will never be returned (since we would just end up re-creating it). * * @param predicate A function that takes in a candidate log segment and the next higher segment * (if there is one) and returns true iff it is deletable * @return the segments ready to be deleted */ private def deletableSegments ( predicate : ( LogSegment , Option [ LogSegment ]) => Boolean ): Iterable [ LogSegment ] = { if ( segments . isEmpty || replicaHighWatermark . isEmpty ) { Seq . empty } else { val highWatermark = replicaHighWatermark . get val deletable = ArrayBuffer . empty [ LogSegment ] var segmentEntry = segments . firstEntry while ( segmentEntry != null ) { val segment = segmentEntry . getValue val nextSegmentEntry = segments . higherEntry ( segmentEntry . getKey ) val ( nextSegment , upperBoundOffset , isLastSegmentAndEmpty ) = if ( nextSegmentEntry != null ) ( nextSegmentEntry . getValue , nextSegmentEntry . getValue . baseOffset , false ) else ( null , logEndOffset , segment . size == 0 ) if ( highWatermark >= upperBoundOffset && predicate ( segment , Option ( nextSegment )) && ! isLastSegmentAndEmpty ) { deletable += segment segmentEntry = nextSegmentEntry } else { segmentEntry = null } } deletable } } private def deleteSegments ( deletable : Iterable [ LogSegment ]): Int = { maybeHandleIOException ( s\"Error while deleting segments for $ topicPartition in dir ${ dir . getParent } \" ) { val numToDelete = deletable . size if ( numToDelete > 0 ) { // we must always have at least one segment, so if we are going to delete all the segments, create a new one first if ( segments . size == numToDelete ) roll () lock synchronized { checkIfMemoryMappedBufferClosed () // remove the segments for lookups deletable . foreach ( deleteSegment ) maybeIncrementLogStartOffset ( segments . firstEntry . getValue . baseOffset ) } } numToDelete } } Log.scala /** * If topic deletion is enabled, delete any log segments that have either expired due to time based retention * or because the log size is > retentionSize. * * Whether or not deletion is enabled, delete any log segments that are before the log start offset */ def deleteOldSegments (): Int = { if ( config . delete ) { deleteRetentionMsBreachedSegments () + deleteRetentionSizeBreachedSegments () + deleteLogStartOffsetBreachedSegments () } else { deleteLogStartOffsetBreachedSegments () } } private def deleteRetentionMsBreachedSegments (): Int = { if ( config . retentionMs < 0 ) return 0 val startMs = time . milliseconds def shouldDelete ( segment : LogSegment , nextSegmentOpt : Option [ LogSegment ]): Boolean = { startMs - segment . largestTimestamp > config . retentionMs } deleteOldSegments ( shouldDelete , RetentionMsBreach ) }","title":"\u6536\u675f\u5230 deleteOldSegments"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/3-flush%20log/","text":"","title":"1.6.3 flush log"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/4-recovery-point/","text":"1.6.4 recovery point \u68c0\u67e5\u70b9\u6587\u4ef6 \u6d88\u606f\u8ffd\u52a0\u5230\u5206\u533a\u5bf9\u5e94\u7684\u65e5\u5fd7\uff0c\u5728\u5237\u65b0\u65e5\u5fd7\u662f\uff0c\u4f1a\u5c06\u6700\u65b0\u7684\u504f\u79fb\u91cf\u4f5c\u4e3a\u65e5\u5fd7\u7684\u68c0\u67e5\u70b9 LogManager \u4f1a\u542f\u52a8\u4e00\u4e2a\u5b9a\u65f6\u4efb\u52a1\uff0c\u8bfb\u53d6\u6240\u6709\u5206\u533a\u65e5\u5fd7\u7684\u68c0\u67e5\u70b9\uff0c\u5e76\u5199\u5165\u5168\u5c40\u7684\u68c0\u67e5\u70b9\u6587\u4ef6","title":"1.6.4 recovery point \u68c0\u67e5\u70b9\u6587\u4ef6"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/4-recovery-point/#164-recovery-point","text":"\u6d88\u606f\u8ffd\u52a0\u5230\u5206\u533a\u5bf9\u5e94\u7684\u65e5\u5fd7\uff0c\u5728\u5237\u65b0\u65e5\u5fd7\u662f\uff0c\u4f1a\u5c06\u6700\u65b0\u7684\u504f\u79fb\u91cf\u4f5c\u4e3a\u65e5\u5fd7\u7684\u68c0\u67e5\u70b9 LogManager \u4f1a\u542f\u52a8\u4e00\u4e2a\u5b9a\u65f6\u4efb\u52a1\uff0c\u8bfb\u53d6\u6240\u6709\u5206\u533a\u65e5\u5fd7\u7684\u68c0\u67e5\u70b9\uff0c\u5e76\u5199\u5165\u5168\u5c40\u7684\u68c0\u67e5\u70b9\u6587\u4ef6","title":"1.6.4 recovery point \u68c0\u67e5\u70b9\u6587\u4ef6"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/5-start-offset/","text":"","title":"5 start offset"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/7-%E5%8E%8B%E7%BC%A9%E7%AD%96%E7%95%A5%E7%9A%84%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/","text":"","title":"7 \u538b\u7f29\u7b56\u7565\u7684\u65e5\u5fd7\u6e05\u7406"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/7-acl%20%E6%8E%A7%E5%88%B6/","text":"1.7 Acl \u6743\u9650\u63a7\u5236 kafka \u63d0\u4f9b\u4e86\u6743\u9650\u63a7\u5236\u63a5\u53e3 org.apache.kafka.server.authorizer.Authorizer \u4e0e\u5176\u9ed8\u8ba4\u5b9e\u73b0 kafka.security.authorizer.AclAuthorizer \u901a\u8fc7\u914d\u7f6e\uff1a authorizer.class.name = kafka.security.authorizer.AclAuthorizer \u53ef\u4ee5\u8ba9 kafka \u521d\u59cb\u5316\u8fd9\u4e2a\u7c7b\uff0c\u5e76\u5728\u5904\u7406\u8bf7\u6c42\u662f\u8fdb\u884c acl \u5224\u65ad 2.4 \u4e4b\u524d\u662f kafka.security.auth.Authorizer \u63a5\u53e3\u548c kafka.security.auth.SimpleAclAuthorizer \u5b9e\u73b0\u7c7b \u5728 KafkaServer startup \u65b9\u6cd5\u4e2d\uff1a /* Get the authorizer and initialize it if one is specified.*/ authorizer = config . authorizer authorizer . foreach ( _ . configure ( config . originals )) val authorizerFutures : Map [ Endpoint , CompletableFuture [ Void ]] = authorizer match { case Some ( authZ ) => authZ . start ( brokerInfo . broker . toServerInfo ( clusterId , config )). asScala . map { case ( ep , cs ) => ep -> cs . toCompletableFuture } case None => brokerInfo . broker . endPoints . map { ep => ep . toJava -> CompletableFuture . completedFuture [ Void ]( null ) }. toMap } \u5728 KafkaApis \u4e2d\u5b9a\u4e49\uff1a private [ server ] def authorize ( requestContext : RequestContext , operation : AclOperation , resourceType : ResourceType , resourceName : String , logIfAllowed : Boolean = true , logIfDenied : Boolean = true , refCount : Int = 1 ): Boolean = { authorizer . forall { authZ => val resource = new ResourcePattern ( resourceType , resourceName , PatternType . LITERAL ) val actions = Collections . singletonList ( new Action ( operation , resource , refCount , logIfAllowed , logIfDenied )) authZ . authorize ( requestContext , actions ). get ( 0 ) == AuthorizationResult . ALLOWED } } handleProduceRequest WRITE TRANSACTIONANL_ID produceRequest.transactionalId IDEMPOTENT_WRITE CLUSTER CLUSTER_NAME WRITE TOPIC handleFetchRequest CLUSTER_ACTION CLUSTER CLUSTER_NAME READ TOPIC handleListOffsetRequestV0 DESCRIBE TOPIC handleListOffsetRequestV1AndAbove DESCRIBE TOPIC","title":"1.7 Acl \u6743\u9650\u63a7\u5236"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/7-acl%20%E6%8E%A7%E5%88%B6/#17-acl","text":"kafka \u63d0\u4f9b\u4e86\u6743\u9650\u63a7\u5236\u63a5\u53e3 org.apache.kafka.server.authorizer.Authorizer \u4e0e\u5176\u9ed8\u8ba4\u5b9e\u73b0 kafka.security.authorizer.AclAuthorizer \u901a\u8fc7\u914d\u7f6e\uff1a authorizer.class.name = kafka.security.authorizer.AclAuthorizer \u53ef\u4ee5\u8ba9 kafka \u521d\u59cb\u5316\u8fd9\u4e2a\u7c7b\uff0c\u5e76\u5728\u5904\u7406\u8bf7\u6c42\u662f\u8fdb\u884c acl \u5224\u65ad 2.4 \u4e4b\u524d\u662f kafka.security.auth.Authorizer \u63a5\u53e3\u548c kafka.security.auth.SimpleAclAuthorizer \u5b9e\u73b0\u7c7b \u5728 KafkaServer startup \u65b9\u6cd5\u4e2d\uff1a /* Get the authorizer and initialize it if one is specified.*/ authorizer = config . authorizer authorizer . foreach ( _ . configure ( config . originals )) val authorizerFutures : Map [ Endpoint , CompletableFuture [ Void ]] = authorizer match { case Some ( authZ ) => authZ . start ( brokerInfo . broker . toServerInfo ( clusterId , config )). asScala . map { case ( ep , cs ) => ep -> cs . toCompletableFuture } case None => brokerInfo . broker . endPoints . map { ep => ep . toJava -> CompletableFuture . completedFuture [ Void ]( null ) }. toMap } \u5728 KafkaApis \u4e2d\u5b9a\u4e49\uff1a private [ server ] def authorize ( requestContext : RequestContext , operation : AclOperation , resourceType : ResourceType , resourceName : String , logIfAllowed : Boolean = true , logIfDenied : Boolean = true , refCount : Int = 1 ): Boolean = { authorizer . forall { authZ => val resource = new ResourcePattern ( resourceType , resourceName , PatternType . LITERAL ) val actions = Collections . singletonList ( new Action ( operation , resource , refCount , logIfAllowed , logIfDenied )) authZ . authorize ( requestContext , actions ). get ( 0 ) == AuthorizationResult . ALLOWED } } handleProduceRequest WRITE TRANSACTIONANL_ID produceRequest.transactionalId IDEMPOTENT_WRITE CLUSTER CLUSTER_NAME WRITE TOPIC handleFetchRequest CLUSTER_ACTION CLUSTER CLUSTER_NAME READ TOPIC handleListOffsetRequestV0 DESCRIBE TOPIC handleListOffsetRequestV1AndAbove DESCRIBE TOPIC","title":"1.7 Acl \u6743\u9650\u63a7\u5236"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/","text":"2 producer \u89e3\u6790","title":"2. producer \u89e3\u6790"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/#2-producer","text":"","title":"2 producer \u89e3\u6790"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/","text":"2.1 producer \u53d1\u9001\u6d88\u606f \u4ee3\u7801\u793a\u4f8b \u6211\u4eec\u5148\u770b\u4e0b KafkaProducer \u5982\u4f55\u53d1\u9001\u6d88\u606f\uff1a Properties props = new Properties (); props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); props . put ( \"acks\" , \"all\" ); props . put ( \"retries\" , 0 ); props . put ( \"linger.ms\" , 1 ); props . put ( \"key.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); props . put ( \"value.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); Producer < String , String > producer = new KafkaProducer <> ( props ); for ( int i = 0 ; i < 100 ; i ++ ) { producer . send ( new ProducerRecord < String , String > ( \"my-topic\" , Integer . toString ( i ), Integer . toString ( i ))); } producer . close (); \u8fd9\u91cc\u53ea\u6709 2 \u90e8\u5206\uff0c\u6784\u9020 KafkaProducer \u5b9e\u4f8b\uff0c\u8c03\u7528 send() \u51fd\u6570\u3002 \u6784\u9020 KafkaProducer KafkaProducer \u5b9e\u4f8b \u9700\u8981\u6ce8\u610f KafkaProducer \u4e2d\u4ee5\u4e0b\u51e0\u4e2a\u5c5e\u6027\uff1a * partitioner : Partitioner \u5b9e\u4f8b\uff0c\u5bf9\u5e94\u5206\u533a\u7b97\u6cd5 * metadata : ProducerMetadata \u5b9e\u4f8b\uff0c\u5305\u542b topic metadata * accumulator : RecordAccumulator \u5b9e\u4f8b\uff0c\u5305\u542b\u6bcf\u4e2a topic-partition \u53d1\u9001\u6570\u636e\u7684\u961f\u5217 * sender : Sender \u5b9e\u4f8b\uff0c\u4e4b\u540e\u4f1a\u5728\u72ec\u7acb\u7684\u7ebf\u7a0b\u4e2d\u8fd0\u884c\uff0c\u8d1f\u8d23\u6570\u636e\u7684\u53d1\u9001 * ioThread : KafkaThread \u5b9e\u4f8b\uff0c\u72ec\u7acb\u7684\u7ebf\u7a0b\uff0c\u8fd0\u884c sender Sender \u5b9e\u4f8b \u9700\u8981\u6ce8\u610f Sender \u4e2d\u4ee5\u4e0b\u51e0\u4e2a\u5c5e\u6027\uff1a * client : KafkaClient \u5b9e\u4f8b(\u5b9e\u9645\u4e3a NetworkClient )\uff0c\u8fdb\u884c\u5b9e\u9645\u7684\u6570\u636e\u53d1\u9001 * accumulator : RecordAccumulator \u5b9e\u4f8b * metadata : ProducerMetadata \u5b9e\u4f8b\uff0c\u5305\u542b topic metadata NetworkClient \u5b9e\u4f8b selector : kafka \u5bf9 java.nio.channels.Selector \u7684\u5305\u88c5 metadataUpdater KafkaProducer \u521d\u59cb\u5316\u4e4b\u540e\uff0c\u521b\u5efa\u7684\u5bf9\u8c61\u5982\u4e0b\u6240\u793a KafkaProducer \u6784\u9020\u65b9\u6cd5\u4e2d\uff0c\u521d\u59cb\u5982\u4e0b\u5173\u952e\u5c5e\u6027 * partitioner * accumulator * metadata this . metadata = new ProducerMetadata ( retryBackoffMs , config . getLong ( ProducerConfig . METADATA_MAX_AGE_CONFIG ), config . getLong ( ProducerConfig . METADATA_MAX_IDLE_CONFIG ), logContext , clusterResourceListeners , Time . SYSTEM ); this . metadata . bootstrap ( addresses ); * this.sender = newSender(logContext, kafkaClient, this.metadata) \u540c\u65f6\u5f00\u542f\u7ebf\u7a0b\uff0c\u8fd0\u884c sender this . ioThread = new KafkaThread ( ioThreadName , this . sender , true ); this . ioThread . start (); send() \u505a\u4e86\u4ec0\u4e48\uff1f send() \u662f\u5f02\u6b65\u7684\uff0c\u8c03\u7528\u4f1a\u7acb\u5373\u8fd4\u56de\uff0crecord \u4f1a\u88ab\u5b58\u50a8\u5230 the buffer of records \u7b49\u5f85\u88ab\u53d1\u9001\u3002This allows sending many records in parallel without blocking to wait for the response after each one. \u4f7f\u7528 ProducerInterceptors \u5bf9 record \u8fdb\u884c\u5904\u7406 \u8c03\u7528 doSend throwIfProducerClosed \uff0c\u68c0\u67e5 sender \u662f\u5426\u5b58\u5728\u4e14\u6b63\u5728\u8fd0\u884c make sure the metadata for the topic is available\uff0c\u8fd9\u91cc\u4f1a\u4e00\u76f4\u5c1d\u8bd5\u83b7\u53d6 metadata\uff0c\u76f4\u5230\u8d85\u8fc7 maxBlockTimeMs long nowMs = time . milliseconds (); ClusterAndWaitTime clusterAndWaitTime ; try { clusterAndWaitTime = waitOnMetadata ( record . topic (), record . partition (), nowMs , maxBlockTimeMs ); } catch ( KafkaException e ) { if ( metadata . isClosed ()) throw new KafkaException ( \"Producer closed while send in progress\" , e ); throw e ; } nowMs += clusterAndWaitTime . waitedOnMetadataMs ; long remainingWaitMs = Math . max ( 0 , maxBlockTimeMs - clusterAndWaitTime . waitedOnMetadataMs ); Cluster cluster = clusterAndWaitTime . cluster ; \u5e8f\u5217\u5316 key, value \u8ba1\u7b97\u6d88\u606f\u6240\u5904\u5206\u533a \u8ba1\u7b97\u6d88\u606f\u5927\u5c0f\uff0c\u786e\u5b9a\u4e0d\u8d85\u8fc7 max.request.size , buffer.memory \u83b7\u53d6\u6d88\u606f timestamp\uff0c\u5982\u679c\u6ca1\u6709\u5c31\u7528\u5f53\u524d\u65f6\u95f4 \u5c06\u6d88\u606f\u8ffd\u52a0\u5230 accumulator RecordAccumulator . RecordAppendResult result = accumulator . append ( tp , timestamp , serializedKey , serializedValue , headers , interceptCallback , remainingWaitMs , true , nowMs ); \u5982\u679c batch \u6ee1\u4e86\u6216\u8005\u65b0\u521b\u5efa\u4e86 batch \uff0c\u5524\u9192 sender if ( result . batchIsFull || result . newBatchCreated ) { log . trace ( \"Waking up the sender since topic {} partition {} is either full or getting a new batch\" , record . topic (), partition ); this . sender . wakeup (); } \u4e4b\u540e\u4f1a\u7531\u72ec\u7acb\u7ebf\u7a0b\u7684 Sender \u4ece RecordAccumulator \u4e2d\u53d6\u5f97\u6d88\u606f\u7136\u540e\u53d1\u9001\u5230 Kafka Sender \u505a\u4e86\u4ec0\u4e48\uff1f Sender \u662f\u4f5c\u4e3a\u72ec\u7acb\u7ebf\u7a0b\u8fd0\u884c\uff0c\u4e3b\u8981\u903b\u8f91\u5728 run \u51fd\u6570\u4e2d: // main loop, runs until close is called while ( running ) { try { runOnce (); } catch ( Exception e ) { log . error ( \"Uncaught error in kafka producer I/O thread: \" , e ); } } run \u5faa\u73af\u8fd0\u884c runOnce \uff0c runOnce \u4e3b\u8981\u6709 2 \u6b65\uff08\u5148\u4e0d\u8ba8\u8bba\u4e8b\u52a1\u6027\u652f\u6301\uff09\uff1a - sendProducerData // \u5c06 record batch \u8f6c\u79fb\u5230\u6bcf\u4e2a\u8282\u70b9\u7684\u751f\u4ea7\u8bf7\u6c42\u5217\u8868\u4e2d 1. \u83b7\u53d6 metadata 2. \u4ece accumulator \u7684 batches \u4e2d\u53d6\u51fa\u53ef\u4ee5\u53d1\u9001\u7684\u6570\u636e\uff0c 1. \u5f97\u5230 batches \u5bf9\u5e94\u7684 leader \u8282\u70b9\u5217\u8868 \uff08 RecordAccumulator.ReadyCheckResult result.readyNodes \uff09 2. \u5f97\u5230\u5728 metadata \u91cc\u627e\u4e0d\u5230 leader \u7684 topic \u5217\u8868\uff08 RecordAccumulator.ReadyCheckResult result.unknownLeaderTopics \uff09 3. \u5982\u679c\u6709\u4efb\u4f55 partition \u7684 leader \u627e\u4e0d\u5230\uff0c\u66f4\u65b0 metadata 4. \u904d\u5386 leader node \u5217\u8868( result.readyNodes ) 1. \u5224\u65ad leader node \u662f\u5426\u5df2\u7ecf\u8fde\u63a5\u5e76\u53ef\u4ee5\u53d1\u9001\u6570\u636e 1. \u5982\u679c\u4e0d\u662f\uff0c\u5219 \u5f00\u59cb\u8fde\u63a5 \uff0c\u5e76\u5728 readyNodes \u4e2d\u79fb\u9664\u8fd9\u4e2a leader node 5. \u83b7\u53d6\u6bcf\u4e2a leader \u5bf9\u5e94\u7684\u53ef\u4ee5\u53d1\u9001\u7684 ProducerBatch \u5217\u8868 6. \u5728 Sender \u7684 inFlightBatches \u91cc\u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 batch \u5217\u8868 7. \u5982\u679c\u9700\u8981\u4fdd\u8bc1\u6d88\u606f\u7684\u987a\u5e8f\uff0c\u5c06 topicPartition \u52a0\u5165\u5230 accumulator \u7684 muted \u8bb0\u5f55 8. sendProduceRequests(batches, now) /** * Transfer the record batches into a list of produce requests on a per-node basis */ private void sendProduceRequests ( Map < Integer , List < ProducerBatch >> collated , long now ) { for ( Map . Entry < Integer , List < ProducerBatch >> entry : collated . entrySet ()) sendProduceRequest ( now , entry . getKey (), acks , requestTimeoutMs , entry . getValue ()); } 9. sendProduceRequest \u91cc\u6784\u9020 ProduceRequest.Builder \uff0c\u6784\u9020 ClientRequest \uff0c\u5e76\u8c03\u7528 NetworkClient \u7684 send() \u51fd\u6570\u53d1\u9001 - client.poll // NetworkClient.poll Do actual reads and writes to sockets \u8fd9\u91cc\u6709\u4e3b\u8981\u7684\u51e0\u70b9\uff1a 1. \u4f55\u65f6\u4e0e broker \u5efa\u7acb\u8fde\u63a5 \uff1a 1. \u83b7\u53d6 readyNodes \u4e4b\u540e\uff0c\u4f1a\u904d\u5386 readyNodes \u68c0\u67e5\u68c0\u67e5 node \u662f\u5426\u5df2\u7ecf\u8fde\u63a5\uff0c\u8fd9\u91cc\u8c03\u7528 NetworkClient . ready() 2. NetworkClient . ready() \u4e2d\u5982\u679c\u5224\u65ad node \u5df2\u8fde\u63a5\uff0c\u4f1a\u76f4\u63a5\u8fd4\u56de true \uff0c\u5982\u679c\u6ca1\u6709\u8fde\u63a5\uff0c\u5219\u8fdb\u884c\u8fde\u63a5\uff0c\u5e76\u8fd4\u56de false @Override public boolean ready ( Node node , long now ) { if ( node . isEmpty ()) throw new IllegalArgumentException ( \"Cannot connect to empty node \" + node ); if ( isReady ( node , now )) return true ; if ( connectionStates . canConnect ( node . idString (), now )) // if we are interested in sending to a node and we don't have a connection to it, initiate one initiateConnect ( node , now ); return false ; } 3. NetworkClient . initiateConnect() \u4e2d\u901a\u8fc7 Selector . connect \u4e0e node \u8fdb\u884c\u8fde\u63a5 private void initiateConnect ( Node node , long now ) { String nodeConnectionId = node . idString (); try { connectionStates . connecting ( nodeConnectionId , now , node . host (), clientDnsLookup ); InetAddress address = connectionStates . currentAddress ( nodeConnectionId ); log . debug ( \"Initiating connection to node {} using address {}\" , node , address ); selector . connect ( nodeConnectionId , new InetSocketAddress ( address , node . port ()), this . socketSendBuffer , this . socketReceiveBuffer ); } catch ( IOException e ) { log . warn ( \"Error connecting to node {}\" , node , e ); // Attempt failed, we'll try again after the backoff connectionStates . disconnected ( nodeConnectionId , now ); // Notify metadata updater of the connection failure metadataUpdater . handleServerDisconnect ( now , nodeConnectionId , Optional . empty ()); } } 4. Selector . connect : 1. \u521b\u5efa SocketChannel \uff0c\u8fdb\u884c\u8fde\u63a5 node \u64cd\u4f5c 2. \u6ce8\u518c SocketChannel \u5230 Selector . nioSelector 3. \u6784\u5efa KafkaChannel \uff08\u901a\u8fc7 (Ssl/Sasl/Plaintext)ChannelBuilder \u6784\u5efa\uff09 4. \u5c06 KafakChannel \u8bb0\u5f55\u5230 Selector . channels \u4e2d kafka \u53d1\u9001\u6d88\u606f\u65f6\u4ee5\u8282\u70b9\u7ec4\u7ec7\u53ef\u53d1\u9001\u7684\u6d88\u606f\uff0c\u5c06\u53d1\u5f80\u540c\u4e00\u4e2a\u8282\u70b9\u7684\u4e0d\u540c topicPartion \u7684\u6570\u636e\u653e\u5728\u4e00\u4e2a\u8bf7\u6c42\u4e2d\u53d1\u9001 NetworkClient \u662f\u5982\u4f55\u53d1\u9001\u6570\u636e\u7684\uff1f Selector \u7684 poll poll pollSelectionKeys(Set<SelectionKey> selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos) attempRead(KafkaChannel channel) : \u8c03\u7528 channel.read() \uff0c\u8fd4\u56de\u8bfb\u53d6\u7684 byte \u6570\uff0c KafkaChannel . read() KafkaChannel . receive() \u901a\u8fc7\u8c03\u7528\u81ea\u8eab\u6240\u6301\u6709\u7684 NetworkReceive \u7684 readFrom() \u51fd\u6570\u8fdb\u884c\u6570\u636e\u8bfb\u53d6 \u901a\u8fc7 channel.maybeCompleteReceive() \u5224\u65ad\u662f\u5426\u6536\u5230\u5b8c\u6574\u7684 NetworkReceive \uff0c\u5982\u679c\u662f\uff0c\u5c06 receive \u52a0\u5165\u5230\u81ea\u8eab\u7684 completedReceives attemptWrite(SelectionKey key, KafkaChannel channel, long nowNanos) write(KafkaChannel channel) : \u8c03\u7528 channel.write() \uff0c\u8fd4\u56de\u53d1\u9001 byte \u6570\uff0c KafkaChannel . write() \u901a\u8fc7\u8c03\u7528\u81ea\u8eab\u6240\u6301\u6709 NetworkSend \u7684 writeTo() \u51fd\u6570\u8fdb\u884c\u6570\u636e\u53d1\u9001 java.nio.channels.GatheringByteChannel . write() \u901a\u8fc7 channel.maybeCompleteSend() \u5224\u65ad NetworkSend \u662f\u5426\u53d1\u9001\u5b8c\u6210\uff0c\u5982\u679c\u662f\uff0c\u5c06 send \u52a0\u5165\u5230\u81ea\u8eab\u7684 completedSends \u8fd9\u91cc\u6709\u4e3b\u8981\u7684\u51e0\u70b9\uff1a 1. \u4f55\u65f6\u8fdb\u884c ssl, sasl \u63e1\u624b \uff1a\u5728 Sender . run() \u4e2d\u4f1a\u68c0\u67e5\u8981\u53d1\u9001\u6570\u636e\u5bf9\u5e94\u7684 leader \u662f\u5426\u8fde\u63a5\uff0c\u5982\u679c\u6ca1\u6709\u5219\u8fdb\u884c\u8fde\u63a5\uff0c\u5728 Selector . pollSelectionKeys() \u4e2d\uff0c\u5982\u679c key \u5bf9\u5e94\u7684 channel \u5df2\u8fde\u63a5\u4f46\u8fd8\u672a ready\uff0c\u5219\u8c03\u7528 KafkaChannel . prepare() \u8fdb\u884c ssl \u63e1\u624b\u548c sasl \u63e1\u624b \u53c2\u8003 https://blog.csdn.net/chunlongyu/article/details/52651960","title":"2.1 producer \u53d1\u9001\u6d88\u606f"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#21-producer","text":"","title":"2.1 producer \u53d1\u9001\u6d88\u606f"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#_1","text":"\u6211\u4eec\u5148\u770b\u4e0b KafkaProducer \u5982\u4f55\u53d1\u9001\u6d88\u606f\uff1a Properties props = new Properties (); props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); props . put ( \"acks\" , \"all\" ); props . put ( \"retries\" , 0 ); props . put ( \"linger.ms\" , 1 ); props . put ( \"key.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); props . put ( \"value.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); Producer < String , String > producer = new KafkaProducer <> ( props ); for ( int i = 0 ; i < 100 ; i ++ ) { producer . send ( new ProducerRecord < String , String > ( \"my-topic\" , Integer . toString ( i ), Integer . toString ( i ))); } producer . close (); \u8fd9\u91cc\u53ea\u6709 2 \u90e8\u5206\uff0c\u6784\u9020 KafkaProducer \u5b9e\u4f8b\uff0c\u8c03\u7528 send() \u51fd\u6570\u3002","title":"\u4ee3\u7801\u793a\u4f8b"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#kafkaproducer","text":"","title":"\u6784\u9020 KafkaProducer"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#kafkaproducer_1","text":"\u9700\u8981\u6ce8\u610f KafkaProducer \u4e2d\u4ee5\u4e0b\u51e0\u4e2a\u5c5e\u6027\uff1a * partitioner : Partitioner \u5b9e\u4f8b\uff0c\u5bf9\u5e94\u5206\u533a\u7b97\u6cd5 * metadata : ProducerMetadata \u5b9e\u4f8b\uff0c\u5305\u542b topic metadata * accumulator : RecordAccumulator \u5b9e\u4f8b\uff0c\u5305\u542b\u6bcf\u4e2a topic-partition \u53d1\u9001\u6570\u636e\u7684\u961f\u5217 * sender : Sender \u5b9e\u4f8b\uff0c\u4e4b\u540e\u4f1a\u5728\u72ec\u7acb\u7684\u7ebf\u7a0b\u4e2d\u8fd0\u884c\uff0c\u8d1f\u8d23\u6570\u636e\u7684\u53d1\u9001 * ioThread : KafkaThread \u5b9e\u4f8b\uff0c\u72ec\u7acb\u7684\u7ebf\u7a0b\uff0c\u8fd0\u884c sender","title":"KafkaProducer \u5b9e\u4f8b"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#sender","text":"\u9700\u8981\u6ce8\u610f Sender \u4e2d\u4ee5\u4e0b\u51e0\u4e2a\u5c5e\u6027\uff1a * client : KafkaClient \u5b9e\u4f8b(\u5b9e\u9645\u4e3a NetworkClient )\uff0c\u8fdb\u884c\u5b9e\u9645\u7684\u6570\u636e\u53d1\u9001 * accumulator : RecordAccumulator \u5b9e\u4f8b * metadata : ProducerMetadata \u5b9e\u4f8b\uff0c\u5305\u542b topic metadata","title":"Sender \u5b9e\u4f8b"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#networkclient","text":"selector : kafka \u5bf9 java.nio.channels.Selector \u7684\u5305\u88c5 metadataUpdater KafkaProducer \u521d\u59cb\u5316\u4e4b\u540e\uff0c\u521b\u5efa\u7684\u5bf9\u8c61\u5982\u4e0b\u6240\u793a KafkaProducer \u6784\u9020\u65b9\u6cd5\u4e2d\uff0c\u521d\u59cb\u5982\u4e0b\u5173\u952e\u5c5e\u6027 * partitioner * accumulator * metadata this . metadata = new ProducerMetadata ( retryBackoffMs , config . getLong ( ProducerConfig . METADATA_MAX_AGE_CONFIG ), config . getLong ( ProducerConfig . METADATA_MAX_IDLE_CONFIG ), logContext , clusterResourceListeners , Time . SYSTEM ); this . metadata . bootstrap ( addresses ); * this.sender = newSender(logContext, kafkaClient, this.metadata) \u540c\u65f6\u5f00\u542f\u7ebf\u7a0b\uff0c\u8fd0\u884c sender this . ioThread = new KafkaThread ( ioThreadName , this . sender , true ); this . ioThread . start ();","title":"NetworkClient \u5b9e\u4f8b"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#send","text":"send() \u662f\u5f02\u6b65\u7684\uff0c\u8c03\u7528\u4f1a\u7acb\u5373\u8fd4\u56de\uff0crecord \u4f1a\u88ab\u5b58\u50a8\u5230 the buffer of records \u7b49\u5f85\u88ab\u53d1\u9001\u3002This allows sending many records in parallel without blocking to wait for the response after each one. \u4f7f\u7528 ProducerInterceptors \u5bf9 record \u8fdb\u884c\u5904\u7406 \u8c03\u7528 doSend throwIfProducerClosed \uff0c\u68c0\u67e5 sender \u662f\u5426\u5b58\u5728\u4e14\u6b63\u5728\u8fd0\u884c make sure the metadata for the topic is available\uff0c\u8fd9\u91cc\u4f1a\u4e00\u76f4\u5c1d\u8bd5\u83b7\u53d6 metadata\uff0c\u76f4\u5230\u8d85\u8fc7 maxBlockTimeMs long nowMs = time . milliseconds (); ClusterAndWaitTime clusterAndWaitTime ; try { clusterAndWaitTime = waitOnMetadata ( record . topic (), record . partition (), nowMs , maxBlockTimeMs ); } catch ( KafkaException e ) { if ( metadata . isClosed ()) throw new KafkaException ( \"Producer closed while send in progress\" , e ); throw e ; } nowMs += clusterAndWaitTime . waitedOnMetadataMs ; long remainingWaitMs = Math . max ( 0 , maxBlockTimeMs - clusterAndWaitTime . waitedOnMetadataMs ); Cluster cluster = clusterAndWaitTime . cluster ; \u5e8f\u5217\u5316 key, value \u8ba1\u7b97\u6d88\u606f\u6240\u5904\u5206\u533a \u8ba1\u7b97\u6d88\u606f\u5927\u5c0f\uff0c\u786e\u5b9a\u4e0d\u8d85\u8fc7 max.request.size , buffer.memory \u83b7\u53d6\u6d88\u606f timestamp\uff0c\u5982\u679c\u6ca1\u6709\u5c31\u7528\u5f53\u524d\u65f6\u95f4 \u5c06\u6d88\u606f\u8ffd\u52a0\u5230 accumulator RecordAccumulator . RecordAppendResult result = accumulator . append ( tp , timestamp , serializedKey , serializedValue , headers , interceptCallback , remainingWaitMs , true , nowMs ); \u5982\u679c batch \u6ee1\u4e86\u6216\u8005\u65b0\u521b\u5efa\u4e86 batch \uff0c\u5524\u9192 sender if ( result . batchIsFull || result . newBatchCreated ) { log . trace ( \"Waking up the sender since topic {} partition {} is either full or getting a new batch\" , record . topic (), partition ); this . sender . wakeup (); } \u4e4b\u540e\u4f1a\u7531\u72ec\u7acb\u7ebf\u7a0b\u7684 Sender \u4ece RecordAccumulator \u4e2d\u53d6\u5f97\u6d88\u606f\u7136\u540e\u53d1\u9001\u5230 Kafka","title":"send() \u505a\u4e86\u4ec0\u4e48\uff1f"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#sender_1","text":"Sender \u662f\u4f5c\u4e3a\u72ec\u7acb\u7ebf\u7a0b\u8fd0\u884c\uff0c\u4e3b\u8981\u903b\u8f91\u5728 run \u51fd\u6570\u4e2d: // main loop, runs until close is called while ( running ) { try { runOnce (); } catch ( Exception e ) { log . error ( \"Uncaught error in kafka producer I/O thread: \" , e ); } } run \u5faa\u73af\u8fd0\u884c runOnce \uff0c runOnce \u4e3b\u8981\u6709 2 \u6b65\uff08\u5148\u4e0d\u8ba8\u8bba\u4e8b\u52a1\u6027\u652f\u6301\uff09\uff1a - sendProducerData // \u5c06 record batch \u8f6c\u79fb\u5230\u6bcf\u4e2a\u8282\u70b9\u7684\u751f\u4ea7\u8bf7\u6c42\u5217\u8868\u4e2d 1. \u83b7\u53d6 metadata 2. \u4ece accumulator \u7684 batches \u4e2d\u53d6\u51fa\u53ef\u4ee5\u53d1\u9001\u7684\u6570\u636e\uff0c 1. \u5f97\u5230 batches \u5bf9\u5e94\u7684 leader \u8282\u70b9\u5217\u8868 \uff08 RecordAccumulator.ReadyCheckResult result.readyNodes \uff09 2. \u5f97\u5230\u5728 metadata \u91cc\u627e\u4e0d\u5230 leader \u7684 topic \u5217\u8868\uff08 RecordAccumulator.ReadyCheckResult result.unknownLeaderTopics \uff09 3. \u5982\u679c\u6709\u4efb\u4f55 partition \u7684 leader \u627e\u4e0d\u5230\uff0c\u66f4\u65b0 metadata 4. \u904d\u5386 leader node \u5217\u8868( result.readyNodes ) 1. \u5224\u65ad leader node \u662f\u5426\u5df2\u7ecf\u8fde\u63a5\u5e76\u53ef\u4ee5\u53d1\u9001\u6570\u636e 1. \u5982\u679c\u4e0d\u662f\uff0c\u5219 \u5f00\u59cb\u8fde\u63a5 \uff0c\u5e76\u5728 readyNodes \u4e2d\u79fb\u9664\u8fd9\u4e2a leader node 5. \u83b7\u53d6\u6bcf\u4e2a leader \u5bf9\u5e94\u7684\u53ef\u4ee5\u53d1\u9001\u7684 ProducerBatch \u5217\u8868 6. \u5728 Sender \u7684 inFlightBatches \u91cc\u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 batch \u5217\u8868 7. \u5982\u679c\u9700\u8981\u4fdd\u8bc1\u6d88\u606f\u7684\u987a\u5e8f\uff0c\u5c06 topicPartition \u52a0\u5165\u5230 accumulator \u7684 muted \u8bb0\u5f55 8. sendProduceRequests(batches, now) /** * Transfer the record batches into a list of produce requests on a per-node basis */ private void sendProduceRequests ( Map < Integer , List < ProducerBatch >> collated , long now ) { for ( Map . Entry < Integer , List < ProducerBatch >> entry : collated . entrySet ()) sendProduceRequest ( now , entry . getKey (), acks , requestTimeoutMs , entry . getValue ()); } 9. sendProduceRequest \u91cc\u6784\u9020 ProduceRequest.Builder \uff0c\u6784\u9020 ClientRequest \uff0c\u5e76\u8c03\u7528 NetworkClient \u7684 send() \u51fd\u6570\u53d1\u9001 - client.poll // NetworkClient.poll Do actual reads and writes to sockets \u8fd9\u91cc\u6709\u4e3b\u8981\u7684\u51e0\u70b9\uff1a 1. \u4f55\u65f6\u4e0e broker \u5efa\u7acb\u8fde\u63a5 \uff1a 1. \u83b7\u53d6 readyNodes \u4e4b\u540e\uff0c\u4f1a\u904d\u5386 readyNodes \u68c0\u67e5\u68c0\u67e5 node \u662f\u5426\u5df2\u7ecf\u8fde\u63a5\uff0c\u8fd9\u91cc\u8c03\u7528 NetworkClient . ready() 2. NetworkClient . ready() \u4e2d\u5982\u679c\u5224\u65ad node \u5df2\u8fde\u63a5\uff0c\u4f1a\u76f4\u63a5\u8fd4\u56de true \uff0c\u5982\u679c\u6ca1\u6709\u8fde\u63a5\uff0c\u5219\u8fdb\u884c\u8fde\u63a5\uff0c\u5e76\u8fd4\u56de false @Override public boolean ready ( Node node , long now ) { if ( node . isEmpty ()) throw new IllegalArgumentException ( \"Cannot connect to empty node \" + node ); if ( isReady ( node , now )) return true ; if ( connectionStates . canConnect ( node . idString (), now )) // if we are interested in sending to a node and we don't have a connection to it, initiate one initiateConnect ( node , now ); return false ; } 3. NetworkClient . initiateConnect() \u4e2d\u901a\u8fc7 Selector . connect \u4e0e node \u8fdb\u884c\u8fde\u63a5 private void initiateConnect ( Node node , long now ) { String nodeConnectionId = node . idString (); try { connectionStates . connecting ( nodeConnectionId , now , node . host (), clientDnsLookup ); InetAddress address = connectionStates . currentAddress ( nodeConnectionId ); log . debug ( \"Initiating connection to node {} using address {}\" , node , address ); selector . connect ( nodeConnectionId , new InetSocketAddress ( address , node . port ()), this . socketSendBuffer , this . socketReceiveBuffer ); } catch ( IOException e ) { log . warn ( \"Error connecting to node {}\" , node , e ); // Attempt failed, we'll try again after the backoff connectionStates . disconnected ( nodeConnectionId , now ); // Notify metadata updater of the connection failure metadataUpdater . handleServerDisconnect ( now , nodeConnectionId , Optional . empty ()); } } 4. Selector . connect : 1. \u521b\u5efa SocketChannel \uff0c\u8fdb\u884c\u8fde\u63a5 node \u64cd\u4f5c 2. \u6ce8\u518c SocketChannel \u5230 Selector . nioSelector 3. \u6784\u5efa KafkaChannel \uff08\u901a\u8fc7 (Ssl/Sasl/Plaintext)ChannelBuilder \u6784\u5efa\uff09 4. \u5c06 KafakChannel \u8bb0\u5f55\u5230 Selector . channels \u4e2d kafka \u53d1\u9001\u6d88\u606f\u65f6\u4ee5\u8282\u70b9\u7ec4\u7ec7\u53ef\u53d1\u9001\u7684\u6d88\u606f\uff0c\u5c06\u53d1\u5f80\u540c\u4e00\u4e2a\u8282\u70b9\u7684\u4e0d\u540c topicPartion \u7684\u6570\u636e\u653e\u5728\u4e00\u4e2a\u8bf7\u6c42\u4e2d\u53d1\u9001","title":"Sender \u505a\u4e86\u4ec0\u4e48\uff1f"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#networkclient_1","text":"","title":"NetworkClient \u662f\u5982\u4f55\u53d1\u9001\u6570\u636e\u7684\uff1f"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#selector-poll","text":"poll pollSelectionKeys(Set<SelectionKey> selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos) attempRead(KafkaChannel channel) : \u8c03\u7528 channel.read() \uff0c\u8fd4\u56de\u8bfb\u53d6\u7684 byte \u6570\uff0c KafkaChannel . read() KafkaChannel . receive() \u901a\u8fc7\u8c03\u7528\u81ea\u8eab\u6240\u6301\u6709\u7684 NetworkReceive \u7684 readFrom() \u51fd\u6570\u8fdb\u884c\u6570\u636e\u8bfb\u53d6 \u901a\u8fc7 channel.maybeCompleteReceive() \u5224\u65ad\u662f\u5426\u6536\u5230\u5b8c\u6574\u7684 NetworkReceive \uff0c\u5982\u679c\u662f\uff0c\u5c06 receive \u52a0\u5165\u5230\u81ea\u8eab\u7684 completedReceives attemptWrite(SelectionKey key, KafkaChannel channel, long nowNanos) write(KafkaChannel channel) : \u8c03\u7528 channel.write() \uff0c\u8fd4\u56de\u53d1\u9001 byte \u6570\uff0c KafkaChannel . write() \u901a\u8fc7\u8c03\u7528\u81ea\u8eab\u6240\u6301\u6709 NetworkSend \u7684 writeTo() \u51fd\u6570\u8fdb\u884c\u6570\u636e\u53d1\u9001 java.nio.channels.GatheringByteChannel . write() \u901a\u8fc7 channel.maybeCompleteSend() \u5224\u65ad NetworkSend \u662f\u5426\u53d1\u9001\u5b8c\u6210\uff0c\u5982\u679c\u662f\uff0c\u5c06 send \u52a0\u5165\u5230\u81ea\u8eab\u7684 completedSends \u8fd9\u91cc\u6709\u4e3b\u8981\u7684\u51e0\u70b9\uff1a 1. \u4f55\u65f6\u8fdb\u884c ssl, sasl \u63e1\u624b \uff1a\u5728 Sender . run() \u4e2d\u4f1a\u68c0\u67e5\u8981\u53d1\u9001\u6570\u636e\u5bf9\u5e94\u7684 leader \u662f\u5426\u8fde\u63a5\uff0c\u5982\u679c\u6ca1\u6709\u5219\u8fdb\u884c\u8fde\u63a5\uff0c\u5728 Selector . pollSelectionKeys() \u4e2d\uff0c\u5982\u679c key \u5bf9\u5e94\u7684 channel \u5df2\u8fde\u63a5\u4f46\u8fd8\u672a ready\uff0c\u5219\u8c03\u7528 KafkaChannel . prepare() \u8fdb\u884c ssl \u63e1\u624b\u548c sasl \u63e1\u624b","title":"Selector \u7684 poll"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#_2","text":"https://blog.csdn.net/chunlongyu/article/details/52651960","title":"\u53c2\u8003"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/2-producer%20%E5%B9%82%E7%AD%89%E6%80%A7/","text":"2.2 producer \u5e42\u7b49\u6027","title":"2.2 producer \u5e42\u7b49\u6027"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/2-producer%20%E5%B9%82%E7%AD%89%E6%80%A7/#22-producer","text":"","title":"2.2 producer \u5e42\u7b49\u6027"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/3-producer%20%E5%8A%A0%E5%AF%86%20%E8%AE%A4%E8%AF%81/","text":"2.3 producer \u52a0\u5bc6/\u8ba4\u8bc1 TransportLayer PlaintextTransportLayer, SslTransportLayer Authenticator","title":"2.3 producer \u52a0\u5bc6/\u8ba4\u8bc1"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/3-producer%20%E5%8A%A0%E5%AF%86%20%E8%AE%A4%E8%AF%81/#23-producer","text":"","title":"2.3 producer \u52a0\u5bc6/\u8ba4\u8bc1"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/3-producer%20%E5%8A%A0%E5%AF%86%20%E8%AE%A4%E8%AF%81/#transportlayer","text":"PlaintextTransportLayer, SslTransportLayer","title":"TransportLayer"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/3-producer%20%E5%8A%A0%E5%AF%86%20%E8%AE%A4%E8%AF%81/#authenticator","text":"","title":"Authenticator"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/","text":"consumer \u4ec0\u4e48\u65f6\u5019\u88ab\u8ba4\u4e3a\u4e0b\u7ebf \u5f53 subscribe \u5b8c\u6210\u5e76\u8c03\u7528 poll() \u4e4b\u540e\uff0cconsumer \u4f1a\u81ea\u52a8\u52a0\u5165 group\uff0c\u5e76\u5468\u671f\u6027\u7684\u5411 server \u53d1\u9001 heartbeat\uff0c\u5f53 server \u8ddd\u4e0a\u4e00\u6b21\u6536\u5230 heartbeat \u7684\u65f6\u95f4\u8d85\u8fc7 session.timeout.ms \u540e\uff0c\u4f1a\u8ba4\u4e3a consumer dead \u53e6\u4e00\u79cd\u60c5\u51b5\u662f consumer \u5728\u6301\u7eed\u53d1\u9001 heartbeat\uff0c\u4f46\u662f\u6ca1\u6709\u5b9e\u9645\u7684\u6d88\u8d39\u884c\u4e3a\uff0c\u5f53\u8ddd\u4e0a\u4e00\u6b21\u8c03\u7528 poll() \u7684\u65f6\u95f4\u8d85\u8fc7 max.poll.interval.ms \u540e\uff0c\u4f1a\u5c06 consumer \u79fb\u51fa group\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b consumer \u53ef\u80fd\u4f1a\u5728\u8c03\u7528 commitSync() \u65f6\u51fa\u73b0 offset commit failure\uff0c\u8fd9\u662f\u6b63\u5e38\u7684\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u53ea\u63a5\u53d7\u6765\u81ea\u6d3b\u8dc3 consumer \u7684 offset\u3002 consumer \u63d0\u4f9b 2 \u4e2a\u53c2\u6570\u6765\u63a7\u5236 poll loop \u7684\u884c\u4e3a\uff1a max.poll.interval.ms : \u901a\u8fc7\u63d0\u9ad8\u8fd9\u4e2a\u53c2\u6570\uff0c\u53ef\u4ee5\u7ed9 consumer \u66f4\u591a\u7684\u65f6\u95f4\u5904\u7406\u5355\u6b21 poll \u62c9\u53d6\u7684\u6570\u636e\u3002 max.poll.records : \u9650\u5236\u5355\u6b21 poll \u8c03\u7528\u62c9\u53d6\u7684\u8bb0\u5f55\u6570\u3002 \u5bf9\u4e8e\u6d88\u606f\u5904\u7406\u7528\u65f6\u65e0\u6cd5\u9884\u6d4b\u7684\u7528\u6237\u573a\u666f\uff0c\u4e0a\u9762 2 \u4e2a\u53c2\u6570\u662f\u4e0d\u591f\u7684\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b\u63a8\u8350\u7684\u65b9\u5f0f\u5c06\u5904\u7406\u6d88\u606f\u7684\u6b65\u9aa4\u653e\u5230\u5355\u72ec\u7684\u7ebf\u7a0b\uff0c\u786e\u4fdd consumer \u53ef\u4ee5\u6301\u7eed\u7684\u8c03\u7528 poll\uff0c\u5173\u95ed\u81ea\u52a8 offset commit\uff0c\u5728\u6d88\u606f\u5904\u7406\u5b8c\u4e4b\u540e\u8c03\u7528 commitSync/commitAsync\u3002 \u81ea\u52a8\u63d0\u4ea4 offset\uff1a enable.auto.commit auto.commit.interval.ms \u624b\u52a8\u63d0\u4ea4 offset \u7684 2 \u79cd\u65b9\u5f0f\uff1a commitSync commitAsync \u8ba2\u9605 topic \u7684 2 \u79cd\u65b9\u5f0f\uff1a subscribe : topic \u7c92\u5ea6\uff0c\u5177\u4f53\u7684 partition \u5206\u914d\u7531\u6d88\u8d39\u7ec4\u534f\u8c03\u8005\u5206\u914d assign : topic-partition \u7c92\u5ea6\uff0c\u53ef\u4ee5\u7531\u81ea\u5df1\u5206\u914d partition other offsets.retention.minutes : \u6d88\u8d39\u7ec4\u8bb0\u5f55\u4fdd\u7559\u65f6\u95f4","title":"3. consumer \u89e3\u6790"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/#consumer","text":"\u5f53 subscribe \u5b8c\u6210\u5e76\u8c03\u7528 poll() \u4e4b\u540e\uff0cconsumer \u4f1a\u81ea\u52a8\u52a0\u5165 group\uff0c\u5e76\u5468\u671f\u6027\u7684\u5411 server \u53d1\u9001 heartbeat\uff0c\u5f53 server \u8ddd\u4e0a\u4e00\u6b21\u6536\u5230 heartbeat \u7684\u65f6\u95f4\u8d85\u8fc7 session.timeout.ms \u540e\uff0c\u4f1a\u8ba4\u4e3a consumer dead \u53e6\u4e00\u79cd\u60c5\u51b5\u662f consumer \u5728\u6301\u7eed\u53d1\u9001 heartbeat\uff0c\u4f46\u662f\u6ca1\u6709\u5b9e\u9645\u7684\u6d88\u8d39\u884c\u4e3a\uff0c\u5f53\u8ddd\u4e0a\u4e00\u6b21\u8c03\u7528 poll() \u7684\u65f6\u95f4\u8d85\u8fc7 max.poll.interval.ms \u540e\uff0c\u4f1a\u5c06 consumer \u79fb\u51fa group\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b consumer \u53ef\u80fd\u4f1a\u5728\u8c03\u7528 commitSync() \u65f6\u51fa\u73b0 offset commit failure\uff0c\u8fd9\u662f\u6b63\u5e38\u7684\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u53ea\u63a5\u53d7\u6765\u81ea\u6d3b\u8dc3 consumer \u7684 offset\u3002 consumer \u63d0\u4f9b 2 \u4e2a\u53c2\u6570\u6765\u63a7\u5236 poll loop \u7684\u884c\u4e3a\uff1a max.poll.interval.ms : \u901a\u8fc7\u63d0\u9ad8\u8fd9\u4e2a\u53c2\u6570\uff0c\u53ef\u4ee5\u7ed9 consumer \u66f4\u591a\u7684\u65f6\u95f4\u5904\u7406\u5355\u6b21 poll \u62c9\u53d6\u7684\u6570\u636e\u3002 max.poll.records : \u9650\u5236\u5355\u6b21 poll \u8c03\u7528\u62c9\u53d6\u7684\u8bb0\u5f55\u6570\u3002 \u5bf9\u4e8e\u6d88\u606f\u5904\u7406\u7528\u65f6\u65e0\u6cd5\u9884\u6d4b\u7684\u7528\u6237\u573a\u666f\uff0c\u4e0a\u9762 2 \u4e2a\u53c2\u6570\u662f\u4e0d\u591f\u7684\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b\u63a8\u8350\u7684\u65b9\u5f0f\u5c06\u5904\u7406\u6d88\u606f\u7684\u6b65\u9aa4\u653e\u5230\u5355\u72ec\u7684\u7ebf\u7a0b\uff0c\u786e\u4fdd consumer \u53ef\u4ee5\u6301\u7eed\u7684\u8c03\u7528 poll\uff0c\u5173\u95ed\u81ea\u52a8 offset commit\uff0c\u5728\u6d88\u606f\u5904\u7406\u5b8c\u4e4b\u540e\u8c03\u7528 commitSync/commitAsync\u3002 \u81ea\u52a8\u63d0\u4ea4 offset\uff1a enable.auto.commit auto.commit.interval.ms \u624b\u52a8\u63d0\u4ea4 offset \u7684 2 \u79cd\u65b9\u5f0f\uff1a commitSync commitAsync \u8ba2\u9605 topic \u7684 2 \u79cd\u65b9\u5f0f\uff1a subscribe : topic \u7c92\u5ea6\uff0c\u5177\u4f53\u7684 partition \u5206\u914d\u7531\u6d88\u8d39\u7ec4\u534f\u8c03\u8005\u5206\u914d assign : topic-partition \u7c92\u5ea6\uff0c\u53ef\u4ee5\u7531\u81ea\u5df1\u5206\u914d partition","title":"consumer \u4ec0\u4e48\u65f6\u5019\u88ab\u8ba4\u4e3a\u4e0b\u7ebf"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/#other","text":"offsets.retention.minutes : \u6d88\u8d39\u7ec4\u8bb0\u5f55\u4fdd\u7559\u65f6\u95f4","title":"other"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/1-consumer%20%E6%8B%89%E5%8F%96%E6%B6%88%E6%81%AF/","text":"3.1 consumer \u62c9\u53d6\u6d88\u606f \u4ee3\u7801\u793a\u4f8b Properties props = new Properties (); props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); props . put ( \"group.id\" , \"test\" ); props . put ( \"enable.auto.commit\" , \"true\" ); props . put ( \"auto.commit.interval.ms\" , \"1000\" ); props . put ( \"session.timeout.ms\" , \"30000\" ); props . put ( \"key.deserializer\" , \"org.apache.kafka.common.serialization.StringDeserializer\" ); props . put ( \"value.deserializer\" , \"org.apache.kafka.common.serialization.StringDeserializer\" ); KafkaConsumer < String , String > consumer = new KafkaConsumer <> ( props ); consumer . subscribe ( Arrays . asList ( \"foo\" , \"bar\" )); while ( true ) { ConsumerRecords < String , String > records = consumer . poll ( 100 ); for ( ConsumerRecord < String , String > record : records ) System . out . printf ( \"offset = %d, key = %s, value = %s\" , record . offset (), record . key (), record . value ()); }","title":"3.1 consumer \u62c9\u53d6\u6d88\u606f"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/1-consumer%20%E6%8B%89%E5%8F%96%E6%B6%88%E6%81%AF/#31-consumer","text":"","title":"3.1 consumer \u62c9\u53d6\u6d88\u606f"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/1-consumer%20%E6%8B%89%E5%8F%96%E6%B6%88%E6%81%AF/#_1","text":"Properties props = new Properties (); props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); props . put ( \"group.id\" , \"test\" ); props . put ( \"enable.auto.commit\" , \"true\" ); props . put ( \"auto.commit.interval.ms\" , \"1000\" ); props . put ( \"session.timeout.ms\" , \"30000\" ); props . put ( \"key.deserializer\" , \"org.apache.kafka.common.serialization.StringDeserializer\" ); props . put ( \"value.deserializer\" , \"org.apache.kafka.common.serialization.StringDeserializer\" ); KafkaConsumer < String , String > consumer = new KafkaConsumer <> ( props ); consumer . subscribe ( Arrays . asList ( \"foo\" , \"bar\" )); while ( true ) { ConsumerRecords < String , String > records = consumer . poll ( 100 ); for ( ConsumerRecord < String , String > record : records ) System . out . printf ( \"offset = %d, key = %s, value = %s\" , record . offset (), record . key (), record . value ()); }","title":"\u4ee3\u7801\u793a\u4f8b"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/1-%E6%B6%88%E8%B4%B9%E7%BB%84%E7%AE%A1%E7%90%86/","text":"3.1 \u6d88\u8d39\u7ec4\u7ba1\u7406 \u5206\u533a\u5206\u914d partition.assignment.strategy org.apache.kafka.clients.consumer.RangeAssignor org.apache.kafka.clients.consumer.RoundRobinAssignor __consumer_offsets Key int16 version string group string topic int32 partition Value int16 version int64 offset string metadata int64 commit_timestamp int64 expire_timestamp","title":"3.1-\u6d88\u8d39\u7ec4\u7ba1\u7406"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/1-%E6%B6%88%E8%B4%B9%E7%BB%84%E7%AE%A1%E7%90%86/#31","text":"","title":"3.1 \u6d88\u8d39\u7ec4\u7ba1\u7406"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/1-%E6%B6%88%E8%B4%B9%E7%BB%84%E7%AE%A1%E7%90%86/#_1","text":"partition.assignment.strategy org.apache.kafka.clients.consumer.RangeAssignor org.apache.kafka.clients.consumer.RoundRobinAssignor","title":"\u5206\u533a\u5206\u914d"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/1-%E6%B6%88%E8%B4%B9%E7%BB%84%E7%AE%A1%E7%90%86/#__consumer_offsets","text":"Key int16 version string group string topic int32 partition Value int16 version int64 offset string metadata int64 commit_timestamp int64 expire_timestamp","title":"__consumer_offsets"},{"location":"4-kafka-network-io/","text":"4 kafka \u5bf9 nio \u7684\u5305\u88c5 NetworkSend \u5c5e\u6027\uff1a * destination \u4ee3\u8868\u8282\u70b9\uff0c\u4e0e KafkaChannel \u7684 id \u4e00\u81f4 * buffers \u53d1\u9001\u7684\u6570\u636e \u4f5c\u7528\uff1a * \u4fee\u6539\u539f\u59cb\u53d1\u9001\u7684 ByteBuffer\uff0c\u5c06\u524d 4 \u5b57\u8282\u4fee\u6539\u4e3a ByteBuffer \u7684\u5927\u5c0f\uff0c\u539f\u59cb\u5185\u5bb9\u540e\u79fb 4 \u5b57\u8282 * \u5b9e\u73b0 writeTo \u65b9\u6cd5\uff0c\u5c06\u81ea\u8eab\u6570\u636e buffers \u5199\u5165 channel \u53d1\u9001 NetworkSend \u65f6\uff0c\u53ef\u4ee5\u6839\u636e destination \u627e\u5230\u5bf9\u5e94\u7684 KafkaChannel \uff0c NetworkReceive \u5c5e\u6027\uff1a * source \u4ee3\u8868\u8282\u70b9\uff0c\u4e0e KafkaChannel \u7684 id \u4e00\u81f4 * size \u5b58\u653e\u63a5\u6536\u6570\u636e\u524d 4 byte\uff0c\u5373\u6b64\u6b21\u6570\u636e\u7684\u5927\u5c0f * buffers \u5b58\u653e\u63a5\u6536\u5230\u7684\u6570\u636e \u4f5c\u7528\uff1a * \u5b9e\u73b0 readFrom \u65b9\u6cd5\uff0c\u4ece channel \u4e2d\u8bfb\u6570\u636e\u5230 buffers \u4e2d * \u4ece ReadableByteChannel \u8bfb\u53d6\uff0c\u524d 4 \u5b57\u8282\u4e3a\u5f53\u524d\u6574\u4e2a\u54cd\u5e94\u7684\u957f\u5ea6\uff0c\u6839\u636e\u957f\u5ea6\u8bfb\u53d6\u5185\u5bb9\u5230 buffer KafkaChannel \u6301\u6709\uff1a * id: \u6807\u8bc6\u8282\u70b9\uff0c\u540c\u65f6 selector \u53ef\u4ee5\u6839\u636e id \u53d6\u5230\u5bf9\u5e94\u7684 KafkaChannel * Send(NetworkSend): \u53ea\u80fd\u6301\u6709\u4e00\u4e2a send\uff0cwrite \u53d1\u9001\u5f53\u524d send \u4e4b\u540e\uff0c\u4f1a\u5c06 send \u7f6e\u4e3a null\uff0c\u4e4b\u540e\u53ef\u4ee5\u8bbe\u7f6e\u4e0b\u4e00\u4e2a send * NetworkReceive: \u53ea\u80fd\u6301\u6709\u4e00\u4e2a receive\uff0cread \u63a5\u6536\u5b8c\u6574\u6570\u636e\u540e\uff0c\u5c06 receive \u53ea\u4e3a null, \u4e4b\u540e\u53ef\u4ee5\u63a5\u6536\u4e0b\u4e00\u4e2a receive * TransportLayer: send, receive \u90fd\u901a\u8fc7 TransportLayer \u8f6c\u53d1\u7ed9 SocketChannel TransportLayer \u7ee7\u627f\u81ea ScatteringByteChannel,GatheringByteChannel \u6301\u6709 SelectionKey, SocketChannel KafkaChannel \u6240\u6709\u8bfb\u5199\u90fd\u7ecf\u8fc7 TransportLayer \u8f6c\u53d1\u7ed9\u539f\u6765\u7684 SocketChannel TransportLayer \u76f8\u6bd4\u4e8e\u539f\u6765\u7684 Channel \u4e3b\u8981\u4f5c\u7528\u662f\u589e\u52a0\u4e86 ssl \u4e0e sasl \u7684\u652f\u6301 Selector \u53ef\u4ee5\u76d1\u542c\u591a\u4e2a\u8fde\u63a5\uff0c\u6bcf\u4e2a\u8fde\u63a5\u7531 id \u4ee5\u53ca\u5bf9\u5e94\u7684 KafkaChannel \u6807\u8bc6 connect connect \u5c06 id \u4e0e\u8fdc\u7a0b\u8fde\u63a5\u7684 KafkaChannel \u5bf9\u5e94\u8d77\u6765 send \u53d1\u9001 NetworkSend\uff0c\u6839\u636e NetworkSend \u7684 destination \u53d6\u5230\u5bf9\u5e94\u7684 KafkaChannel\uff0c\u8bbe\u7f6e KafkaChannel \u7684 send poll nio select key\uff0c\u6839\u636e key \u627e\u5230\u5bf9\u5e94\u7684 KafkaChannel\uff0c\u6267\u884c KafkaChannel \u7684 read, write \u65b9\u6cd5 \u8865\u5145\u77e5\u8bc6 interestOps SelectionKey.OP_READ 1<<0 0000 0001 SelectionKey.OP_WRITE 1<<2 0000 0100 SelectionKey.OP_CONNECT 1<<3 0000 1000 SelectionKey.OP_ACCEPT 1<<4 0001 0000 0000 0000 \u53d6\u6d88 OP_CONNECT\uff0c\u589e\u52a0 OP_READ key.interestOps(key.interestOps() & ~SelectionKey.OP_CONNECT | SelectionKey.OP_READ); ScatteringByteChannel,GatheringByteChannel scatteringbytechannel: \u4ece\u901a\u9053\u8bfb\u53d6\u6570\u636e\u5206\u6563\u5230\u591a\u4e2a\u7f13\u51b2\u533a buffer\uff0c gatheringbytechannel: \u5c06\u591a\u4e2a\u7f13\u51b2\u533a buffer \u805a\u96c6\u8d77\u6765\u5199\u5230\u901a\u9053","title":"4. kafka network io"},{"location":"4-kafka-network-io/#4-kafka-nio","text":"","title":"4 kafka \u5bf9 nio \u7684\u5305\u88c5"},{"location":"4-kafka-network-io/#networksend","text":"\u5c5e\u6027\uff1a * destination \u4ee3\u8868\u8282\u70b9\uff0c\u4e0e KafkaChannel \u7684 id \u4e00\u81f4 * buffers \u53d1\u9001\u7684\u6570\u636e \u4f5c\u7528\uff1a * \u4fee\u6539\u539f\u59cb\u53d1\u9001\u7684 ByteBuffer\uff0c\u5c06\u524d 4 \u5b57\u8282\u4fee\u6539\u4e3a ByteBuffer \u7684\u5927\u5c0f\uff0c\u539f\u59cb\u5185\u5bb9\u540e\u79fb 4 \u5b57\u8282 * \u5b9e\u73b0 writeTo \u65b9\u6cd5\uff0c\u5c06\u81ea\u8eab\u6570\u636e buffers \u5199\u5165 channel \u53d1\u9001 NetworkSend \u65f6\uff0c\u53ef\u4ee5\u6839\u636e destination \u627e\u5230\u5bf9\u5e94\u7684 KafkaChannel \uff0c","title":"NetworkSend"},{"location":"4-kafka-network-io/#networkreceive","text":"\u5c5e\u6027\uff1a * source \u4ee3\u8868\u8282\u70b9\uff0c\u4e0e KafkaChannel \u7684 id \u4e00\u81f4 * size \u5b58\u653e\u63a5\u6536\u6570\u636e\u524d 4 byte\uff0c\u5373\u6b64\u6b21\u6570\u636e\u7684\u5927\u5c0f * buffers \u5b58\u653e\u63a5\u6536\u5230\u7684\u6570\u636e \u4f5c\u7528\uff1a * \u5b9e\u73b0 readFrom \u65b9\u6cd5\uff0c\u4ece channel \u4e2d\u8bfb\u6570\u636e\u5230 buffers \u4e2d * \u4ece ReadableByteChannel \u8bfb\u53d6\uff0c\u524d 4 \u5b57\u8282\u4e3a\u5f53\u524d\u6574\u4e2a\u54cd\u5e94\u7684\u957f\u5ea6\uff0c\u6839\u636e\u957f\u5ea6\u8bfb\u53d6\u5185\u5bb9\u5230 buffer","title":"NetworkReceive"},{"location":"4-kafka-network-io/#kafkachannel","text":"\u6301\u6709\uff1a * id: \u6807\u8bc6\u8282\u70b9\uff0c\u540c\u65f6 selector \u53ef\u4ee5\u6839\u636e id \u53d6\u5230\u5bf9\u5e94\u7684 KafkaChannel * Send(NetworkSend): \u53ea\u80fd\u6301\u6709\u4e00\u4e2a send\uff0cwrite \u53d1\u9001\u5f53\u524d send \u4e4b\u540e\uff0c\u4f1a\u5c06 send \u7f6e\u4e3a null\uff0c\u4e4b\u540e\u53ef\u4ee5\u8bbe\u7f6e\u4e0b\u4e00\u4e2a send * NetworkReceive: \u53ea\u80fd\u6301\u6709\u4e00\u4e2a receive\uff0cread \u63a5\u6536\u5b8c\u6574\u6570\u636e\u540e\uff0c\u5c06 receive \u53ea\u4e3a null, \u4e4b\u540e\u53ef\u4ee5\u63a5\u6536\u4e0b\u4e00\u4e2a receive * TransportLayer: send, receive \u90fd\u901a\u8fc7 TransportLayer \u8f6c\u53d1\u7ed9 SocketChannel","title":"KafkaChannel"},{"location":"4-kafka-network-io/#transportlayer","text":"\u7ee7\u627f\u81ea ScatteringByteChannel,GatheringByteChannel \u6301\u6709 SelectionKey, SocketChannel KafkaChannel \u6240\u6709\u8bfb\u5199\u90fd\u7ecf\u8fc7 TransportLayer \u8f6c\u53d1\u7ed9\u539f\u6765\u7684 SocketChannel TransportLayer \u76f8\u6bd4\u4e8e\u539f\u6765\u7684 Channel \u4e3b\u8981\u4f5c\u7528\u662f\u589e\u52a0\u4e86 ssl \u4e0e sasl \u7684\u652f\u6301","title":"TransportLayer"},{"location":"4-kafka-network-io/#selector","text":"\u53ef\u4ee5\u76d1\u542c\u591a\u4e2a\u8fde\u63a5\uff0c\u6bcf\u4e2a\u8fde\u63a5\u7531 id \u4ee5\u53ca\u5bf9\u5e94\u7684 KafkaChannel \u6807\u8bc6","title":"Selector"},{"location":"4-kafka-network-io/#connect","text":"connect \u5c06 id \u4e0e\u8fdc\u7a0b\u8fde\u63a5\u7684 KafkaChannel \u5bf9\u5e94\u8d77\u6765","title":"connect"},{"location":"4-kafka-network-io/#send","text":"\u53d1\u9001 NetworkSend\uff0c\u6839\u636e NetworkSend \u7684 destination \u53d6\u5230\u5bf9\u5e94\u7684 KafkaChannel\uff0c\u8bbe\u7f6e KafkaChannel \u7684 send","title":"send"},{"location":"4-kafka-network-io/#poll","text":"nio select key\uff0c\u6839\u636e key \u627e\u5230\u5bf9\u5e94\u7684 KafkaChannel\uff0c\u6267\u884c KafkaChannel \u7684 read, write \u65b9\u6cd5","title":"poll"},{"location":"4-kafka-network-io/#_1","text":"","title":"\u8865\u5145\u77e5\u8bc6"},{"location":"4-kafka-network-io/#interestops","text":"SelectionKey.OP_READ 1<<0 0000 0001 SelectionKey.OP_WRITE 1<<2 0000 0100 SelectionKey.OP_CONNECT 1<<3 0000 1000 SelectionKey.OP_ACCEPT 1<<4 0001 0000 0000 0000 \u53d6\u6d88 OP_CONNECT\uff0c\u589e\u52a0 OP_READ key.interestOps(key.interestOps() & ~SelectionKey.OP_CONNECT | SelectionKey.OP_READ);","title":"interestOps"},{"location":"4-kafka-network-io/#scatteringbytechannelgatheringbytechannel","text":"scatteringbytechannel: \u4ece\u901a\u9053\u8bfb\u53d6\u6570\u636e\u5206\u6563\u5230\u591a\u4e2a\u7f13\u51b2\u533a buffer\uff0c gatheringbytechannel: \u5c06\u591a\u4e2a\u7f13\u51b2\u533a buffer \u805a\u96c6\u8d77\u6765\u5199\u5230\u901a\u9053","title":"ScatteringByteChannel,GatheringByteChannel"},{"location":"5-release%20note/","text":"5 release note \u611f\u5174\u8da3\u7684 KIP \u91cd\u5206\u533a API \u589e\u52a0\u4e86\u91cd\u5206\u533a\u533a\u7684 API\uff0c\u76f8\u6bd4\u57fa\u4e8e zookeeper \u7684\u65b9\u5f0f\uff0c\u8fd9\u79cd\u91cd\u5206\u533a\u53ef\u4ee5\u7ec8\u6b62\uff0c\u53ef\u4ee5\u5728\u4e0a\u4e00\u4e2a\u8fd8\u5728\u6267\u884c\u65f6\u589e\u52a0\u4efb\u52a1 https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment kafka consumer \u652f\u6301\u4ece replica \u62c9\u6570\u636e https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica controller \u652f\u6301 AlterIsrRequest \u53ef\u4ee5\u4e0d\u518d\u4f9d\u8d56 zookeeper /isr_change_notification \u4f20\u9012 isr \u53d8\u5316\u4fe1\u606f https://cwiki.apache.org/confluence/display/KAFKA/KIP-497%3A+Add+inter-broker+API+to+alter+ISR \u6d88\u8d39\u7ec4\u5728\u5355\u72ec\u7684\u7ebf\u7a0b\u53d1\u9001\u5fc3\u8df3 https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread","title":"5. Release Note"},{"location":"5-release%20note/#5-release-note","text":"\u611f\u5174\u8da3\u7684 KIP","title":"5 release note"},{"location":"5-release%20note/#api","text":"\u589e\u52a0\u4e86\u91cd\u5206\u533a\u533a\u7684 API\uff0c\u76f8\u6bd4\u57fa\u4e8e zookeeper \u7684\u65b9\u5f0f\uff0c\u8fd9\u79cd\u91cd\u5206\u533a\u53ef\u4ee5\u7ec8\u6b62\uff0c\u53ef\u4ee5\u5728\u4e0a\u4e00\u4e2a\u8fd8\u5728\u6267\u884c\u65f6\u589e\u52a0\u4efb\u52a1 https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment","title":"\u91cd\u5206\u533a API"},{"location":"5-release%20note/#kafka-consumer-replica","text":"https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica","title":"kafka consumer \u652f\u6301\u4ece replica \u62c9\u6570\u636e"},{"location":"5-release%20note/#controller-alterisrrequest","text":"\u53ef\u4ee5\u4e0d\u518d\u4f9d\u8d56 zookeeper /isr_change_notification \u4f20\u9012 isr \u53d8\u5316\u4fe1\u606f https://cwiki.apache.org/confluence/display/KAFKA/KIP-497%3A+Add+inter-broker+API+to+alter+ISR","title":"controller \u652f\u6301 AlterIsrRequest"},{"location":"5-release%20note/#_1","text":"https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread","title":"\u6d88\u8d39\u7ec4\u5728\u5355\u72ec\u7684\u7ebf\u7a0b\u53d1\u9001\u5fc3\u8df3"},{"location":"5-release%20note/2.7/","text":"https://cwiki.apache.org/confluence/display/KAFKA/KIP-554%3A+Add+Broker-side+SCRAM+Config+API","title":2.7},{"location":"5-release%20note/2.8.0/","text":"2.8.0 KIP(Kafka Improvement Proposals) KIP-500: which allows you to run Kafka brokers without Apache ZooKeeper, instead depending on an internal Raft implementation. KIP-700: decouples the AdminClient from the Metadata API by adding a new API to directly query the brokers for information about the cluster. KIP-684: support TLS client authentication for SASL_SSL listeners","title":"2.8.0"},{"location":"5-release%20note/2.8.0/#280","text":"KIP(Kafka Improvement Proposals) KIP-500: which allows you to run Kafka brokers without Apache ZooKeeper, instead depending on an internal Raft implementation. KIP-700: decouples the AdminClient from the Metadata API by adding a new API to directly query the brokers for information about the cluster. KIP-684: support TLS client authentication for SASL_SSL listeners","title":"2.8.0"},{"location":"6-src/","text":"6 \u6e90\u7801","title":"6. Src"},{"location":"6-src/#6","text":"","title":"6 \u6e90\u7801"},{"location":"6-src/clients/clients/Metadata/","text":"Metadata implements Closeable \u88ab the client thread (for partitioning) \u548c the background sender thread \u5171\u4eab Metadata is maintained for only a subset of topics, which can be added to over time. \u5f53\u6211\u4eec\u8bf7\u6c42 metadata \u4e2d\u6ca1\u6709\u7684 topic \u65f6\u4f1a\u89e6\u53d1 matadata \u66f4\u65b0\u3002 \u5982\u679c topic expiry \u88ab\u8bbe\u7f6e\uff0c\u4efb\u4f55\u5728 expiry interval \u65f6\u95f4\u5185\u6ca1\u6709\u4f7f\u7528\u7684 topic \u4f1a\u88ab\u4ece metadata \u4e2d\u79fb\u9664\u3002 \u6d88\u8d39\u7ec4\u7981\u6b62 topic expiry \u56e0\u4e3a\u5b83 explicityly manage topics\uff0cproducers \u4f9d\u8d56 topic expiry to limit the refresh set. \u5c5e\u6027 // metadata \u66f4\u65b0\u5931\u8d25\u65f6,\u4e3a\u907f\u514d\u9891\u7e41\u66f4\u65b0 meta,\u6700\u5c0f\u7684\u95f4\u9694\u65f6\u95f4,\u9ed8\u8ba4 100ms private final long refreshBackoffMs ; // metadata \u7684\u8fc7\u671f\u65f6\u95f4, \u9ed8\u8ba4 60,000ms private final long metadataExpireMs ; private int updateVersion ; // bumped on every metadata response private int requestVersion ; // bumped on every new topic addition // \u6700\u540e\u4e00\u6b21\u66f4\u65b0\u7684\u65f6\u95f4\uff08\u5305\u542b\u66f4\u65b0\u5931\u8d25\u7684\u60c5\u51b5\uff09 private long lastRefreshMs ; // \u6700\u540e\u4e00\u6b21\u6210\u529f\u66f4\u65b0\u7684\u65f6\u95f4 private long lastSuccessfulRefreshMs ; private KafkaException fatalException ; private Set < String > invalidTopics ; private Set < String > unauthorizedTopics ; private boolean needFullUpdate ; private boolean needPartialUpdate ; private final ClusterResourceListeners clusterResourceListeners ; private boolean isClosed ; private final Map < TopicPartition , Integer > lastSeenLeaderEpochs ; private MetadataCache cache = MetadataCache . empty (); MetadataCache MetadataCache private final String clusterId ; private final Map < Integer , Node > nodes ; private final Set < String > unauthorizedTopics ; private final Set < String > invalidTopics ; private final Set < String > internalTopics ; private final Node controller ; private final Map < TopicPartition , PartitionMetadata > metadataByPartition ; private Cluster clusterInstance ; \u65b9\u6cd5 add","title":"Metadata"},{"location":"6-src/clients/clients/Metadata/#metadata","text":"implements Closeable \u88ab the client thread (for partitioning) \u548c the background sender thread \u5171\u4eab Metadata is maintained for only a subset of topics, which can be added to over time. \u5f53\u6211\u4eec\u8bf7\u6c42 metadata \u4e2d\u6ca1\u6709\u7684 topic \u65f6\u4f1a\u89e6\u53d1 matadata \u66f4\u65b0\u3002 \u5982\u679c topic expiry \u88ab\u8bbe\u7f6e\uff0c\u4efb\u4f55\u5728 expiry interval \u65f6\u95f4\u5185\u6ca1\u6709\u4f7f\u7528\u7684 topic \u4f1a\u88ab\u4ece metadata \u4e2d\u79fb\u9664\u3002 \u6d88\u8d39\u7ec4\u7981\u6b62 topic expiry \u56e0\u4e3a\u5b83 explicityly manage topics\uff0cproducers \u4f9d\u8d56 topic expiry to limit the refresh set.","title":"Metadata"},{"location":"6-src/clients/clients/Metadata/#_1","text":"// metadata \u66f4\u65b0\u5931\u8d25\u65f6,\u4e3a\u907f\u514d\u9891\u7e41\u66f4\u65b0 meta,\u6700\u5c0f\u7684\u95f4\u9694\u65f6\u95f4,\u9ed8\u8ba4 100ms private final long refreshBackoffMs ; // metadata \u7684\u8fc7\u671f\u65f6\u95f4, \u9ed8\u8ba4 60,000ms private final long metadataExpireMs ; private int updateVersion ; // bumped on every metadata response private int requestVersion ; // bumped on every new topic addition // \u6700\u540e\u4e00\u6b21\u66f4\u65b0\u7684\u65f6\u95f4\uff08\u5305\u542b\u66f4\u65b0\u5931\u8d25\u7684\u60c5\u51b5\uff09 private long lastRefreshMs ; // \u6700\u540e\u4e00\u6b21\u6210\u529f\u66f4\u65b0\u7684\u65f6\u95f4 private long lastSuccessfulRefreshMs ; private KafkaException fatalException ; private Set < String > invalidTopics ; private Set < String > unauthorizedTopics ; private boolean needFullUpdate ; private boolean needPartialUpdate ; private final ClusterResourceListeners clusterResourceListeners ; private boolean isClosed ; private final Map < TopicPartition , Integer > lastSeenLeaderEpochs ; private MetadataCache cache = MetadataCache . empty ();","title":"\u5c5e\u6027"},{"location":"6-src/clients/clients/Metadata/#metadatacache","text":"MetadataCache private final String clusterId ; private final Map < Integer , Node > nodes ; private final Set < String > unauthorizedTopics ; private final Set < String > invalidTopics ; private final Set < String > internalTopics ; private final Node controller ; private final Map < TopicPartition , PartitionMetadata > metadataByPartition ; private Cluster clusterInstance ;","title":"MetadataCache"},{"location":"6-src/clients/clients/Metadata/#_2","text":"","title":"\u65b9\u6cd5"},{"location":"6-src/clients/clients/Metadata/#add","text":"","title":"add"},{"location":"6-src/clients/clients/NetworkClient/","text":"NetworkClient implements KafkaClient \u65b9\u6cd5 poll /** * Do actual reads and writes to sockets. * * @param timeout The maximum amount of time to wait (in ms) for responses if there are none immediately, * must be non-negative. The actual timeout will be the minimum of timeout, request timeout and * metadata timeout * @param now The current time in milliseconds * @return The list of responses received */ @Override public List < ClientResponse > poll ( long timeout , long now ) { ensureActive (); if ( ! abortedSends . isEmpty ()) { // If there are aborted sends because of unsupported version exceptions or disconnects, // handle them immediately without waiting for Selector#poll. List < ClientResponse > responses = new ArrayList <> (); handleAbortedSends ( responses ); completeResponses ( responses ); return responses ; } long metadataTimeout = metadataUpdater . maybeUpdate ( now ); try { this . selector . poll ( Utils . min ( timeout , metadataTimeout , defaultRequestTimeoutMs )); } catch ( IOException e ) { log . error ( \"Unexpected error during I/O\" , e ); } // process completed actions long updatedNow = this . time . milliseconds (); List < ClientResponse > responses = new ArrayList <> (); handleCompletedSends ( responses , updatedNow ); handleCompletedReceives ( responses , updatedNow ); handleDisconnections ( responses , updatedNow ); handleConnections (); handleInitiateApiVersionRequests ( updatedNow ); handleTimedOutConnections ( responses , updatedNow ); handleTimedOutRequests ( responses , updatedNow ); completeResponses ( responses ); return responses ; } \u5982\u679c\u9700\u8981\u66f4\u65b0 Metadata\uff0c\u90a3\u4e48\u5c31\u53d1\u9001 Metadata \u8bf7\u6c42 \u8c03\u7528 Selector \u8fdb\u884c\u76f8\u5e94\u7684 IO \u64cd\u4f5c \u5904\u7406 server \u7aef\u7684 response \u53ca\u4e00\u4e9b\u5176\u4ed6\u64cd\u4f5c send /** * Queue up the given request for sending. Requests can only be sent out to ready nodes. * @param request The request * @param now The current timestamp */ @Override public void send ( ClientRequest request , long now ) { doSend ( request , false , now ); } doSend \u5185\u90e8\u7c7b DefaultMetadataUpdater maybeUpdate \u5982\u679c Metadata \u9700\u8981\u66f4\u65b0\uff0c\u9009\u62e9\u8fde\u63a5\u6570\u6700\u5c0f\u7684 node \u53d1\u9001 Metadata \u8bf7\u6c42","title":"Network Client"},{"location":"6-src/clients/clients/NetworkClient/#networkclient","text":"implements KafkaClient","title":"NetworkClient"},{"location":"6-src/clients/clients/NetworkClient/#_1","text":"","title":"\u65b9\u6cd5"},{"location":"6-src/clients/clients/NetworkClient/#poll","text":"/** * Do actual reads and writes to sockets. * * @param timeout The maximum amount of time to wait (in ms) for responses if there are none immediately, * must be non-negative. The actual timeout will be the minimum of timeout, request timeout and * metadata timeout * @param now The current time in milliseconds * @return The list of responses received */ @Override public List < ClientResponse > poll ( long timeout , long now ) { ensureActive (); if ( ! abortedSends . isEmpty ()) { // If there are aborted sends because of unsupported version exceptions or disconnects, // handle them immediately without waiting for Selector#poll. List < ClientResponse > responses = new ArrayList <> (); handleAbortedSends ( responses ); completeResponses ( responses ); return responses ; } long metadataTimeout = metadataUpdater . maybeUpdate ( now ); try { this . selector . poll ( Utils . min ( timeout , metadataTimeout , defaultRequestTimeoutMs )); } catch ( IOException e ) { log . error ( \"Unexpected error during I/O\" , e ); } // process completed actions long updatedNow = this . time . milliseconds (); List < ClientResponse > responses = new ArrayList <> (); handleCompletedSends ( responses , updatedNow ); handleCompletedReceives ( responses , updatedNow ); handleDisconnections ( responses , updatedNow ); handleConnections (); handleInitiateApiVersionRequests ( updatedNow ); handleTimedOutConnections ( responses , updatedNow ); handleTimedOutRequests ( responses , updatedNow ); completeResponses ( responses ); return responses ; } \u5982\u679c\u9700\u8981\u66f4\u65b0 Metadata\uff0c\u90a3\u4e48\u5c31\u53d1\u9001 Metadata \u8bf7\u6c42 \u8c03\u7528 Selector \u8fdb\u884c\u76f8\u5e94\u7684 IO \u64cd\u4f5c \u5904\u7406 server \u7aef\u7684 response \u53ca\u4e00\u4e9b\u5176\u4ed6\u64cd\u4f5c","title":"poll"},{"location":"6-src/clients/clients/NetworkClient/#send","text":"/** * Queue up the given request for sending. Requests can only be sent out to ready nodes. * @param request The request * @param now The current timestamp */ @Override public void send ( ClientRequest request , long now ) { doSend ( request , false , now ); }","title":"send"},{"location":"6-src/clients/clients/NetworkClient/#dosend","text":"","title":"doSend"},{"location":"6-src/clients/clients/NetworkClient/#_2","text":"","title":"\u5185\u90e8\u7c7b"},{"location":"6-src/clients/clients/NetworkClient/#defaultmetadataupdater","text":"","title":"DefaultMetadataUpdater"},{"location":"6-src/clients/clients/NetworkClient/#maybeupdate","text":"\u5982\u679c Metadata \u9700\u8981\u66f4\u65b0\uff0c\u9009\u62e9\u8fde\u63a5\u6570\u6700\u5c0f\u7684 node \u53d1\u9001 Metadata \u8bf7\u6c42","title":"maybeUpdate"},{"location":"6-src/clients/clients/producer/KafkaProducer/","text":"Producer \u751f\u4ea7\u8005\u7ef4\u6301\u4e00\u4e2a pool of buffer space \u5b58\u50a8\u8fd8\u6ca1\u6709\u53d1\u5f80 server \u7684 record \u540c\u65f6\u6709\u80cc\u540e\u7684 I/O thread \u8d1f\u8d23\u5c06 record \u53d1\u5f80 server send \u65b9\u6cd5\u662f\u5f02\u6b65\u7684\uff0c\u5f53\u8c03\u7528\u65f6\u628a record \u52a0\u5165\u4e00\u4e2a buffer of pending record sends \u5e76\u7acb\u5373\u8fd4\u56de\u3002\u8fd9\u5141\u8bb8 producer \u901a\u8fc7\u6253\u5305\u5355\u72ec\u7684 record \u63d0\u4f9b\u6548\u7387 batch.size : producer \u4e3a\u6bcf\u4e2a partition \u7ef4\u62a4\u4e00\u4e2a\u672a\u53d1\u9001 record \u7684 buffer\uff0cbuffer \u7684\u5927\u5c0f\u7531 batch.size \u914d\u7f6e\u3002\u589e\u52a0\u914d\u7f6e\u53ef\u4ee5\u8ba9 record \u6253\u5305\u589e\u5927\uff0c\u63d0\u9ad8\u53d1\u9001\u6548\u7387\uff0c\u4f46\u4f1a\u589e\u52a0\u5185\u5b58\u4f7f\u7528\uff0c\u964d\u4f4e\u53d1\u9001\u65f6\u6548 linger.ms : \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0ca buffer is available to send immediately even if there is additional unused space in the buffseer buffer.memory : \u63a7\u5236 producer \u53ef\u4ee5\u4f7f\u7528\u7684 buffer \u7684\u5927\u5c0f\uff0c\u5982\u679c record \u53d1\u9001\u5feb\u4e8e producer \u4e0e server \u7684\u4ea4\u4e92\uff0cbuffer \u53ef\u7528\u7a7a\u95f4\u4f1a\u88ab\u8017\u5c3d\uff0c send \u8c03\u7528\u4f1a\u88ab\u963b\u585e\u3002 max.block.ms \u914d\u7f6e\u963b\u585e\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u5982\u679c\u8d85\u65f6\uff0c\u4f1a\u629b\u51fa TimeoutException \u5f02\u5e38 0.11 \u4e4b\u540e\uff0cKafka \u652f\u6301\u5e42\u7b49(idempotent) \u4e0e\u4e8b\u52a1(transacional) \u4e3a\u4e86\u652f\u6301\u5e42\u7b49\uff0c enable.idempotence \u5fc5\u987b\u88ab\u8bbe\u7f6e\u4e3a true \uff0c\u8bbe\u7f6e\u4e4b\u540e retries \u4f1a\u88ab\u9ed8\u8ba4\u8bbe\u4e3a Integer.MAX_VALUE \u5e76\u4e14 acks \u4f1a\u9ed8\u8ba4\u8bbe\u4e3a all \u4e3a\u4e86\u652f\u6301\u4e8b\u52a1\uff0c transactional.id \u5fc5\u987b\u88ab\u8bbe\u7f6e\uff0c\u8bbe\u7f6e\u4e4b\u540e\uff0c enable.idempotence \u4f1a\u81ea\u52a8\u5f00\u542f\u3002\u6b64\u5916: topic which are included in transactions \u5e94\u8be5 configured for durability\uff0c\u5177\u4f53\u7684: replication.factor \u81f3\u5c11\u4e3a 3\uff0c min.insync.replicas \u5e94\u8be5\u8bbe\u7f6e\u4e3a 2\u3002 \u4e3a\u4e86\u7aef\u5230\u7aef\u7684\u4e8b\u52a1\u6027\u4fdd\u8bc1\uff0cconsumer \u5e94\u8be5\u88ab\u8bbe\u7f6e\u4e3a\u53ea\u8bfb committed messages \u5c5e\u6027 private final Logger log ; private static final String JMX_PREFIX = \"kafka.producer\" ; public static final String NETWORK_THREAD_PREFIX = \"kafka-producer-network-thread\" ; public static final String PRODUCER_METRIC_GROUP_NAME = \"producer-metrics\" ; private final String clientId ; // Visible for testing final Metrics metrics ; private final Partitioner partitioner ; private final int maxRequestSize ; private final long totalMemorySize ; private final ProducerMetadata metadata ; private final RecordAccumulator accumulator ; private final Sender sender ; private final Thread ioThread ; private final CompressionType compressionType ; private final Sensor errors ; private final Time time ; private final Serializer < K > keySerializer ; private final Serializer < V > valueSerializer ; private final ProducerConfig producerConfig ; private final long maxBlockTimeMs ; private final ProducerInterceptors < K , V > interceptors ; private final ApiVersions apiVersions ; private final TransactionManager transactionManager ; \u6784\u9020\u65b9\u6cd5 KafkaProducer \u6784\u9020\u65b9\u6cd5\u4e2d\uff0c\u521d\u59cb\u5982\u4e0b\u5173\u952e\u5c5e\u6027 this.partitioner \u5206\u533a\u7b97\u6cd5 this.accumulator = new RecordAccumulator this.metadata = new ProducerMetadata this . metadata = new ProducerMetadata ( retryBackoffMs , config . getLong ( ProducerConfig . METADATA_MAX_AGE_CONFIG ), config . getLong ( ProducerConfig . METADATA_MAX_IDLE_CONFIG ), logContext , clusterResourceListeners , Time . SYSTEM ); this . metadata . bootstrap ( addresses ); this.sender = newSender(logContext, kafkaClient, this.metadata) \u540c\u65f6\u5f00\u542f\u7ebf\u7a0b\uff0c\u8fd0\u884c sender this . ioThread = new KafkaThread ( ioThreadName , this . sender , true ); this . ioThread . start (); send /** * Asynchronously send a record to a topic and invoke the provided callback when the send has been acknowledged. * <p> * The send is asynchronous and this method will return immediately once the record has been stored in the buffer of * records waiting to be sent. This allows sending many records in parallel without blocking to wait for the * response after each one. * <p> * The result of the send is a {@link RecordMetadata} specifying the partition the record was sent to, the offset * it was assigned and the timestamp of the record. If * {@link org.apache.kafka.common.record.TimestampType#CREATE_TIME CreateTime} is used by the topic, the timestamp * will be the user provided timestamp or the record send time if the user did not specify a timestamp for the * record. If {@link org.apache.kafka.common.record.TimestampType#LOG_APPEND_TIME LogAppendTime} is used for the * topic, the timestamp will be the Kafka broker local time when the message is appended. * <p> * Since the send call is asynchronous it returns a {@link java.util.concurrent.Future Future} for the * {@link RecordMetadata} that will be assigned to this record. Invoking {@link java.util.concurrent.Future#get() * get()} on this future will block until the associated request completes and then return the metadata for the record * or throw any exception that occurred while sending the record. * <p> * If you want to simulate a simple blocking call you can call the <code>get()</code> method immediately: * * <pre> * {@code * byte[] key = \"key\".getBytes(); * byte[] value = \"value\".getBytes(); * ProducerRecord<byte[],byte[]> record = new ProducerRecord<byte[],byte[]>(\"my-topic\", key, value) * producer.send(record).get(); * }</pre> * <p> * Fully non-blocking usage can make use of the {@link Callback} parameter to provide a callback that * will be invoked when the request is complete. * * <pre> * {@code * ProducerRecord<byte[],byte[]> record = new ProducerRecord<byte[],byte[]>(\"the-topic\", key, value); * producer.send(myRecord, * new Callback() { * public void onCompletion(RecordMetadata metadata, Exception e) { * if(e != null) { * e.printStackTrace(); * } else { * System.out.println(\"The offset of the record we just sent is: \" + metadata.offset()); * } * } * }); * } * </pre> * * Callbacks for records being sent to the same partition are guaranteed to execute in order. That is, in the * following example <code>callback1</code> is guaranteed to execute before <code>callback2</code>: * * <pre> * {@code * producer.send(new ProducerRecord<byte[],byte[]>(topic, partition, key1, value1), callback1); * producer.send(new ProducerRecord<byte[],byte[]>(topic, partition, key2, value2), callback2); * } * </pre> * <p> * When used as part of a transaction, it is not necessary to define a callback or check the result of the future * in order to detect errors from <code>send</code>. If any of the send calls failed with an irrecoverable error, * the final {@link #commitTransaction()} call will fail and throw the exception from the last failed send. When * this happens, your application should call {@link #abortTransaction()} to reset the state and continue to send * data. * </p> * <p> * Some transactional send errors cannot be resolved with a call to {@link #abortTransaction()}. In particular, * if a transactional send finishes with a {@link ProducerFencedException}, a {@link org.apache.kafka.common.errors.OutOfOrderSequenceException}, * a {@link org.apache.kafka.common.errors.UnsupportedVersionException}, or an * {@link org.apache.kafka.common.errors.AuthorizationException}, then the only option left is to call {@link #close()}. * Fatal errors cause the producer to enter a defunct state in which future API calls will continue to raise * the same underyling error wrapped in a new {@link KafkaException}. * </p> * <p> * It is a similar picture when idempotence is enabled, but no <code>transactional.id</code> has been configured. * In this case, {@link org.apache.kafka.common.errors.UnsupportedVersionException} and * {@link org.apache.kafka.common.errors.AuthorizationException} are considered fatal errors. However, * {@link ProducerFencedException} does not need to be handled. Additionally, it is possible to continue * sending after receiving an {@link org.apache.kafka.common.errors.OutOfOrderSequenceException}, but doing so * can result in out of order delivery of pending messages. To ensure proper ordering, you should close the * producer and create a new instance. * </p> * <p> * If the message format of the destination topic is not upgraded to 0.11.0.0, idempotent and transactional * produce requests will fail with an {@link org.apache.kafka.common.errors.UnsupportedForMessageFormatException} * error. If this is encountered during a transaction, it is possible to abort and continue. But note that future * sends to the same topic will continue receiving the same exception until the topic is upgraded. * </p> * <p> * Note that callbacks will generally execute in the I/O thread of the producer and so should be reasonably fast or * they will delay the sending of messages from other threads. If you want to execute blocking or computationally * expensive callbacks it is recommended to use your own {@link java.util.concurrent.Executor} in the callback body * to parallelize processing. * * @param record The record to send * @param callback A user-supplied callback to execute when the record has been acknowledged by the server (null * indicates no callback) * * @throws AuthenticationException if authentication fails. See the exception for more details * @throws AuthorizationException fatal error indicating that the producer is not allowed to write * @throws IllegalStateException if a transactional.id has been configured and no transaction has been started, or * when send is invoked after producer has been closed. * @throws InterruptException If the thread is interrupted while blocked * @throws SerializationException If the key or value are not valid objects given the configured serializers * @throws KafkaException If a Kafka related error occurs that does not belong to the public API exceptions. */ @Override public Future < RecordMetadata > send ( ProducerRecord < K , V > record , Callback callback ) { // intercept the record, which can be potentially modified; this method does not throw exceptions ProducerRecord < K , V > interceptedRecord = this . interceptors . onSend ( record ); return doSend ( interceptedRecord , callback ); } doSend /** * Implementation of asynchronously send a record to a topic. */ private Future < RecordMetadata > doSend ( ProducerRecord < K , V > record , Callback callback ) { TopicPartition tp = null ; try { throwIfProducerClosed (); // first make sure the metadata for the topic is available long nowMs = time . milliseconds (); ClusterAndWaitTime clusterAndWaitTime ; try { clusterAndWaitTime = waitOnMetadata ( record . topic (), record . partition (), nowMs , maxBlockTimeMs ); } catch ( KafkaException e ) { if ( metadata . isClosed ()) throw new KafkaException ( \"Producer closed while send in progress\" , e ); throw e ; } nowMs += clusterAndWaitTime . waitedOnMetadataMs ; long remainingWaitMs = Math . max ( 0 , maxBlockTimeMs - clusterAndWaitTime . waitedOnMetadataMs ); Cluster cluster = clusterAndWaitTime . cluster ; // 3. \u5e8f\u5217\u5316 key, value byte [] serializedKey ; try { serializedKey = keySerializer . serialize ( record . topic (), record . headers (), record . key ()); } catch ( ClassCastException cce ) { throw new SerializationException ( \"Can't convert key of class \" + record . key (). getClass (). getName () + \" to class \" + producerConfig . getClass ( ProducerConfig . KEY_SERIALIZER_CLASS_CONFIG ). getName () + \" specified in key.serializer\" , cce ); } byte [] serializedValue ; try { serializedValue = valueSerializer . serialize ( record . topic (), record . headers (), record . value ()); } catch ( ClassCastException cce ) { throw new SerializationException ( \"Can't convert value of class \" + record . value (). getClass (). getName () + \" to class \" + producerConfig . getClass ( ProducerConfig . VALUE_SERIALIZER_CLASS_CONFIG ). getName () + \" specified in value.serializer\" , cce ); } // 4. \u8ba1\u7b97\u6d88\u606f\u6240\u5904\u5206\u533a int partition = partition ( record , serializedKey , serializedValue , cluster ); tp = new TopicPartition ( record . topic (), partition ); setReadOnly ( record . headers ()); Header [] headers = record . headers (). toArray (); // 5. \u8ba1\u7b97\u6d88\u606f\u5927\u5c0f\uff0c\u786e\u5b9a\u4e0d\u8d85\u8fc7 `max.request.size`, `buffer.memory` int serializedSize = AbstractRecords . estimateSizeInBytesUpperBound ( apiVersions . maxUsableProduceMagic (), compressionType , serializedKey , serializedValue , headers ); ensureValidRecordSize ( serializedSize ); // 6. \u83b7\u53d6\u6d88\u606f timestamp\uff0c\u5982\u679c\u6ca1\u6709\u5c31\u7528\u5f53\u524d\u65f6\u95f4 long timestamp = record . timestamp () == null ? nowMs : record . timestamp (); if ( log . isTraceEnabled ()) { log . trace ( \"Attempting to append record {} with callback {} to topic {} partition {}\" , record , callback , record . topic (), partition ); } // producer callback will make sure to call both 'callback' and interceptor callback Callback interceptCallback = new InterceptorCallback <> ( callback , this . interceptors , tp ); if ( transactionManager != null && transactionManager . isTransactional ()) { transactionManager . failIfNotReadyForSend (); } // 7. \u5c06\u6d88\u606f\u8ffd\u52a0\u5230 accumulator RecordAccumulator . RecordAppendResult result = accumulator . append ( tp , timestamp , serializedKey , serializedValue , headers , interceptCallback , remainingWaitMs , true , nowMs ); if ( result . abortForNewBatch ) { int prevPartition = partition ; partitioner . onNewBatch ( record . topic (), cluster , prevPartition ); partition = partition ( record , serializedKey , serializedValue , cluster ); tp = new TopicPartition ( record . topic (), partition ); if ( log . isTraceEnabled ()) { log . trace ( \"Retrying append due to new batch creation for topic {} partition {}. The old partition was {}\" , record . topic (), partition , prevPartition ); } // producer callback will make sure to call both 'callback' and interceptor callback interceptCallback = new InterceptorCallback <> ( callback , this . interceptors , tp ); result = accumulator . append ( tp , timestamp , serializedKey , serializedValue , headers , interceptCallback , remainingWaitMs , false , nowMs ); } if ( transactionManager != null && transactionManager . isTransactional ()) transactionManager . maybeAddPartitionToTransaction ( tp ); // 8. \u5982\u679c `batch` \u6ee1\u4e86\u6216\u8005\u65b0\u521b\u5efa\u4e86 `batch`\uff0c\u5524\u9192 sender if ( result . batchIsFull || result . newBatchCreated ) { log . trace ( \"Waking up the sender since topic {} partition {} is either full or getting a new batch\" , record . topic (), partition ); this . sender . wakeup (); } return result . future ; // handling exceptions and record the errors; // for API exceptions return them in the future, // for other exceptions throw directly } catch ( ApiException e ) { log . debug ( \"Exception occurred during message send:\" , e ); if ( callback != null ) callback . onCompletion ( null , e ); this . errors . record (); this . interceptors . onSendError ( record , tp , e ); return new FutureFailure ( e ); } catch ( InterruptedException e ) { this . errors . record (); this . interceptors . onSendError ( record , tp , e ); throw new InterruptException ( e ); } catch ( KafkaException e ) { this . errors . record (); this . interceptors . onSendError ( record , tp , e ); throw e ; } catch ( Exception e ) { // we notify interceptor about all exceptions, since onSend is called before anything else in this method this . interceptors . onSendError ( record , tp , e ); throw e ; } }","title":"Kafka Producer"},{"location":"6-src/clients/clients/producer/KafkaProducer/#producer","text":"\u751f\u4ea7\u8005\u7ef4\u6301\u4e00\u4e2a pool of buffer space \u5b58\u50a8\u8fd8\u6ca1\u6709\u53d1\u5f80 server \u7684 record \u540c\u65f6\u6709\u80cc\u540e\u7684 I/O thread \u8d1f\u8d23\u5c06 record \u53d1\u5f80 server send \u65b9\u6cd5\u662f\u5f02\u6b65\u7684\uff0c\u5f53\u8c03\u7528\u65f6\u628a record \u52a0\u5165\u4e00\u4e2a buffer of pending record sends \u5e76\u7acb\u5373\u8fd4\u56de\u3002\u8fd9\u5141\u8bb8 producer \u901a\u8fc7\u6253\u5305\u5355\u72ec\u7684 record \u63d0\u4f9b\u6548\u7387 batch.size : producer \u4e3a\u6bcf\u4e2a partition \u7ef4\u62a4\u4e00\u4e2a\u672a\u53d1\u9001 record \u7684 buffer\uff0cbuffer \u7684\u5927\u5c0f\u7531 batch.size \u914d\u7f6e\u3002\u589e\u52a0\u914d\u7f6e\u53ef\u4ee5\u8ba9 record \u6253\u5305\u589e\u5927\uff0c\u63d0\u9ad8\u53d1\u9001\u6548\u7387\uff0c\u4f46\u4f1a\u589e\u52a0\u5185\u5b58\u4f7f\u7528\uff0c\u964d\u4f4e\u53d1\u9001\u65f6\u6548 linger.ms : \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0ca buffer is available to send immediately even if there is additional unused space in the buffseer buffer.memory : \u63a7\u5236 producer \u53ef\u4ee5\u4f7f\u7528\u7684 buffer \u7684\u5927\u5c0f\uff0c\u5982\u679c record \u53d1\u9001\u5feb\u4e8e producer \u4e0e server \u7684\u4ea4\u4e92\uff0cbuffer \u53ef\u7528\u7a7a\u95f4\u4f1a\u88ab\u8017\u5c3d\uff0c send \u8c03\u7528\u4f1a\u88ab\u963b\u585e\u3002 max.block.ms \u914d\u7f6e\u963b\u585e\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u5982\u679c\u8d85\u65f6\uff0c\u4f1a\u629b\u51fa TimeoutException \u5f02\u5e38 0.11 \u4e4b\u540e\uff0cKafka \u652f\u6301\u5e42\u7b49(idempotent) \u4e0e\u4e8b\u52a1(transacional) \u4e3a\u4e86\u652f\u6301\u5e42\u7b49\uff0c enable.idempotence \u5fc5\u987b\u88ab\u8bbe\u7f6e\u4e3a true \uff0c\u8bbe\u7f6e\u4e4b\u540e retries \u4f1a\u88ab\u9ed8\u8ba4\u8bbe\u4e3a Integer.MAX_VALUE \u5e76\u4e14 acks \u4f1a\u9ed8\u8ba4\u8bbe\u4e3a all \u4e3a\u4e86\u652f\u6301\u4e8b\u52a1\uff0c transactional.id \u5fc5\u987b\u88ab\u8bbe\u7f6e\uff0c\u8bbe\u7f6e\u4e4b\u540e\uff0c enable.idempotence \u4f1a\u81ea\u52a8\u5f00\u542f\u3002\u6b64\u5916: topic which are included in transactions \u5e94\u8be5 configured for durability\uff0c\u5177\u4f53\u7684: replication.factor \u81f3\u5c11\u4e3a 3\uff0c min.insync.replicas \u5e94\u8be5\u8bbe\u7f6e\u4e3a 2\u3002 \u4e3a\u4e86\u7aef\u5230\u7aef\u7684\u4e8b\u52a1\u6027\u4fdd\u8bc1\uff0cconsumer \u5e94\u8be5\u88ab\u8bbe\u7f6e\u4e3a\u53ea\u8bfb committed messages","title":"Producer"},{"location":"6-src/clients/clients/producer/KafkaProducer/#_1","text":"private final Logger log ; private static final String JMX_PREFIX = \"kafka.producer\" ; public static final String NETWORK_THREAD_PREFIX = \"kafka-producer-network-thread\" ; public static final String PRODUCER_METRIC_GROUP_NAME = \"producer-metrics\" ; private final String clientId ; // Visible for testing final Metrics metrics ; private final Partitioner partitioner ; private final int maxRequestSize ; private final long totalMemorySize ; private final ProducerMetadata metadata ; private final RecordAccumulator accumulator ; private final Sender sender ; private final Thread ioThread ; private final CompressionType compressionType ; private final Sensor errors ; private final Time time ; private final Serializer < K > keySerializer ; private final Serializer < V > valueSerializer ; private final ProducerConfig producerConfig ; private final long maxBlockTimeMs ; private final ProducerInterceptors < K , V > interceptors ; private final ApiVersions apiVersions ; private final TransactionManager transactionManager ;","title":"\u5c5e\u6027"},{"location":"6-src/clients/clients/producer/KafkaProducer/#_2","text":"KafkaProducer \u6784\u9020\u65b9\u6cd5\u4e2d\uff0c\u521d\u59cb\u5982\u4e0b\u5173\u952e\u5c5e\u6027 this.partitioner \u5206\u533a\u7b97\u6cd5 this.accumulator = new RecordAccumulator this.metadata = new ProducerMetadata this . metadata = new ProducerMetadata ( retryBackoffMs , config . getLong ( ProducerConfig . METADATA_MAX_AGE_CONFIG ), config . getLong ( ProducerConfig . METADATA_MAX_IDLE_CONFIG ), logContext , clusterResourceListeners , Time . SYSTEM ); this . metadata . bootstrap ( addresses ); this.sender = newSender(logContext, kafkaClient, this.metadata) \u540c\u65f6\u5f00\u542f\u7ebf\u7a0b\uff0c\u8fd0\u884c sender this . ioThread = new KafkaThread ( ioThreadName , this . sender , true ); this . ioThread . start ();","title":"\u6784\u9020\u65b9\u6cd5"},{"location":"6-src/clients/clients/producer/KafkaProducer/#send","text":"/** * Asynchronously send a record to a topic and invoke the provided callback when the send has been acknowledged. * <p> * The send is asynchronous and this method will return immediately once the record has been stored in the buffer of * records waiting to be sent. This allows sending many records in parallel without blocking to wait for the * response after each one. * <p> * The result of the send is a {@link RecordMetadata} specifying the partition the record was sent to, the offset * it was assigned and the timestamp of the record. If * {@link org.apache.kafka.common.record.TimestampType#CREATE_TIME CreateTime} is used by the topic, the timestamp * will be the user provided timestamp or the record send time if the user did not specify a timestamp for the * record. If {@link org.apache.kafka.common.record.TimestampType#LOG_APPEND_TIME LogAppendTime} is used for the * topic, the timestamp will be the Kafka broker local time when the message is appended. * <p> * Since the send call is asynchronous it returns a {@link java.util.concurrent.Future Future} for the * {@link RecordMetadata} that will be assigned to this record. Invoking {@link java.util.concurrent.Future#get() * get()} on this future will block until the associated request completes and then return the metadata for the record * or throw any exception that occurred while sending the record. * <p> * If you want to simulate a simple blocking call you can call the <code>get()</code> method immediately: * * <pre> * {@code * byte[] key = \"key\".getBytes(); * byte[] value = \"value\".getBytes(); * ProducerRecord<byte[],byte[]> record = new ProducerRecord<byte[],byte[]>(\"my-topic\", key, value) * producer.send(record).get(); * }</pre> * <p> * Fully non-blocking usage can make use of the {@link Callback} parameter to provide a callback that * will be invoked when the request is complete. * * <pre> * {@code * ProducerRecord<byte[],byte[]> record = new ProducerRecord<byte[],byte[]>(\"the-topic\", key, value); * producer.send(myRecord, * new Callback() { * public void onCompletion(RecordMetadata metadata, Exception e) { * if(e != null) { * e.printStackTrace(); * } else { * System.out.println(\"The offset of the record we just sent is: \" + metadata.offset()); * } * } * }); * } * </pre> * * Callbacks for records being sent to the same partition are guaranteed to execute in order. That is, in the * following example <code>callback1</code> is guaranteed to execute before <code>callback2</code>: * * <pre> * {@code * producer.send(new ProducerRecord<byte[],byte[]>(topic, partition, key1, value1), callback1); * producer.send(new ProducerRecord<byte[],byte[]>(topic, partition, key2, value2), callback2); * } * </pre> * <p> * When used as part of a transaction, it is not necessary to define a callback or check the result of the future * in order to detect errors from <code>send</code>. If any of the send calls failed with an irrecoverable error, * the final {@link #commitTransaction()} call will fail and throw the exception from the last failed send. When * this happens, your application should call {@link #abortTransaction()} to reset the state and continue to send * data. * </p> * <p> * Some transactional send errors cannot be resolved with a call to {@link #abortTransaction()}. In particular, * if a transactional send finishes with a {@link ProducerFencedException}, a {@link org.apache.kafka.common.errors.OutOfOrderSequenceException}, * a {@link org.apache.kafka.common.errors.UnsupportedVersionException}, or an * {@link org.apache.kafka.common.errors.AuthorizationException}, then the only option left is to call {@link #close()}. * Fatal errors cause the producer to enter a defunct state in which future API calls will continue to raise * the same underyling error wrapped in a new {@link KafkaException}. * </p> * <p> * It is a similar picture when idempotence is enabled, but no <code>transactional.id</code> has been configured. * In this case, {@link org.apache.kafka.common.errors.UnsupportedVersionException} and * {@link org.apache.kafka.common.errors.AuthorizationException} are considered fatal errors. However, * {@link ProducerFencedException} does not need to be handled. Additionally, it is possible to continue * sending after receiving an {@link org.apache.kafka.common.errors.OutOfOrderSequenceException}, but doing so * can result in out of order delivery of pending messages. To ensure proper ordering, you should close the * producer and create a new instance. * </p> * <p> * If the message format of the destination topic is not upgraded to 0.11.0.0, idempotent and transactional * produce requests will fail with an {@link org.apache.kafka.common.errors.UnsupportedForMessageFormatException} * error. If this is encountered during a transaction, it is possible to abort and continue. But note that future * sends to the same topic will continue receiving the same exception until the topic is upgraded. * </p> * <p> * Note that callbacks will generally execute in the I/O thread of the producer and so should be reasonably fast or * they will delay the sending of messages from other threads. If you want to execute blocking or computationally * expensive callbacks it is recommended to use your own {@link java.util.concurrent.Executor} in the callback body * to parallelize processing. * * @param record The record to send * @param callback A user-supplied callback to execute when the record has been acknowledged by the server (null * indicates no callback) * * @throws AuthenticationException if authentication fails. See the exception for more details * @throws AuthorizationException fatal error indicating that the producer is not allowed to write * @throws IllegalStateException if a transactional.id has been configured and no transaction has been started, or * when send is invoked after producer has been closed. * @throws InterruptException If the thread is interrupted while blocked * @throws SerializationException If the key or value are not valid objects given the configured serializers * @throws KafkaException If a Kafka related error occurs that does not belong to the public API exceptions. */ @Override public Future < RecordMetadata > send ( ProducerRecord < K , V > record , Callback callback ) { // intercept the record, which can be potentially modified; this method does not throw exceptions ProducerRecord < K , V > interceptedRecord = this . interceptors . onSend ( record ); return doSend ( interceptedRecord , callback ); }","title":"send"},{"location":"6-src/clients/clients/producer/KafkaProducer/#dosend","text":"/** * Implementation of asynchronously send a record to a topic. */ private Future < RecordMetadata > doSend ( ProducerRecord < K , V > record , Callback callback ) { TopicPartition tp = null ; try { throwIfProducerClosed (); // first make sure the metadata for the topic is available long nowMs = time . milliseconds (); ClusterAndWaitTime clusterAndWaitTime ; try { clusterAndWaitTime = waitOnMetadata ( record . topic (), record . partition (), nowMs , maxBlockTimeMs ); } catch ( KafkaException e ) { if ( metadata . isClosed ()) throw new KafkaException ( \"Producer closed while send in progress\" , e ); throw e ; } nowMs += clusterAndWaitTime . waitedOnMetadataMs ; long remainingWaitMs = Math . max ( 0 , maxBlockTimeMs - clusterAndWaitTime . waitedOnMetadataMs ); Cluster cluster = clusterAndWaitTime . cluster ; // 3. \u5e8f\u5217\u5316 key, value byte [] serializedKey ; try { serializedKey = keySerializer . serialize ( record . topic (), record . headers (), record . key ()); } catch ( ClassCastException cce ) { throw new SerializationException ( \"Can't convert key of class \" + record . key (). getClass (). getName () + \" to class \" + producerConfig . getClass ( ProducerConfig . KEY_SERIALIZER_CLASS_CONFIG ). getName () + \" specified in key.serializer\" , cce ); } byte [] serializedValue ; try { serializedValue = valueSerializer . serialize ( record . topic (), record . headers (), record . value ()); } catch ( ClassCastException cce ) { throw new SerializationException ( \"Can't convert value of class \" + record . value (). getClass (). getName () + \" to class \" + producerConfig . getClass ( ProducerConfig . VALUE_SERIALIZER_CLASS_CONFIG ). getName () + \" specified in value.serializer\" , cce ); } // 4. \u8ba1\u7b97\u6d88\u606f\u6240\u5904\u5206\u533a int partition = partition ( record , serializedKey , serializedValue , cluster ); tp = new TopicPartition ( record . topic (), partition ); setReadOnly ( record . headers ()); Header [] headers = record . headers (). toArray (); // 5. \u8ba1\u7b97\u6d88\u606f\u5927\u5c0f\uff0c\u786e\u5b9a\u4e0d\u8d85\u8fc7 `max.request.size`, `buffer.memory` int serializedSize = AbstractRecords . estimateSizeInBytesUpperBound ( apiVersions . maxUsableProduceMagic (), compressionType , serializedKey , serializedValue , headers ); ensureValidRecordSize ( serializedSize ); // 6. \u83b7\u53d6\u6d88\u606f timestamp\uff0c\u5982\u679c\u6ca1\u6709\u5c31\u7528\u5f53\u524d\u65f6\u95f4 long timestamp = record . timestamp () == null ? nowMs : record . timestamp (); if ( log . isTraceEnabled ()) { log . trace ( \"Attempting to append record {} with callback {} to topic {} partition {}\" , record , callback , record . topic (), partition ); } // producer callback will make sure to call both 'callback' and interceptor callback Callback interceptCallback = new InterceptorCallback <> ( callback , this . interceptors , tp ); if ( transactionManager != null && transactionManager . isTransactional ()) { transactionManager . failIfNotReadyForSend (); } // 7. \u5c06\u6d88\u606f\u8ffd\u52a0\u5230 accumulator RecordAccumulator . RecordAppendResult result = accumulator . append ( tp , timestamp , serializedKey , serializedValue , headers , interceptCallback , remainingWaitMs , true , nowMs ); if ( result . abortForNewBatch ) { int prevPartition = partition ; partitioner . onNewBatch ( record . topic (), cluster , prevPartition ); partition = partition ( record , serializedKey , serializedValue , cluster ); tp = new TopicPartition ( record . topic (), partition ); if ( log . isTraceEnabled ()) { log . trace ( \"Retrying append due to new batch creation for topic {} partition {}. The old partition was {}\" , record . topic (), partition , prevPartition ); } // producer callback will make sure to call both 'callback' and interceptor callback interceptCallback = new InterceptorCallback <> ( callback , this . interceptors , tp ); result = accumulator . append ( tp , timestamp , serializedKey , serializedValue , headers , interceptCallback , remainingWaitMs , false , nowMs ); } if ( transactionManager != null && transactionManager . isTransactional ()) transactionManager . maybeAddPartitionToTransaction ( tp ); // 8. \u5982\u679c `batch` \u6ee1\u4e86\u6216\u8005\u65b0\u521b\u5efa\u4e86 `batch`\uff0c\u5524\u9192 sender if ( result . batchIsFull || result . newBatchCreated ) { log . trace ( \"Waking up the sender since topic {} partition {} is either full or getting a new batch\" , record . topic (), partition ); this . sender . wakeup (); } return result . future ; // handling exceptions and record the errors; // for API exceptions return them in the future, // for other exceptions throw directly } catch ( ApiException e ) { log . debug ( \"Exception occurred during message send:\" , e ); if ( callback != null ) callback . onCompletion ( null , e ); this . errors . record (); this . interceptors . onSendError ( record , tp , e ); return new FutureFailure ( e ); } catch ( InterruptedException e ) { this . errors . record (); this . interceptors . onSendError ( record , tp , e ); throw new InterruptException ( e ); } catch ( KafkaException e ) { this . errors . record (); this . interceptors . onSendError ( record , tp , e ); throw e ; } catch ( Exception e ) { // we notify interceptor about all exceptions, since onSend is called before anything else in this method this . interceptors . onSendError ( record , tp , e ); throw e ; } }","title":"doSend"},{"location":"6-src/clients/clients/producer/internals/ProducerBatch/","text":"ProducerBatch","title":"Producer Batch"},{"location":"6-src/clients/clients/producer/internals/ProducerBatch/#producerbatch","text":"","title":"ProducerBatch"},{"location":"6-src/clients/clients/producer/internals/ProducerMetadata/","text":"ProducerMetadata extends Metadata This class is shared by the client thread(for partitioning) and background sender thread.","title":"Producer Metadata"},{"location":"6-src/clients/clients/producer/internals/ProducerMetadata/#producermetadata","text":"extends Metadata This class is shared by the client thread(for partitioning) and background sender thread.","title":"ProducerMetadata"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/","text":"RecordAccumulator This class acts as a queue that accumulates records into MemoryRecords instances to be send to the server. \u5c5e\u6027 batches private final ConcurrentMap < TopicPartition , Deque < ProducerBatch >> batches { // TopicPartition: Deque<ProducerBatch> } \u65b9\u6cd5 append \u5c06\u6d88\u606f\u52a0\u5165 batches \uff0c batches \u7684\u7ed3\u6784\u5982\u4e0a public RecordAppendResult append ( TopicPartition tp , long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long maxTimeToBlock , boolean abortOnNewBatch , long nowMs ) throws InterruptedException { // We keep track of the number of appending thread to make sure we do not miss batches in // abortIncompleteBatches(). appendsInProgress . incrementAndGet (); ByteBuffer buffer = null ; if ( headers == null ) headers = Record . EMPTY_HEADERS ; try { // check if we have an in-progress batch Deque < ProducerBatch > dq = getOrCreateDeque ( tp ); synchronized ( dq ) { if ( closed ) throw new KafkaException ( \"Producer closed while send in progress\" ); RecordAppendResult appendResult = tryAppend ( timestamp , key , value , headers , callback , dq , nowMs ); // \u8ffd\u52a0\u6570\u636e if ( appendResult != null ) // topic-partition \u5df2\u7ecf\u8ffd\u52a0\u5230\u5b58\u5728\u7684 batch\uff0c\u76f4\u63a5\u8fd4\u56de return appendResult ; } // \u5426\u5219\u5c1d\u8bd5\u521b\u5efa\u65b0\u7684 batch // we don't have an in-progress record batch try to allocate a new batch if ( abortOnNewBatch ) { // Return a result that will cause another call to append. return new RecordAppendResult ( null , false , false , true ); } byte maxUsableMagic = apiVersions . maxUsableProduceMagic (); int size = Math . max ( this . batchSize , AbstractRecords . estimateSizeInBytesUpperBound ( maxUsableMagic , compression , key , value , headers )); log . trace ( \"Allocating a new {} byte message buffer for topic {} partition {} with remaining timeout {}ms\" , size , tp . topic (), tp . partition (), maxTimeToBlock ); buffer = free . allocate ( size , maxTimeToBlock ); // Update the current time in case the buffer allocation blocked above. nowMs = time . milliseconds (); synchronized ( dq ) { // Need to check if producer is closed again after grabbing the dequeue lock. if ( closed ) throw new KafkaException ( \"Producer closed while send in progress\" ); RecordAppendResult appendResult = tryAppend ( timestamp , key , value , headers , callback , dq , nowMs ); if ( appendResult != null ) { // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often... return appendResult ; } MemoryRecordsBuilder recordsBuilder = recordsBuilder ( buffer , maxUsableMagic ); ProducerBatch batch = new ProducerBatch ( tp , recordsBuilder , nowMs ); FutureRecordMetadata future = Objects . requireNonNull ( batch . tryAppend ( timestamp , key , value , headers , callback , nowMs )); dq . addLast ( batch ); incomplete . add ( batch ); // Don't deallocate this buffer in the finally block as it's being used in the record batch buffer = null ; // \u5982\u679c dp.size() > 1 \u6216\u8005 batch.isFull() \u5c31\u8bf4\u660e\u6709 batch \u53ef\u4ee5\u53d1\u9001 return new RecordAppendResult ( future , dq . size () > 1 || batch . isFull (), true , false ); } } finally { if ( buffer != null ) free . deallocate ( buffer ); appendsInProgress . decrementAndGet (); } } ready \u83b7\u53d6\u5df2\u7ecf\u51c6\u5907\u597d\u53d1\u9001\u7684 batch \u5bf9\u5e94\u7684 leader (node) drain \u8fd4\u56de { // Map<Integer, List<ProducerBatch>> nodeId: [], nodeId: [], ... }","title":"Record Accumulator"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#recordaccumulator","text":"This class acts as a queue that accumulates records into MemoryRecords instances to be send to the server.","title":"RecordAccumulator"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#_1","text":"batches private final ConcurrentMap < TopicPartition , Deque < ProducerBatch >> batches { // TopicPartition: Deque<ProducerBatch> }","title":"\u5c5e\u6027"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#_2","text":"","title":"\u65b9\u6cd5"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#append","text":"\u5c06\u6d88\u606f\u52a0\u5165 batches \uff0c batches \u7684\u7ed3\u6784\u5982\u4e0a public RecordAppendResult append ( TopicPartition tp , long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long maxTimeToBlock , boolean abortOnNewBatch , long nowMs ) throws InterruptedException { // We keep track of the number of appending thread to make sure we do not miss batches in // abortIncompleteBatches(). appendsInProgress . incrementAndGet (); ByteBuffer buffer = null ; if ( headers == null ) headers = Record . EMPTY_HEADERS ; try { // check if we have an in-progress batch Deque < ProducerBatch > dq = getOrCreateDeque ( tp ); synchronized ( dq ) { if ( closed ) throw new KafkaException ( \"Producer closed while send in progress\" ); RecordAppendResult appendResult = tryAppend ( timestamp , key , value , headers , callback , dq , nowMs ); // \u8ffd\u52a0\u6570\u636e if ( appendResult != null ) // topic-partition \u5df2\u7ecf\u8ffd\u52a0\u5230\u5b58\u5728\u7684 batch\uff0c\u76f4\u63a5\u8fd4\u56de return appendResult ; } // \u5426\u5219\u5c1d\u8bd5\u521b\u5efa\u65b0\u7684 batch // we don't have an in-progress record batch try to allocate a new batch if ( abortOnNewBatch ) { // Return a result that will cause another call to append. return new RecordAppendResult ( null , false , false , true ); } byte maxUsableMagic = apiVersions . maxUsableProduceMagic (); int size = Math . max ( this . batchSize , AbstractRecords . estimateSizeInBytesUpperBound ( maxUsableMagic , compression , key , value , headers )); log . trace ( \"Allocating a new {} byte message buffer for topic {} partition {} with remaining timeout {}ms\" , size , tp . topic (), tp . partition (), maxTimeToBlock ); buffer = free . allocate ( size , maxTimeToBlock ); // Update the current time in case the buffer allocation blocked above. nowMs = time . milliseconds (); synchronized ( dq ) { // Need to check if producer is closed again after grabbing the dequeue lock. if ( closed ) throw new KafkaException ( \"Producer closed while send in progress\" ); RecordAppendResult appendResult = tryAppend ( timestamp , key , value , headers , callback , dq , nowMs ); if ( appendResult != null ) { // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often... return appendResult ; } MemoryRecordsBuilder recordsBuilder = recordsBuilder ( buffer , maxUsableMagic ); ProducerBatch batch = new ProducerBatch ( tp , recordsBuilder , nowMs ); FutureRecordMetadata future = Objects . requireNonNull ( batch . tryAppend ( timestamp , key , value , headers , callback , nowMs )); dq . addLast ( batch ); incomplete . add ( batch ); // Don't deallocate this buffer in the finally block as it's being used in the record batch buffer = null ; // \u5982\u679c dp.size() > 1 \u6216\u8005 batch.isFull() \u5c31\u8bf4\u660e\u6709 batch \u53ef\u4ee5\u53d1\u9001 return new RecordAppendResult ( future , dq . size () > 1 || batch . isFull (), true , false ); } } finally { if ( buffer != null ) free . deallocate ( buffer ); appendsInProgress . decrementAndGet (); } }","title":"append"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#ready","text":"\u83b7\u53d6\u5df2\u7ecf\u51c6\u5907\u597d\u53d1\u9001\u7684 batch \u5bf9\u5e94\u7684 leader (node)","title":"ready"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#drain","text":"\u8fd4\u56de { // Map<Integer, List<ProducerBatch>> nodeId: [], nodeId: [], ... }","title":"drain"},{"location":"6-src/clients/clients/producer/internals/Sender/","text":"Sender implements Runnable The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata requests to renew its view of the cluster and then send produce requests to the appropriate nodes. KafkaClient RecordAccumulator ProducerMetadata Map<TopicPartition, List<ProducerBatch>> inFlightBatches \u65b9\u6cd5 run while ( running ) { try { runOnce (); } catch ( Exception e ) { log . error ( \"Uncaught error in kafka producer I/O thread: \" , e ); } } runOnce /** * Run a single iteration of sending * */ void runOnce () { if ( transactionManager != null ) { try { transactionManager . maybeResolveSequences (); // do not continue sending if the transaction manager is in a failed state if ( transactionManager . hasFatalError ()) { RuntimeException lastError = transactionManager . lastError (); if ( lastError != null ) maybeAbortBatches ( lastError ); client . poll ( retryBackoffMs , time . milliseconds ()); return ; } // Check whether we need a new producerId. If so, we will enqueue an InitProducerId // request which will be sent below transactionManager . bumpIdempotentEpochAndResetIdIfNeeded (); if ( maybeSendAndPollTransactionalRequest ()) { return ; } } catch ( AuthenticationException e ) { // This is already logged as error, but propagated here to perform any clean ups. log . trace ( \"Authentication exception while processing transactional request\" , e ); transactionManager . authenticationFailed ( e ); } } long currentTimeMs = time . milliseconds (); // \u5c06 record batch \u8f6c\u79fb\u5230\u6bcf\u4e2a\u8282\u70b9\u7684\u751f\u4ea7\u8bf7\u6c42\u5217\u8868\u4e2d long pollTimeout = sendProducerData ( currentTimeMs ); // `NetworkClient.poll` Do actual reads and writes to sockets client . poll ( pollTimeout , currentTimeMs ); } sendProducerData private long sendProducerData ( long now ) { // 1. \u83b7\u53d6 metadata Cluster cluster = metadata . fetch (); // get the list of partitions with data ready to send // 2. \u4ece accumulator \u7684 batchers \u4e2d\u53d6\u51fa\u53ef\u4ee5\u53d1\u9001\u7684\u6570\u636e\uff0c // 1. \u83b7\u53d6\u5bf9\u5e94\u7684 leader \u8282\u70b9\u5217\u8868 // 2. \u627e\u4e0d\u5230 leader \u7684 topic \u5217\u8868 RecordAccumulator . ReadyCheckResult result = this . accumulator . ready ( cluster , now ); // if there are any partitions whose leaders are not known yet, force metadata update if ( ! result . unknownLeaderTopics . isEmpty ()) { // The set of topics with unknown leader contains topics with leader election pending as well as // topics which may have expired. Add the topic again to metadata to ensure it is included // and request metadata update, since there are messages to send to the topic. for ( String topic : result . unknownLeaderTopics ) this . metadata . add ( topic , now ); log . debug ( \"Requesting metadata update due to unknown leader topics from the batched records: {}\" , result . unknownLeaderTopics ); this . metadata . requestUpdate (); } // remove any nodes we aren't ready to send to Iterator < Node > iter = result . readyNodes . iterator (); long notReadyTimeout = Long . MAX_VALUE ; // 4. \u904d\u5386 leader \u8282\u70b9\u5217\u8868 while ( iter . hasNext ()) { Node node = iter . next (); // 4.1 \u5224\u65ad leader \u8282\u70b9\u662f\u5426\u5df2\u7ecf\u8fde\u63a5\u5e76\u53ef\u4ee5\u53d1\u9001\u6570\u636e\uff0c\u5982\u679c\u662f\u8fd4\u56de true\uff0c\u4e0d\u662f\u5219\u8fd4\u56de false \u5e76\u5f00\u59cb\u8fdb\u884c\u8fde\u63a5 if ( ! this . client . ready ( node , now )) { iter . remove (); notReadyTimeout = Math . min ( notReadyTimeout , this . client . pollDelayMs ( node , now )); } } // create produce requests // 5. \u83b7\u53d6\u6bcf\u4e2a leader \u5bf9\u5e94\u7684\u53ef\u4ee5\u53d1\u9001\u7684 `ProducerBatch` \u5217\u8868 Map < Integer , List < ProducerBatch >> batches = this . accumulator . drain ( cluster , result . readyNodes , this . maxRequestSize , now ); // 6. \u5728 Sender \u7684 inFlightBatches \u91cc\u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 batch \u5217\u8868 addToInflightBatches ( batches ); // 7. \u5982\u679c\u9700\u8981\u4fdd\u8bc1\u6d88\u606f\u7684\u987a\u5e8f\uff0c\u5c06 topicPartition \u52a0\u5165\u5230 accumulator \u7684 muted \u8bb0\u5f55 if ( guaranteeMessageOrder ) { // Mute all the partitions drained for ( List < ProducerBatch > batchList : batches . values ()) { for ( ProducerBatch batch : batchList ) this . accumulator . mutePartition ( batch . topicPartition ); } } accumulator . resetNextBatchExpiryTime (); List < ProducerBatch > expiredInflightBatches = getExpiredInflightBatches ( now ); List < ProducerBatch > expiredBatches = this . accumulator . expiredBatches ( now ); expiredBatches . addAll ( expiredInflightBatches ); // Reset the producer id if an expired batch has previously been sent to the broker. Also update the metrics // for expired batches. see the documentation of @TransactionState.resetIdempotentProducerId to understand why // we need to reset the producer id here. if ( ! expiredBatches . isEmpty ()) log . trace ( \"Expired {} batches in accumulator\" , expiredBatches . size ()); for ( ProducerBatch expiredBatch : expiredBatches ) { String errorMessage = \"Expiring \" + expiredBatch . recordCount + \" record(s) for \" + expiredBatch . topicPartition + \":\" + ( now - expiredBatch . createdMs ) + \" ms has passed since batch creation\" ; failBatch ( expiredBatch , - 1 , NO_TIMESTAMP , new TimeoutException ( errorMessage ), false ); if ( transactionManager != null && expiredBatch . inRetry ()) { // This ensures that no new batches are drained until the current in flight batches are fully resolved. transactionManager . markSequenceUnresolved ( expiredBatch ); } } sensors . updateProduceRequestMetrics ( batches ); // If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately // loop and try sending more data. Otherwise, the timeout will be the smaller value between next batch expiry // time, and the delay time for checking data availability. Note that the nodes may have data that isn't yet // sendable due to lingering, backing off, etc. This specifically does not include nodes with sendable data // that aren't ready to send since they would cause busy looping. long pollTimeout = Math . min ( result . nextReadyCheckDelayMs , notReadyTimeout ); pollTimeout = Math . min ( pollTimeout , this . accumulator . nextExpiryTimeMs () - now ); pollTimeout = Math . max ( pollTimeout , 0 ); if ( ! result . readyNodes . isEmpty ()) { log . trace ( \"Nodes with data ready to send: {}\" , result . readyNodes ); // if some partitions are already ready to be sent, the select time would be 0; // otherwise if some partition already has some data accumulated but not ready yet, // the select time will be the time difference between now and its linger expiry time; // otherwise the select time will be the time difference between now and the metadata expiry time; pollTimeout = 0 ; } sendProduceRequests ( batches , now ); return pollTimeout ; } sendProduceRequests /** * Transfer the record batches into a list of produce requests on a per-node basis */ private void sendProduceRequests ( Map < Integer , List < ProducerBatch >> collated , long now ) { for ( Map . Entry < Integer , List < ProducerBatch >> entry : collated . entrySet ()) sendProduceRequest ( now , entry . getKey (), acks , requestTimeoutMs , entry . getValue ()); } sendProduceRequest /** * Create a produce request from the given record batches */ private void sendProduceRequest ( long now , int destination , short acks , int timeout , List < ProducerBatch > batches ) { if ( batches . isEmpty ()) return ; final Map < TopicPartition , ProducerBatch > recordsByPartition = new HashMap <> ( batches . size ()); // find the minimum magic version used when creating the record sets byte minUsedMagic = apiVersions . maxUsableProduceMagic (); for ( ProducerBatch batch : batches ) { if ( batch . magic () < minUsedMagic ) minUsedMagic = batch . magic (); } ProduceRequestData . TopicProduceDataCollection tpd = new ProduceRequestData . TopicProduceDataCollection (); for ( ProducerBatch batch : batches ) { TopicPartition tp = batch . topicPartition ; MemoryRecords records = batch . records (); // down convert if necessary to the minimum magic used. In general, there can be a delay between the time // that the producer starts building the batch and the time that we send the request, and we may have // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use // the new message format, but found that the broker didn't support it, so we need to down-convert on the // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may // not all support the same message format version. For example, if a partition migrates from a broker // which is supporting the new magic version to one which doesn't, then we will need to convert. if ( ! records . hasMatchingMagic ( minUsedMagic )) records = batch . records (). downConvert ( minUsedMagic , 0 , time ). records (); ProduceRequestData . TopicProduceData tpData = tpd . find ( tp . topic ()); if ( tpData == null ) { tpData = new ProduceRequestData . TopicProduceData (). setName ( tp . topic ()); tpd . add ( tpData ); } tpData . partitionData (). add ( new ProduceRequestData . PartitionProduceData () . setIndex ( tp . partition ()) . setRecords ( records )); recordsByPartition . put ( tp , batch ); } String transactionalId = null ; if ( transactionManager != null && transactionManager . isTransactional ()) { transactionalId = transactionManager . transactionalId (); } ProduceRequest . Builder requestBuilder = ProduceRequest . forMagic ( minUsedMagic , new ProduceRequestData () . setAcks ( acks ) . setTimeoutMs ( timeout ) . setTransactionalId ( transactionalId ) . setTopicData ( tpd )); RequestCompletionHandler callback = response -> handleProduceResponse ( response , recordsByPartition , time . milliseconds ()); String nodeId = Integer . toString ( destination ); ClientRequest clientRequest = client . newClientRequest ( nodeId , requestBuilder , now , acks != 0 , requestTimeoutMs , callback ); client . send ( clientRequest , now ); log . trace ( \"Sent produce request to {}: {}\" , nodeId , requestBuilder ); } wekeup /** * Wake up the selector associated with this send thread */ public void wakeup () { this . client . wakeup (); }","title":"Sender"},{"location":"6-src/clients/clients/producer/internals/Sender/#sender","text":"implements Runnable The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata requests to renew its view of the cluster and then send produce requests to the appropriate nodes. KafkaClient RecordAccumulator ProducerMetadata Map<TopicPartition, List<ProducerBatch>> inFlightBatches","title":"Sender"},{"location":"6-src/clients/clients/producer/internals/Sender/#_1","text":"","title":"\u65b9\u6cd5"},{"location":"6-src/clients/clients/producer/internals/Sender/#run","text":"while ( running ) { try { runOnce (); } catch ( Exception e ) { log . error ( \"Uncaught error in kafka producer I/O thread: \" , e ); } }","title":"run"},{"location":"6-src/clients/clients/producer/internals/Sender/#runonce","text":"/** * Run a single iteration of sending * */ void runOnce () { if ( transactionManager != null ) { try { transactionManager . maybeResolveSequences (); // do not continue sending if the transaction manager is in a failed state if ( transactionManager . hasFatalError ()) { RuntimeException lastError = transactionManager . lastError (); if ( lastError != null ) maybeAbortBatches ( lastError ); client . poll ( retryBackoffMs , time . milliseconds ()); return ; } // Check whether we need a new producerId. If so, we will enqueue an InitProducerId // request which will be sent below transactionManager . bumpIdempotentEpochAndResetIdIfNeeded (); if ( maybeSendAndPollTransactionalRequest ()) { return ; } } catch ( AuthenticationException e ) { // This is already logged as error, but propagated here to perform any clean ups. log . trace ( \"Authentication exception while processing transactional request\" , e ); transactionManager . authenticationFailed ( e ); } } long currentTimeMs = time . milliseconds (); // \u5c06 record batch \u8f6c\u79fb\u5230\u6bcf\u4e2a\u8282\u70b9\u7684\u751f\u4ea7\u8bf7\u6c42\u5217\u8868\u4e2d long pollTimeout = sendProducerData ( currentTimeMs ); // `NetworkClient.poll` Do actual reads and writes to sockets client . poll ( pollTimeout , currentTimeMs ); }","title":"runOnce"},{"location":"6-src/clients/clients/producer/internals/Sender/#sendproducerdata","text":"private long sendProducerData ( long now ) { // 1. \u83b7\u53d6 metadata Cluster cluster = metadata . fetch (); // get the list of partitions with data ready to send // 2. \u4ece accumulator \u7684 batchers \u4e2d\u53d6\u51fa\u53ef\u4ee5\u53d1\u9001\u7684\u6570\u636e\uff0c // 1. \u83b7\u53d6\u5bf9\u5e94\u7684 leader \u8282\u70b9\u5217\u8868 // 2. \u627e\u4e0d\u5230 leader \u7684 topic \u5217\u8868 RecordAccumulator . ReadyCheckResult result = this . accumulator . ready ( cluster , now ); // if there are any partitions whose leaders are not known yet, force metadata update if ( ! result . unknownLeaderTopics . isEmpty ()) { // The set of topics with unknown leader contains topics with leader election pending as well as // topics which may have expired. Add the topic again to metadata to ensure it is included // and request metadata update, since there are messages to send to the topic. for ( String topic : result . unknownLeaderTopics ) this . metadata . add ( topic , now ); log . debug ( \"Requesting metadata update due to unknown leader topics from the batched records: {}\" , result . unknownLeaderTopics ); this . metadata . requestUpdate (); } // remove any nodes we aren't ready to send to Iterator < Node > iter = result . readyNodes . iterator (); long notReadyTimeout = Long . MAX_VALUE ; // 4. \u904d\u5386 leader \u8282\u70b9\u5217\u8868 while ( iter . hasNext ()) { Node node = iter . next (); // 4.1 \u5224\u65ad leader \u8282\u70b9\u662f\u5426\u5df2\u7ecf\u8fde\u63a5\u5e76\u53ef\u4ee5\u53d1\u9001\u6570\u636e\uff0c\u5982\u679c\u662f\u8fd4\u56de true\uff0c\u4e0d\u662f\u5219\u8fd4\u56de false \u5e76\u5f00\u59cb\u8fdb\u884c\u8fde\u63a5 if ( ! this . client . ready ( node , now )) { iter . remove (); notReadyTimeout = Math . min ( notReadyTimeout , this . client . pollDelayMs ( node , now )); } } // create produce requests // 5. \u83b7\u53d6\u6bcf\u4e2a leader \u5bf9\u5e94\u7684\u53ef\u4ee5\u53d1\u9001\u7684 `ProducerBatch` \u5217\u8868 Map < Integer , List < ProducerBatch >> batches = this . accumulator . drain ( cluster , result . readyNodes , this . maxRequestSize , now ); // 6. \u5728 Sender \u7684 inFlightBatches \u91cc\u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 batch \u5217\u8868 addToInflightBatches ( batches ); // 7. \u5982\u679c\u9700\u8981\u4fdd\u8bc1\u6d88\u606f\u7684\u987a\u5e8f\uff0c\u5c06 topicPartition \u52a0\u5165\u5230 accumulator \u7684 muted \u8bb0\u5f55 if ( guaranteeMessageOrder ) { // Mute all the partitions drained for ( List < ProducerBatch > batchList : batches . values ()) { for ( ProducerBatch batch : batchList ) this . accumulator . mutePartition ( batch . topicPartition ); } } accumulator . resetNextBatchExpiryTime (); List < ProducerBatch > expiredInflightBatches = getExpiredInflightBatches ( now ); List < ProducerBatch > expiredBatches = this . accumulator . expiredBatches ( now ); expiredBatches . addAll ( expiredInflightBatches ); // Reset the producer id if an expired batch has previously been sent to the broker. Also update the metrics // for expired batches. see the documentation of @TransactionState.resetIdempotentProducerId to understand why // we need to reset the producer id here. if ( ! expiredBatches . isEmpty ()) log . trace ( \"Expired {} batches in accumulator\" , expiredBatches . size ()); for ( ProducerBatch expiredBatch : expiredBatches ) { String errorMessage = \"Expiring \" + expiredBatch . recordCount + \" record(s) for \" + expiredBatch . topicPartition + \":\" + ( now - expiredBatch . createdMs ) + \" ms has passed since batch creation\" ; failBatch ( expiredBatch , - 1 , NO_TIMESTAMP , new TimeoutException ( errorMessage ), false ); if ( transactionManager != null && expiredBatch . inRetry ()) { // This ensures that no new batches are drained until the current in flight batches are fully resolved. transactionManager . markSequenceUnresolved ( expiredBatch ); } } sensors . updateProduceRequestMetrics ( batches ); // If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately // loop and try sending more data. Otherwise, the timeout will be the smaller value between next batch expiry // time, and the delay time for checking data availability. Note that the nodes may have data that isn't yet // sendable due to lingering, backing off, etc. This specifically does not include nodes with sendable data // that aren't ready to send since they would cause busy looping. long pollTimeout = Math . min ( result . nextReadyCheckDelayMs , notReadyTimeout ); pollTimeout = Math . min ( pollTimeout , this . accumulator . nextExpiryTimeMs () - now ); pollTimeout = Math . max ( pollTimeout , 0 ); if ( ! result . readyNodes . isEmpty ()) { log . trace ( \"Nodes with data ready to send: {}\" , result . readyNodes ); // if some partitions are already ready to be sent, the select time would be 0; // otherwise if some partition already has some data accumulated but not ready yet, // the select time will be the time difference between now and its linger expiry time; // otherwise the select time will be the time difference between now and the metadata expiry time; pollTimeout = 0 ; } sendProduceRequests ( batches , now ); return pollTimeout ; }","title":"sendProducerData"},{"location":"6-src/clients/clients/producer/internals/Sender/#sendproducerequests","text":"/** * Transfer the record batches into a list of produce requests on a per-node basis */ private void sendProduceRequests ( Map < Integer , List < ProducerBatch >> collated , long now ) { for ( Map . Entry < Integer , List < ProducerBatch >> entry : collated . entrySet ()) sendProduceRequest ( now , entry . getKey (), acks , requestTimeoutMs , entry . getValue ()); }","title":"sendProduceRequests"},{"location":"6-src/clients/clients/producer/internals/Sender/#sendproducerequest","text":"/** * Create a produce request from the given record batches */ private void sendProduceRequest ( long now , int destination , short acks , int timeout , List < ProducerBatch > batches ) { if ( batches . isEmpty ()) return ; final Map < TopicPartition , ProducerBatch > recordsByPartition = new HashMap <> ( batches . size ()); // find the minimum magic version used when creating the record sets byte minUsedMagic = apiVersions . maxUsableProduceMagic (); for ( ProducerBatch batch : batches ) { if ( batch . magic () < minUsedMagic ) minUsedMagic = batch . magic (); } ProduceRequestData . TopicProduceDataCollection tpd = new ProduceRequestData . TopicProduceDataCollection (); for ( ProducerBatch batch : batches ) { TopicPartition tp = batch . topicPartition ; MemoryRecords records = batch . records (); // down convert if necessary to the minimum magic used. In general, there can be a delay between the time // that the producer starts building the batch and the time that we send the request, and we may have // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use // the new message format, but found that the broker didn't support it, so we need to down-convert on the // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may // not all support the same message format version. For example, if a partition migrates from a broker // which is supporting the new magic version to one which doesn't, then we will need to convert. if ( ! records . hasMatchingMagic ( minUsedMagic )) records = batch . records (). downConvert ( minUsedMagic , 0 , time ). records (); ProduceRequestData . TopicProduceData tpData = tpd . find ( tp . topic ()); if ( tpData == null ) { tpData = new ProduceRequestData . TopicProduceData (). setName ( tp . topic ()); tpd . add ( tpData ); } tpData . partitionData (). add ( new ProduceRequestData . PartitionProduceData () . setIndex ( tp . partition ()) . setRecords ( records )); recordsByPartition . put ( tp , batch ); } String transactionalId = null ; if ( transactionManager != null && transactionManager . isTransactional ()) { transactionalId = transactionManager . transactionalId (); } ProduceRequest . Builder requestBuilder = ProduceRequest . forMagic ( minUsedMagic , new ProduceRequestData () . setAcks ( acks ) . setTimeoutMs ( timeout ) . setTransactionalId ( transactionalId ) . setTopicData ( tpd )); RequestCompletionHandler callback = response -> handleProduceResponse ( response , recordsByPartition , time . milliseconds ()); String nodeId = Integer . toString ( destination ); ClientRequest clientRequest = client . newClientRequest ( nodeId , requestBuilder , now , acks != 0 , requestTimeoutMs , callback ); client . send ( clientRequest , now ); log . trace ( \"Sent produce request to {}: {}\" , nodeId , requestBuilder ); }","title":"sendProduceRequest"},{"location":"6-src/clients/clients/producer/internals/Sender/#wekeup","text":"/** * Wake up the selector associated with this send thread */ public void wakeup () { this . client . wakeup (); }","title":"wekeup"},{"location":"6-src/clients/common/network/KafkaChannel/","text":"KafkaChannel \u5c5e\u6027 // \u552f\u4e00\u7684 id \u6807\u8bc6 private final String id ; // \u8bfb\u5199\u90fd\u4f1a\u8f6c\u4ea4\u4e2a transportLayer private final TransportLayer transportLayer ; private final Supplier < Authenticator > authenticatorCreator ; // \u7528\u6765 authentication \uff08 \u6216\u8005 re-authentication\uff09\uff0c\u8bfb\u5199\u540c\u6837\u901a\u8fc7 transportLayer \u8fdb\u884c private Authenticator authenticator ; // Tracks accumulated network thread time. This is updated on the network thread. // The values are read and reset after each response is sent. private long networkThreadTimeNanos ; private final int maxReceiveSize ; private final MemoryPool memoryPool ; private final ChannelMetadataRegistry metadataRegistry ; // \u6682\u5b58\u63a5\u6536\u7684\u6570\u636e private NetworkReceive receive ; // \u5c06\u8981\u53d1\u9001\u7684\u6570\u636e private NetworkSend send ; // Track connection and mute state of channels to enable outstanding requests on channels to be // processed after the channel is disconnected. private boolean disconnected ; private ChannelMuteState muteState ; private ChannelState state ; private SocketAddress remoteAddress ; private int successfulAuthentications ; private boolean midWrite ; private long lastReauthenticationStartNanos ; \u65b9\u6cd5 setSend public void setSend ( Send send ) { // \u4e00\u4e2a channel \u53ea\u80fd\u6682\u5b58\u4e00\u4e2a Send\uff0c\u5f53\u524d Send \u672a\u53d1\u9001\u524d\u4e0d\u80fd\u53d1\u9001\u4e0b\u4e00\u4e2a if ( this . send != null ) throw new IllegalStateException ( \"Attempt to begin a send operation with prior send operation still in progress, connection id is \" + id ); this . send = send ; this . transportLayer . addInterestOps ( SelectionKey . OP_WRITE ); } read & receive public long read () throws IOException { if ( receive == null ) { receive = new NetworkReceive ( maxReceiveSize , id , memoryPool ); } long bytesReceived = receive ( this . receive ); if ( this . receive . requiredMemoryAmountKnown () && ! this . receive . memoryAllocated () && isInMutableState ()) { //pool must be out of memory, mute ourselves. mute (); } return bytesReceived ; } private long receive ( NetworkReceive receive ) throws IOException { try { return receive . readFrom ( transportLayer ); } catch ( SslAuthenticationException e ) { // With TLSv1.3, post-handshake messages may throw SSLExceptions, which are // handled as authentication failures String remoteDesc = remoteAddress != null ? remoteAddress . toString () : null ; state = new ChannelState ( ChannelState . State . AUTHENTICATION_FAILED , e , remoteDesc ); throw e ; } } write public long write () throws IOException { if ( send == null ) return 0 ; midWrite = true ; return send . writeTo ( transportLayer ); }","title":"Kafka Channel"},{"location":"6-src/clients/common/network/KafkaChannel/#kafkachannel","text":"","title":"KafkaChannel"},{"location":"6-src/clients/common/network/KafkaChannel/#_1","text":"// \u552f\u4e00\u7684 id \u6807\u8bc6 private final String id ; // \u8bfb\u5199\u90fd\u4f1a\u8f6c\u4ea4\u4e2a transportLayer private final TransportLayer transportLayer ; private final Supplier < Authenticator > authenticatorCreator ; // \u7528\u6765 authentication \uff08 \u6216\u8005 re-authentication\uff09\uff0c\u8bfb\u5199\u540c\u6837\u901a\u8fc7 transportLayer \u8fdb\u884c private Authenticator authenticator ; // Tracks accumulated network thread time. This is updated on the network thread. // The values are read and reset after each response is sent. private long networkThreadTimeNanos ; private final int maxReceiveSize ; private final MemoryPool memoryPool ; private final ChannelMetadataRegistry metadataRegistry ; // \u6682\u5b58\u63a5\u6536\u7684\u6570\u636e private NetworkReceive receive ; // \u5c06\u8981\u53d1\u9001\u7684\u6570\u636e private NetworkSend send ; // Track connection and mute state of channels to enable outstanding requests on channels to be // processed after the channel is disconnected. private boolean disconnected ; private ChannelMuteState muteState ; private ChannelState state ; private SocketAddress remoteAddress ; private int successfulAuthentications ; private boolean midWrite ; private long lastReauthenticationStartNanos ;","title":"\u5c5e\u6027"},{"location":"6-src/clients/common/network/KafkaChannel/#_2","text":"","title":"\u65b9\u6cd5"},{"location":"6-src/clients/common/network/KafkaChannel/#setsend","text":"public void setSend ( Send send ) { // \u4e00\u4e2a channel \u53ea\u80fd\u6682\u5b58\u4e00\u4e2a Send\uff0c\u5f53\u524d Send \u672a\u53d1\u9001\u524d\u4e0d\u80fd\u53d1\u9001\u4e0b\u4e00\u4e2a if ( this . send != null ) throw new IllegalStateException ( \"Attempt to begin a send operation with prior send operation still in progress, connection id is \" + id ); this . send = send ; this . transportLayer . addInterestOps ( SelectionKey . OP_WRITE ); }","title":"setSend"},{"location":"6-src/clients/common/network/KafkaChannel/#read-receive","text":"public long read () throws IOException { if ( receive == null ) { receive = new NetworkReceive ( maxReceiveSize , id , memoryPool ); } long bytesReceived = receive ( this . receive ); if ( this . receive . requiredMemoryAmountKnown () && ! this . receive . memoryAllocated () && isInMutableState ()) { //pool must be out of memory, mute ourselves. mute (); } return bytesReceived ; } private long receive ( NetworkReceive receive ) throws IOException { try { return receive . readFrom ( transportLayer ); } catch ( SslAuthenticationException e ) { // With TLSv1.3, post-handshake messages may throw SSLExceptions, which are // handled as authentication failures String remoteDesc = remoteAddress != null ? remoteAddress . toString () : null ; state = new ChannelState ( ChannelState . State . AUTHENTICATION_FAILED , e , remoteDesc ); throw e ; } }","title":"read &amp; receive"},{"location":"6-src/clients/common/network/KafkaChannel/#write","text":"public long write () throws IOException { if ( send == null ) return 0 ; midWrite = true ; return send . writeTo ( transportLayer ); }","title":"write"},{"location":"6-src/clients/common/network/NetworkReceive/","text":"NetworkReceive /** * A size delimited Receive that consists of a 4 byte network-ordered size N followed by N bytes of content */ public class NetworkReceive implements Receive { public final static String UNKNOWN_SOURCE = \"\" ; public final static int UNLIMITED = - 1 ; private static final Logger log = LoggerFactory . getLogger ( NetworkReceive . class ); private static final ByteBuffer EMPTY_BUFFER = ByteBuffer . allocate ( 0 ); private final String source ; // \u5934\u90e8 4 byte \u7684 buffer private final ByteBuffer size ; private final int maxSize ; private final MemoryPool memoryPool ; private int requestedBufferSize = - 1 ; // \u6d88\u606f\u9664\u5934\u90e8 4 byte \u4e4b\u540e\uff0c\u5269\u4f59\u7684\u5185\u5bb9 private ByteBuffer buffer ; public NetworkReceive ( String source , ByteBuffer buffer ) { this . source = source ; this . buffer = buffer ; this . size = null ; this . maxSize = UNLIMITED ; this . memoryPool = MemoryPool . NONE ; } public NetworkReceive ( String source ) { this . source = source ; this . size = ByteBuffer . allocate ( 4 ); this . buffer = null ; this . maxSize = UNLIMITED ; this . memoryPool = MemoryPool . NONE ; } public NetworkReceive ( int maxSize , String source ) { this . source = source ; this . size = ByteBuffer . allocate ( 4 ); this . buffer = null ; this . maxSize = maxSize ; this . memoryPool = MemoryPool . NONE ; } public NetworkReceive ( int maxSize , String source , MemoryPool memoryPool ) { this . source = source ; this . size = ByteBuffer . allocate ( 4 ); this . buffer = null ; this . maxSize = maxSize ; this . memoryPool = memoryPool ; } public NetworkReceive () { this ( UNKNOWN_SOURCE ); } @Override public String source () { return source ; } @Override public boolean complete () { return ! size . hasRemaining () && buffer != null && ! buffer . hasRemaining (); } public long readFrom ( ScatteringByteChannel channel ) throws IOException { int read = 0 ; if ( size . hasRemaining ()) { int bytesRead = channel . read ( size ); if ( bytesRead < 0 ) throw new EOFException (); read += bytesRead ; if ( ! size . hasRemaining ()) { size . rewind (); int receiveSize = size . getInt (); if ( receiveSize < 0 ) throw new InvalidReceiveException ( \"Invalid receive (size = \" + receiveSize + \")\" ); if ( maxSize != UNLIMITED && receiveSize > maxSize ) throw new InvalidReceiveException ( \"Invalid receive (size = \" + receiveSize + \" larger than \" + maxSize + \")\" ); requestedBufferSize = receiveSize ; //may be 0 for some payloads (SASL) if ( receiveSize == 0 ) { buffer = EMPTY_BUFFER ; } } } if ( buffer == null && requestedBufferSize != - 1 ) { //we know the size we want but havent been able to allocate it yet buffer = memoryPool . tryAllocate ( requestedBufferSize ); if ( buffer == null ) log . trace ( \"Broker low on memory - could not allocate buffer of size {} for source {}\" , requestedBufferSize , source ); } if ( buffer != null ) { int bytesRead = channel . read ( buffer ); if ( bytesRead < 0 ) throw new EOFException (); read += bytesRead ; } return read ; } @Override public boolean requiredMemoryAmountKnown () { return requestedBufferSize != - 1 ; } @Override public boolean memoryAllocated () { return buffer != null ; } @Override public void close () throws IOException { if ( buffer != null && buffer != EMPTY_BUFFER ) { memoryPool . release ( buffer ); buffer = null ; } } public ByteBuffer payload () { return this . buffer ; } public int bytesRead () { if ( buffer == null ) return size . position (); return buffer . position () + size . position (); } /** * Returns the total size of the receive including payload and size buffer * for use in metrics. This is consistent with {@link NetworkSend#size()} */ public int size () { return payload (). limit () + size . limit (); } }","title":"Network Receive"},{"location":"6-src/clients/common/network/NetworkReceive/#networkreceive","text":"/** * A size delimited Receive that consists of a 4 byte network-ordered size N followed by N bytes of content */ public class NetworkReceive implements Receive { public final static String UNKNOWN_SOURCE = \"\" ; public final static int UNLIMITED = - 1 ; private static final Logger log = LoggerFactory . getLogger ( NetworkReceive . class ); private static final ByteBuffer EMPTY_BUFFER = ByteBuffer . allocate ( 0 ); private final String source ; // \u5934\u90e8 4 byte \u7684 buffer private final ByteBuffer size ; private final int maxSize ; private final MemoryPool memoryPool ; private int requestedBufferSize = - 1 ; // \u6d88\u606f\u9664\u5934\u90e8 4 byte \u4e4b\u540e\uff0c\u5269\u4f59\u7684\u5185\u5bb9 private ByteBuffer buffer ; public NetworkReceive ( String source , ByteBuffer buffer ) { this . source = source ; this . buffer = buffer ; this . size = null ; this . maxSize = UNLIMITED ; this . memoryPool = MemoryPool . NONE ; } public NetworkReceive ( String source ) { this . source = source ; this . size = ByteBuffer . allocate ( 4 ); this . buffer = null ; this . maxSize = UNLIMITED ; this . memoryPool = MemoryPool . NONE ; } public NetworkReceive ( int maxSize , String source ) { this . source = source ; this . size = ByteBuffer . allocate ( 4 ); this . buffer = null ; this . maxSize = maxSize ; this . memoryPool = MemoryPool . NONE ; } public NetworkReceive ( int maxSize , String source , MemoryPool memoryPool ) { this . source = source ; this . size = ByteBuffer . allocate ( 4 ); this . buffer = null ; this . maxSize = maxSize ; this . memoryPool = memoryPool ; } public NetworkReceive () { this ( UNKNOWN_SOURCE ); } @Override public String source () { return source ; } @Override public boolean complete () { return ! size . hasRemaining () && buffer != null && ! buffer . hasRemaining (); } public long readFrom ( ScatteringByteChannel channel ) throws IOException { int read = 0 ; if ( size . hasRemaining ()) { int bytesRead = channel . read ( size ); if ( bytesRead < 0 ) throw new EOFException (); read += bytesRead ; if ( ! size . hasRemaining ()) { size . rewind (); int receiveSize = size . getInt (); if ( receiveSize < 0 ) throw new InvalidReceiveException ( \"Invalid receive (size = \" + receiveSize + \")\" ); if ( maxSize != UNLIMITED && receiveSize > maxSize ) throw new InvalidReceiveException ( \"Invalid receive (size = \" + receiveSize + \" larger than \" + maxSize + \")\" ); requestedBufferSize = receiveSize ; //may be 0 for some payloads (SASL) if ( receiveSize == 0 ) { buffer = EMPTY_BUFFER ; } } } if ( buffer == null && requestedBufferSize != - 1 ) { //we know the size we want but havent been able to allocate it yet buffer = memoryPool . tryAllocate ( requestedBufferSize ); if ( buffer == null ) log . trace ( \"Broker low on memory - could not allocate buffer of size {} for source {}\" , requestedBufferSize , source ); } if ( buffer != null ) { int bytesRead = channel . read ( buffer ); if ( bytesRead < 0 ) throw new EOFException (); read += bytesRead ; } return read ; } @Override public boolean requiredMemoryAmountKnown () { return requestedBufferSize != - 1 ; } @Override public boolean memoryAllocated () { return buffer != null ; } @Override public void close () throws IOException { if ( buffer != null && buffer != EMPTY_BUFFER ) { memoryPool . release ( buffer ); buffer = null ; } } public ByteBuffer payload () { return this . buffer ; } public int bytesRead () { if ( buffer == null ) return size . position (); return buffer . position () + size . position (); } /** * Returns the total size of the receive including payload and size buffer * for use in metrics. This is consistent with {@link NetworkSend#size()} */ public int size () { return payload (). limit () + size . limit (); } }","title":"NetworkReceive"},{"location":"6-src/clients/common/network/NetworkSend/","text":"NetworkSend public class NetworkSend implements Send { private final String destinationId ; private final Send send ; public NetworkSend ( String destinationId , Send send ) { this . destinationId = destinationId ; this . send = send ; } public String destinationId () { return destinationId ; } @Override public boolean completed () { return send . completed (); } @Override public long writeTo ( TransferableChannel channel ) throws IOException { return send . writeTo ( channel ); } @Override public long size () { return send . size (); } }","title":"Network Send"},{"location":"6-src/clients/common/network/NetworkSend/#networksend","text":"public class NetworkSend implements Send { private final String destinationId ; private final Send send ; public NetworkSend ( String destinationId , Send send ) { this . destinationId = destinationId ; this . send = send ; } public String destinationId () { return destinationId ; } @Override public boolean completed () { return send . completed (); } @Override public long writeTo ( TransferableChannel channel ) throws IOException { return send . writeTo ( channel ); } @Override public long size () { return send . size (); } }","title":"NetworkSend"},{"location":"6-src/clients/common/network/Receive/","text":"Receive package org.apache.kafka.common.network ; import java.io.Closeable ; import java.io.IOException ; import java.nio.channels.ScatteringByteChannel ; /** * This interface models the in-progress reading of data from a channel to a source identified by an integer id */ public interface Receive extends Closeable { /** * The numeric id of the source from which we are receiving data. */ String source (); /** * Are we done receiving data? */ boolean complete (); /** * Read bytes into this receive from the given channel * @param channel The channel to read from * @return The number of bytes read * @throws IOException If the reading fails */ long readFrom ( ScatteringByteChannel channel ) throws IOException ; /** * Do we know yet how much memory we require to fully read this */ boolean requiredMemoryAmountKnown (); /** * Has the underlying memory required to complete reading been allocated yet? */ boolean memoryAllocated (); }","title":"Receive"},{"location":"6-src/clients/common/network/Receive/#receive","text":"package org.apache.kafka.common.network ; import java.io.Closeable ; import java.io.IOException ; import java.nio.channels.ScatteringByteChannel ; /** * This interface models the in-progress reading of data from a channel to a source identified by an integer id */ public interface Receive extends Closeable { /** * The numeric id of the source from which we are receiving data. */ String source (); /** * Are we done receiving data? */ boolean complete (); /** * Read bytes into this receive from the given channel * @param channel The channel to read from * @return The number of bytes read * @throws IOException If the reading fails */ long readFrom ( ScatteringByteChannel channel ) throws IOException ; /** * Do we know yet how much memory we require to fully read this */ boolean requiredMemoryAmountKnown (); /** * Has the underlying memory required to complete reading been allocated yet? */ boolean memoryAllocated (); }","title":"Receive"},{"location":"6-src/clients/common/network/Selector/","text":"Selector a nioSelector interface for doing non-blocking multi-connection network I/O. This class works with NetworkSend and NetworkReceive to transmit size-delimited network requests and responses. a connection can be added to the nioSelector associated with an integer id by doing. nioSelector . connect ( \"42\" , new InetSocketAddress ( \"google.com\" , server . port ), 64000 , 64000 ); the connect call does not block on the creation of the TCP connection, so the connect method only begins initiating the connection. the successful invocation of this method does not mean a valid connection has been established. Sending requests , receiving responses , processing connection completions , and disconnections on the existing connections are all done using the poll() call. nioSelector . send ( new NetworkSend ( myDestination , myBytes )); nioSelector . send ( new NetworkSend ( myOtherDestination , myOtherBytes )); nioSelector . poll ( TIMEOUT_MS ); The nioSelector maintains several lists that are reset by each call to poll() which are available via various getters. These are reset by each call to poll() . This class is not thread safe! \u5c5e\u6027 private final Logger log ; // java nio Selector private final java . nio . channels . Selector nioSelector ; // key \u4e3a connection id\uff0c\u503c\u4e3a\u5bf9\u5e94\u7684 KafkaChannel private final Map < String , KafkaChannel > channels ; // private final Set < KafkaChannel > explicitlyMutedChannels ; private boolean outOfMemory ; // \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u53d1\u9001\u5b8c\u6210\u7684 Send private final List < NetworkSend > completedSends ; // \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u63a5\u6536\u5b8c\u6210\u7684 Receive private final LinkedHashMap < String , NetworkReceive > completedReceives ; // \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u8fde\u63a5 connection \u5bf9\u5e94\u7684 id private final List < String > connected ; // \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u65ad\u5f00\u8fde\u63a5 connection \u5bf9\u5e94\u7684 id \u548c \u72b6\u6001 private final Map < String , ChannelState > disconnected ; // \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u53d1\u9001\u5931\u8d25\u7684 Send \u5bf9\u5e94\u7684 connection id private final List < String > failedSends ; // \u8bb0\u5f55\u7acb\u5373\u5df2\u8fde\u63a5\u7684 connection key private final Set < SelectionKey > immediatelyConnectedKeys ; // private final Map < String , KafkaChannel > closingChannels ; // private Set < SelectionKey > keysWithBufferedRead ; private final Time time ; private final SelectorMetrics sensors ; private final ChannelBuilder channelBuilder ; // Max size in bytes of a single network receive (use {@link NetworkReceive#UNLIMITED} for no limit)jk:w private final int maxReceiveSize ; private final boolean recordTimePerConnection ; private final IdleExpiryManager idleExpiryManager ; private final LinkedHashMap < String , DelayedAuthenticationFailureClose > delayedClosingChannels ; private final MemoryPool memoryPool ; private final long lowMemThreshold ; private final int failedAuthenticationDelayMs ; \u65b9\u6cd5 \u6784\u9020\u65b9\u6cd5\u4e2d\uff0c\u6267\u884c\uff1a try { this . nioSelector = java . nio . channels . Selector . open (); } catch ( IOException e ) { throw new KafkaException ( e ); } connect /** * Begin connecting to the given address and add the connection to this nioSelector associated with the given id * number. * <p> * Note that this call only initiates the connection, which will be completed on a future {@link #poll(long)} * call. Check {@link #connected()} to see which (if any) connections have completed after a given poll call. * @param id The id for the new connection * @param address The address to connect to * @param sendBufferSize The send buffer for the new connection * @param receiveBufferSize The receive buffer for the new connection * @throws IllegalStateException if there is already a connection for that id * @throws IOException if DNS resolution fails on the hostname or if the broker is down */ @Override public void connect ( String id , InetSocketAddress address , int sendBufferSize , int receiveBufferSize ) throws IOException { ensureNotRegistered ( id ); SocketChannel socketChannel = SocketChannel . open (); SelectionKey key = null ; try { configureSocketChannel ( socketChannel , sendBufferSize , receiveBufferSize ); // \u8c03\u7528 channel.connect(address) boolean connected = doConnect ( socketChannel , address ); // 1. \u5c06 channel \u6ce8\u518c\u5230 nioSelector\uff0c\u83b7\u5f97 SelectionKey // 2. \u6784\u9020 KafkaChannel // 3. \u5c06 id, KafkaChannel \u5bf9\u5e94\u5173\u7cfb\u8bb0\u5f55\u5230 this.channels // 4. \u5982\u679c\u6709 idelExpiryManage\uff0c\u5c06 connection id \u52a0\u5165 // 5. \u5e76\u8fd4\u56de SelectionKey key = registerChannel ( id , socketChannel , SelectionKey . OP_CONNECT ); if ( connected ) { // OP_CONNECT won't trigger for immediately connected channels log . debug ( \"Immediately connected to node {}\" , id ); immediatelyConnectedKeys . add ( key ); key . interestOps ( 0 ); } } catch ( IOException | RuntimeException e ) { if ( key != null ) immediatelyConnectedKeys . remove ( key ); channels . remove ( id ); socketChannel . close (); throw e ; } } send /** * Queue the given request for sending in the subsequent {@link #poll(long)} calls * @param send The request to send */ public void send ( NetworkSend send ) { // 1. destinationId \u5373\u4e3a this.channels \u4e2d\u7684 connectionId String connectionId = send . destinationId (); // 2. \u6839\u636e id \u4ece this.channels \u6216\u8005 closingChannels \u4e2d\u83b7\u53d6 id \u5bf9\u5e94\u7684 KafkaChannel KafkaChannel channel = openOrClosingChannelOrFail ( connectionId ); if ( closingChannels . containsKey ( connectionId )) { // ensure notification via `disconnected`, leave channel in the state in which closing was triggered this . failedSends . add ( connectionId ); } else { try { // 3.1. \u5c06 KafkaChannel \u7684 send \u8bbe\u7f6e\u4e3a send // 3.2. TransportLayer SelectionKey \u8bbe\u7f6e OP_WRITE \u4e8b\u4ef6 channel . setSend ( send ); } catch ( Exception e ) { // update the state for consistency, the channel will be discarded after `close` channel . state ( ChannelState . FAILED_SEND ); // ensure notification via `disconnected` when `failedSends` are processed in the next poll this . failedSends . add ( connectionId ); close ( channel , CloseMode . DISCARD_NO_NOTIFY ); if ( ! ( e instanceof CancelledKeyException )) { log . error ( \"Unexpected exception during send, closing connection {} and rethrowing exception {}\" , connectionId , e ); throw e ; } } } } poll /** * Do whatever I/O can be done on each connection without blocking. This includes completing connections, completing * disconnections, initiating new sends, or making progress on in-progress sends or receives. * * When this call is completed the user can check for completed sends, receives, connections or disconnects using * {@link #completedSends()}, {@link #completedReceives()}, {@link #connected()}, {@link #disconnected()}. These * lists will be cleared at the beginning of each `poll` call and repopulated by the call if there is * any completed I/O. * * In the \"Plaintext\" setting, we are using socketChannel to read & write to the network. But for the \"SSL\" setting, * we encrypt the data before we use socketChannel to write data to the network, and decrypt before we return the responses. * This requires additional buffers to be maintained as we are reading from network, since the data on the wire is encrypted * we won't be able to read exact no.of bytes as kafka protocol requires. We read as many bytes as we can, up to SSLEngine's * application buffer size. This means we might be reading additional bytes than the requested size. * If there is no further data to read from socketChannel selector won't invoke that channel and we have additional bytes * in the buffer. To overcome this issue we added \"keysWithBufferedRead\" map which tracks channels which have data in the SSL * buffers. If there are channels with buffered data that can by processed, we set \"timeout\" to 0 and process the data even * if there is no more data to read from the socket. * * Atmost one entry is added to \"completedReceives\" for a channel in each poll. This is necessary to guarantee that * requests from a channel are processed on the broker in the order they are sent. Since outstanding requests added * by SocketServer to the request queue may be processed by different request handler threads, requests on each * channel must be processed one-at-a-time to guarantee ordering. * * @param timeout The amount of time to wait, in milliseconds, which must be non-negative * @throws IllegalArgumentException If `timeout` is negative * @throws IllegalStateException If a send is given for which we have no existing connection or for which there is * already an in-progress send */ @Override public void poll ( long timeout ) throws IOException { if ( timeout < 0 ) throw new IllegalArgumentException ( \"timeout should be >= 0\" ); boolean madeReadProgressLastCall = madeReadProgressLastPoll ; // 1. \u6e05\u7a7a completedSends, completedReceives \u7b49\u8bb0\u5f55 clear (); boolean dataInBuffers = ! keysWithBufferedRead . isEmpty (); if ( ! immediatelyConnectedKeys . isEmpty () || ( madeReadProgressLastCall && dataInBuffers )) timeout = 0 ; if ( ! memoryPool . isOutOfMemory () && outOfMemory ) { //we have recovered from memory pressure. unmute any channel not explicitly muted for other reasons log . trace ( \"Broker no longer low on memory - unmuting incoming sockets\" ); for ( KafkaChannel channel : channels . values ()) { if ( channel . isInMutableState () && ! explicitlyMutedChannels . contains ( channel )) { channel . maybeUnmute (); } } outOfMemory = false ; } /* check ready keys */ long startSelect = time . nanoseconds (); // \u8c03\u7528 java nio select\uff0c\u5e76\u8fd4\u56de\uff08select \u8fd9\u91cc\u4f1a\u963b\u585e\uff09 int numReadyKeys = select ( timeout ); long endSelect = time . nanoseconds (); this . sensors . selectTime . record ( endSelect - startSelect , time . milliseconds ()); if ( numReadyKeys > 0 || ! immediatelyConnectedKeys . isEmpty () || dataInBuffers ) { // \u83b7\u53d6\u6709\u4e8b\u4ef6\u53d1\u751f\u7684 key Set < SelectionKey > readyKeys = this . nioSelector . selectedKeys (); // Poll from channels that have buffered data (but nothing more from the underlying socket) if ( dataInBuffers ) { keysWithBufferedRead . removeAll ( readyKeys ); //so no channel gets polled twice Set < SelectionKey > toPoll = keysWithBufferedRead ; keysWithBufferedRead = new HashSet <> (); //poll() calls will repopulate if needed pollSelectionKeys ( toPoll , false , endSelect ); } // Poll from channels where the underlying socket has more data pollSelectionKeys ( readyKeys , false , endSelect ); // Clear all selected keys so that they are included in the ready count for the next select readyKeys . clear (); pollSelectionKeys ( immediatelyConnectedKeys , true , endSelect ); immediatelyConnectedKeys . clear (); } else { madeReadProgressLastPoll = true ; //no work is also \"progress\" } long endIo = time . nanoseconds (); this . sensors . ioTime . record ( endIo - endSelect , time . milliseconds ()); // Close channels that were delayed and are now ready to be closed completeDelayedChannelClose ( endIo ); // we use the time at the end of select to ensure that we don't close any connections that // have just been processed in pollSelectionKeys maybeCloseOldestConnection ( endSelect ); } pollSelectionKeys /** * handle any ready I/O on a set of selection keys * @param selectionKeys set of keys to handle * @param isImmediatelyConnected true if running over a set of keys for just-connected sockets * @param currentTimeNanos time at which set of keys was determined */ // package-private for testing void pollSelectionKeys ( Set < SelectionKey > selectionKeys , boolean isImmediatelyConnected , long currentTimeNanos ) { for ( SelectionKey key : determineHandlingOrder ( selectionKeys )) { KafkaChannel channel = channel ( key ); long channelStartTimeNanos = recordTimePerConnection ? time . nanoseconds () : 0 ; boolean sendFailed = false ; String nodeId = channel . id (); // register all per-connection metrics at once sensors . maybeRegisterConnectionMetrics ( nodeId ); if ( idleExpiryManager != null ) idleExpiryManager . update ( nodeId , currentTimeNanos ); try { /* complete any connections that have finished their handshake (either normally or immediately) */ if ( isImmediatelyConnected || key . isConnectable ()) { if ( channel . finishConnect ()) { this . connected . add ( nodeId ); this . sensors . connectionCreated . record (); SocketChannel socketChannel = ( SocketChannel ) key . channel (); log . debug ( \"Created socket with SO_RCVBUF = {}, SO_SNDBUF = {}, SO_TIMEOUT = {} to node {}\" , socketChannel . socket (). getReceiveBufferSize (), socketChannel . socket (). getSendBufferSize (), socketChannel . socket (). getSoTimeout (), nodeId ); } else { continue ; } } /* if channel is not ready finish prepare */ if ( channel . isConnected () && ! channel . ready ()) { channel . prepare (); if ( channel . ready ()) { long readyTimeMs = time . milliseconds (); boolean isReauthentication = channel . successfulAuthentications () > 1 ; if ( isReauthentication ) { sensors . successfulReauthentication . record ( 1.0 , readyTimeMs ); if ( channel . reauthenticationLatencyMs () == null ) log . warn ( \"Should never happen: re-authentication latency for a re-authenticated channel was null; continuing...\" ); else sensors . reauthenticationLatency . record ( channel . reauthenticationLatencyMs (). doubleValue (), readyTimeMs ); } else { sensors . successfulAuthentication . record ( 1.0 , readyTimeMs ); if ( ! channel . connectedClientSupportsReauthentication ()) sensors . successfulAuthenticationNoReauth . record ( 1.0 , readyTimeMs ); } log . debug ( \"Successfully {}authenticated with {}\" , isReauthentication ? \"re-\" : \"\" , channel . socketDescription ()); } } if ( channel . ready () && channel . state () == ChannelState . NOT_CONNECTED ) channel . state ( ChannelState . READY ); Optional < NetworkReceive > responseReceivedDuringReauthentication = channel . pollResponseReceivedDuringReauthentication (); responseReceivedDuringReauthentication . ifPresent ( receive -> { long currentTimeMs = time . milliseconds (); addToCompletedReceives ( channel , receive , currentTimeMs ); }); //if channel is ready and has bytes to read from socket or buffer, and has no //previous completed receive then read from it if ( channel . ready () && ( key . isReadable () || channel . hasBytesBuffered ()) && ! hasCompletedReceive ( channel ) && ! explicitlyMutedChannels . contains ( channel )) { attemptRead ( channel ); } if ( channel . hasBytesBuffered ()) { //this channel has bytes enqueued in intermediary buffers that we could not read //(possibly because no memory). it may be the case that the underlying socket will //not come up in the next poll() and so we need to remember this channel for the //next poll call otherwise data may be stuck in said buffers forever. If we attempt //to process buffered data and no progress is made, the channel buffered status is //cleared to avoid the overhead of checking every time. keysWithBufferedRead . add ( key ); } /* if channel is ready write to any sockets that have space in their buffer and for which we have data */ long nowNanos = channelStartTimeNanos != 0 ? channelStartTimeNanos : currentTimeNanos ; try { attemptWrite ( key , channel , nowNanos ); } catch ( Exception e ) { sendFailed = true ; throw e ; } /* cancel any defunct sockets */ if ( ! key . isValid ()) close ( channel , CloseMode . GRACEFUL ); } catch ( Exception e ) { String desc = channel . socketDescription (); if ( e instanceof IOException ) { log . debug ( \"Connection with {} disconnected\" , desc , e ); } else if ( e instanceof AuthenticationException ) { boolean isReauthentication = channel . successfulAuthentications () > 0 ; if ( isReauthentication ) sensors . failedReauthentication . record (); else sensors . failedAuthentication . record (); String exceptionMessage = e . getMessage (); if ( e instanceof DelayedResponseAuthenticationException ) exceptionMessage = e . getCause (). getMessage (); log . info ( \"Failed {}authentication with {} ({})\" , isReauthentication ? \"re-\" : \"\" , desc , exceptionMessage ); } else { log . warn ( \"Unexpected error from {}; closing connection\" , desc , e ); } if ( e instanceof DelayedResponseAuthenticationException ) maybeDelayCloseOnAuthenticationFailure ( channel ); else close ( channel , sendFailed ? CloseMode . NOTIFY_ONLY : CloseMode . GRACEFUL ); } finally { maybeRecordTimePerConnection ( channel , channelStartTimeNanos ); } } } wakeup /** * Interrupt the nioSelector if it is blocked waiting to do I/O. */ @Override public void wakeup () { this . nioSelector . wakeup (); } write // package-private for testing void write ( KafkaChannel channel ) throws IOException { String nodeId = channel . id (); long bytesSent = channel . write (); Send send = channel . maybeCompleteSend (); // We may complete the send with bytesSent < 1 if `TransportLayer.hasPendingWrites` was true and `channel.write()` // caused the pending writes to be written to the socket channel buffer if ( bytesSent > 0 || send != null ) { long currentTimeMs = time . milliseconds (); if ( bytesSent > 0 ) this . sensors . recordBytesSent ( nodeId , bytesSent , currentTimeMs ); if ( send != null ) { this . completedSends . add ( send ); this . sensors . recordCompletedSend ( nodeId , send . size (), currentTimeMs ); } } }","title":"Selector"},{"location":"6-src/clients/common/network/Selector/#selector","text":"a nioSelector interface for doing non-blocking multi-connection network I/O. This class works with NetworkSend and NetworkReceive to transmit size-delimited network requests and responses. a connection can be added to the nioSelector associated with an integer id by doing. nioSelector . connect ( \"42\" , new InetSocketAddress ( \"google.com\" , server . port ), 64000 , 64000 ); the connect call does not block on the creation of the TCP connection, so the connect method only begins initiating the connection. the successful invocation of this method does not mean a valid connection has been established. Sending requests , receiving responses , processing connection completions , and disconnections on the existing connections are all done using the poll() call. nioSelector . send ( new NetworkSend ( myDestination , myBytes )); nioSelector . send ( new NetworkSend ( myOtherDestination , myOtherBytes )); nioSelector . poll ( TIMEOUT_MS ); The nioSelector maintains several lists that are reset by each call to poll() which are available via various getters. These are reset by each call to poll() . This class is not thread safe!","title":"Selector"},{"location":"6-src/clients/common/network/Selector/#_1","text":"private final Logger log ; // java nio Selector private final java . nio . channels . Selector nioSelector ; // key \u4e3a connection id\uff0c\u503c\u4e3a\u5bf9\u5e94\u7684 KafkaChannel private final Map < String , KafkaChannel > channels ; // private final Set < KafkaChannel > explicitlyMutedChannels ; private boolean outOfMemory ; // \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u53d1\u9001\u5b8c\u6210\u7684 Send private final List < NetworkSend > completedSends ; // \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u63a5\u6536\u5b8c\u6210\u7684 Receive private final LinkedHashMap < String , NetworkReceive > completedReceives ; // \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u8fde\u63a5 connection \u5bf9\u5e94\u7684 id private final List < String > connected ; // \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u65ad\u5f00\u8fde\u63a5 connection \u5bf9\u5e94\u7684 id \u548c \u72b6\u6001 private final Map < String , ChannelState > disconnected ; // \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u53d1\u9001\u5931\u8d25\u7684 Send \u5bf9\u5e94\u7684 connection id private final List < String > failedSends ; // \u8bb0\u5f55\u7acb\u5373\u5df2\u8fde\u63a5\u7684 connection key private final Set < SelectionKey > immediatelyConnectedKeys ; // private final Map < String , KafkaChannel > closingChannels ; // private Set < SelectionKey > keysWithBufferedRead ; private final Time time ; private final SelectorMetrics sensors ; private final ChannelBuilder channelBuilder ; // Max size in bytes of a single network receive (use {@link NetworkReceive#UNLIMITED} for no limit)jk:w private final int maxReceiveSize ; private final boolean recordTimePerConnection ; private final IdleExpiryManager idleExpiryManager ; private final LinkedHashMap < String , DelayedAuthenticationFailureClose > delayedClosingChannels ; private final MemoryPool memoryPool ; private final long lowMemThreshold ; private final int failedAuthenticationDelayMs ;","title":"\u5c5e\u6027"},{"location":"6-src/clients/common/network/Selector/#_2","text":"\u6784\u9020\u65b9\u6cd5\u4e2d\uff0c\u6267\u884c\uff1a try { this . nioSelector = java . nio . channels . Selector . open (); } catch ( IOException e ) { throw new KafkaException ( e ); }","title":"\u65b9\u6cd5"},{"location":"6-src/clients/common/network/Selector/#connect","text":"/** * Begin connecting to the given address and add the connection to this nioSelector associated with the given id * number. * <p> * Note that this call only initiates the connection, which will be completed on a future {@link #poll(long)} * call. Check {@link #connected()} to see which (if any) connections have completed after a given poll call. * @param id The id for the new connection * @param address The address to connect to * @param sendBufferSize The send buffer for the new connection * @param receiveBufferSize The receive buffer for the new connection * @throws IllegalStateException if there is already a connection for that id * @throws IOException if DNS resolution fails on the hostname or if the broker is down */ @Override public void connect ( String id , InetSocketAddress address , int sendBufferSize , int receiveBufferSize ) throws IOException { ensureNotRegistered ( id ); SocketChannel socketChannel = SocketChannel . open (); SelectionKey key = null ; try { configureSocketChannel ( socketChannel , sendBufferSize , receiveBufferSize ); // \u8c03\u7528 channel.connect(address) boolean connected = doConnect ( socketChannel , address ); // 1. \u5c06 channel \u6ce8\u518c\u5230 nioSelector\uff0c\u83b7\u5f97 SelectionKey // 2. \u6784\u9020 KafkaChannel // 3. \u5c06 id, KafkaChannel \u5bf9\u5e94\u5173\u7cfb\u8bb0\u5f55\u5230 this.channels // 4. \u5982\u679c\u6709 idelExpiryManage\uff0c\u5c06 connection id \u52a0\u5165 // 5. \u5e76\u8fd4\u56de SelectionKey key = registerChannel ( id , socketChannel , SelectionKey . OP_CONNECT ); if ( connected ) { // OP_CONNECT won't trigger for immediately connected channels log . debug ( \"Immediately connected to node {}\" , id ); immediatelyConnectedKeys . add ( key ); key . interestOps ( 0 ); } } catch ( IOException | RuntimeException e ) { if ( key != null ) immediatelyConnectedKeys . remove ( key ); channels . remove ( id ); socketChannel . close (); throw e ; } }","title":"connect"},{"location":"6-src/clients/common/network/Selector/#send","text":"/** * Queue the given request for sending in the subsequent {@link #poll(long)} calls * @param send The request to send */ public void send ( NetworkSend send ) { // 1. destinationId \u5373\u4e3a this.channels \u4e2d\u7684 connectionId String connectionId = send . destinationId (); // 2. \u6839\u636e id \u4ece this.channels \u6216\u8005 closingChannels \u4e2d\u83b7\u53d6 id \u5bf9\u5e94\u7684 KafkaChannel KafkaChannel channel = openOrClosingChannelOrFail ( connectionId ); if ( closingChannels . containsKey ( connectionId )) { // ensure notification via `disconnected`, leave channel in the state in which closing was triggered this . failedSends . add ( connectionId ); } else { try { // 3.1. \u5c06 KafkaChannel \u7684 send \u8bbe\u7f6e\u4e3a send // 3.2. TransportLayer SelectionKey \u8bbe\u7f6e OP_WRITE \u4e8b\u4ef6 channel . setSend ( send ); } catch ( Exception e ) { // update the state for consistency, the channel will be discarded after `close` channel . state ( ChannelState . FAILED_SEND ); // ensure notification via `disconnected` when `failedSends` are processed in the next poll this . failedSends . add ( connectionId ); close ( channel , CloseMode . DISCARD_NO_NOTIFY ); if ( ! ( e instanceof CancelledKeyException )) { log . error ( \"Unexpected exception during send, closing connection {} and rethrowing exception {}\" , connectionId , e ); throw e ; } } } }","title":"send"},{"location":"6-src/clients/common/network/Selector/#poll","text":"/** * Do whatever I/O can be done on each connection without blocking. This includes completing connections, completing * disconnections, initiating new sends, or making progress on in-progress sends or receives. * * When this call is completed the user can check for completed sends, receives, connections or disconnects using * {@link #completedSends()}, {@link #completedReceives()}, {@link #connected()}, {@link #disconnected()}. These * lists will be cleared at the beginning of each `poll` call and repopulated by the call if there is * any completed I/O. * * In the \"Plaintext\" setting, we are using socketChannel to read & write to the network. But for the \"SSL\" setting, * we encrypt the data before we use socketChannel to write data to the network, and decrypt before we return the responses. * This requires additional buffers to be maintained as we are reading from network, since the data on the wire is encrypted * we won't be able to read exact no.of bytes as kafka protocol requires. We read as many bytes as we can, up to SSLEngine's * application buffer size. This means we might be reading additional bytes than the requested size. * If there is no further data to read from socketChannel selector won't invoke that channel and we have additional bytes * in the buffer. To overcome this issue we added \"keysWithBufferedRead\" map which tracks channels which have data in the SSL * buffers. If there are channels with buffered data that can by processed, we set \"timeout\" to 0 and process the data even * if there is no more data to read from the socket. * * Atmost one entry is added to \"completedReceives\" for a channel in each poll. This is necessary to guarantee that * requests from a channel are processed on the broker in the order they are sent. Since outstanding requests added * by SocketServer to the request queue may be processed by different request handler threads, requests on each * channel must be processed one-at-a-time to guarantee ordering. * * @param timeout The amount of time to wait, in milliseconds, which must be non-negative * @throws IllegalArgumentException If `timeout` is negative * @throws IllegalStateException If a send is given for which we have no existing connection or for which there is * already an in-progress send */ @Override public void poll ( long timeout ) throws IOException { if ( timeout < 0 ) throw new IllegalArgumentException ( \"timeout should be >= 0\" ); boolean madeReadProgressLastCall = madeReadProgressLastPoll ; // 1. \u6e05\u7a7a completedSends, completedReceives \u7b49\u8bb0\u5f55 clear (); boolean dataInBuffers = ! keysWithBufferedRead . isEmpty (); if ( ! immediatelyConnectedKeys . isEmpty () || ( madeReadProgressLastCall && dataInBuffers )) timeout = 0 ; if ( ! memoryPool . isOutOfMemory () && outOfMemory ) { //we have recovered from memory pressure. unmute any channel not explicitly muted for other reasons log . trace ( \"Broker no longer low on memory - unmuting incoming sockets\" ); for ( KafkaChannel channel : channels . values ()) { if ( channel . isInMutableState () && ! explicitlyMutedChannels . contains ( channel )) { channel . maybeUnmute (); } } outOfMemory = false ; } /* check ready keys */ long startSelect = time . nanoseconds (); // \u8c03\u7528 java nio select\uff0c\u5e76\u8fd4\u56de\uff08select \u8fd9\u91cc\u4f1a\u963b\u585e\uff09 int numReadyKeys = select ( timeout ); long endSelect = time . nanoseconds (); this . sensors . selectTime . record ( endSelect - startSelect , time . milliseconds ()); if ( numReadyKeys > 0 || ! immediatelyConnectedKeys . isEmpty () || dataInBuffers ) { // \u83b7\u53d6\u6709\u4e8b\u4ef6\u53d1\u751f\u7684 key Set < SelectionKey > readyKeys = this . nioSelector . selectedKeys (); // Poll from channels that have buffered data (but nothing more from the underlying socket) if ( dataInBuffers ) { keysWithBufferedRead . removeAll ( readyKeys ); //so no channel gets polled twice Set < SelectionKey > toPoll = keysWithBufferedRead ; keysWithBufferedRead = new HashSet <> (); //poll() calls will repopulate if needed pollSelectionKeys ( toPoll , false , endSelect ); } // Poll from channels where the underlying socket has more data pollSelectionKeys ( readyKeys , false , endSelect ); // Clear all selected keys so that they are included in the ready count for the next select readyKeys . clear (); pollSelectionKeys ( immediatelyConnectedKeys , true , endSelect ); immediatelyConnectedKeys . clear (); } else { madeReadProgressLastPoll = true ; //no work is also \"progress\" } long endIo = time . nanoseconds (); this . sensors . ioTime . record ( endIo - endSelect , time . milliseconds ()); // Close channels that were delayed and are now ready to be closed completeDelayedChannelClose ( endIo ); // we use the time at the end of select to ensure that we don't close any connections that // have just been processed in pollSelectionKeys maybeCloseOldestConnection ( endSelect ); }","title":"poll"},{"location":"6-src/clients/common/network/Selector/#pollselectionkeys","text":"/** * handle any ready I/O on a set of selection keys * @param selectionKeys set of keys to handle * @param isImmediatelyConnected true if running over a set of keys for just-connected sockets * @param currentTimeNanos time at which set of keys was determined */ // package-private for testing void pollSelectionKeys ( Set < SelectionKey > selectionKeys , boolean isImmediatelyConnected , long currentTimeNanos ) { for ( SelectionKey key : determineHandlingOrder ( selectionKeys )) { KafkaChannel channel = channel ( key ); long channelStartTimeNanos = recordTimePerConnection ? time . nanoseconds () : 0 ; boolean sendFailed = false ; String nodeId = channel . id (); // register all per-connection metrics at once sensors . maybeRegisterConnectionMetrics ( nodeId ); if ( idleExpiryManager != null ) idleExpiryManager . update ( nodeId , currentTimeNanos ); try { /* complete any connections that have finished their handshake (either normally or immediately) */ if ( isImmediatelyConnected || key . isConnectable ()) { if ( channel . finishConnect ()) { this . connected . add ( nodeId ); this . sensors . connectionCreated . record (); SocketChannel socketChannel = ( SocketChannel ) key . channel (); log . debug ( \"Created socket with SO_RCVBUF = {}, SO_SNDBUF = {}, SO_TIMEOUT = {} to node {}\" , socketChannel . socket (). getReceiveBufferSize (), socketChannel . socket (). getSendBufferSize (), socketChannel . socket (). getSoTimeout (), nodeId ); } else { continue ; } } /* if channel is not ready finish prepare */ if ( channel . isConnected () && ! channel . ready ()) { channel . prepare (); if ( channel . ready ()) { long readyTimeMs = time . milliseconds (); boolean isReauthentication = channel . successfulAuthentications () > 1 ; if ( isReauthentication ) { sensors . successfulReauthentication . record ( 1.0 , readyTimeMs ); if ( channel . reauthenticationLatencyMs () == null ) log . warn ( \"Should never happen: re-authentication latency for a re-authenticated channel was null; continuing...\" ); else sensors . reauthenticationLatency . record ( channel . reauthenticationLatencyMs (). doubleValue (), readyTimeMs ); } else { sensors . successfulAuthentication . record ( 1.0 , readyTimeMs ); if ( ! channel . connectedClientSupportsReauthentication ()) sensors . successfulAuthenticationNoReauth . record ( 1.0 , readyTimeMs ); } log . debug ( \"Successfully {}authenticated with {}\" , isReauthentication ? \"re-\" : \"\" , channel . socketDescription ()); } } if ( channel . ready () && channel . state () == ChannelState . NOT_CONNECTED ) channel . state ( ChannelState . READY ); Optional < NetworkReceive > responseReceivedDuringReauthentication = channel . pollResponseReceivedDuringReauthentication (); responseReceivedDuringReauthentication . ifPresent ( receive -> { long currentTimeMs = time . milliseconds (); addToCompletedReceives ( channel , receive , currentTimeMs ); }); //if channel is ready and has bytes to read from socket or buffer, and has no //previous completed receive then read from it if ( channel . ready () && ( key . isReadable () || channel . hasBytesBuffered ()) && ! hasCompletedReceive ( channel ) && ! explicitlyMutedChannels . contains ( channel )) { attemptRead ( channel ); } if ( channel . hasBytesBuffered ()) { //this channel has bytes enqueued in intermediary buffers that we could not read //(possibly because no memory). it may be the case that the underlying socket will //not come up in the next poll() and so we need to remember this channel for the //next poll call otherwise data may be stuck in said buffers forever. If we attempt //to process buffered data and no progress is made, the channel buffered status is //cleared to avoid the overhead of checking every time. keysWithBufferedRead . add ( key ); } /* if channel is ready write to any sockets that have space in their buffer and for which we have data */ long nowNanos = channelStartTimeNanos != 0 ? channelStartTimeNanos : currentTimeNanos ; try { attemptWrite ( key , channel , nowNanos ); } catch ( Exception e ) { sendFailed = true ; throw e ; } /* cancel any defunct sockets */ if ( ! key . isValid ()) close ( channel , CloseMode . GRACEFUL ); } catch ( Exception e ) { String desc = channel . socketDescription (); if ( e instanceof IOException ) { log . debug ( \"Connection with {} disconnected\" , desc , e ); } else if ( e instanceof AuthenticationException ) { boolean isReauthentication = channel . successfulAuthentications () > 0 ; if ( isReauthentication ) sensors . failedReauthentication . record (); else sensors . failedAuthentication . record (); String exceptionMessage = e . getMessage (); if ( e instanceof DelayedResponseAuthenticationException ) exceptionMessage = e . getCause (). getMessage (); log . info ( \"Failed {}authentication with {} ({})\" , isReauthentication ? \"re-\" : \"\" , desc , exceptionMessage ); } else { log . warn ( \"Unexpected error from {}; closing connection\" , desc , e ); } if ( e instanceof DelayedResponseAuthenticationException ) maybeDelayCloseOnAuthenticationFailure ( channel ); else close ( channel , sendFailed ? CloseMode . NOTIFY_ONLY : CloseMode . GRACEFUL ); } finally { maybeRecordTimePerConnection ( channel , channelStartTimeNanos ); } } }","title":"pollSelectionKeys"},{"location":"6-src/clients/common/network/Selector/#wakeup","text":"/** * Interrupt the nioSelector if it is blocked waiting to do I/O. */ @Override public void wakeup () { this . nioSelector . wakeup (); }","title":"wakeup"},{"location":"6-src/clients/common/network/Selector/#write","text":"// package-private for testing void write ( KafkaChannel channel ) throws IOException { String nodeId = channel . id (); long bytesSent = channel . write (); Send send = channel . maybeCompleteSend (); // We may complete the send with bytesSent < 1 if `TransportLayer.hasPendingWrites` was true and `channel.write()` // caused the pending writes to be written to the socket channel buffer if ( bytesSent > 0 || send != null ) { long currentTimeMs = time . milliseconds (); if ( bytesSent > 0 ) this . sensors . recordBytesSent ( nodeId , bytesSent , currentTimeMs ); if ( send != null ) { this . completedSends . add ( send ); this . sensors . recordCompletedSend ( nodeId , send . size (), currentTimeMs ); } } }","title":"write"},{"location":"6-src/clients/common/network/Send/","text":"Send package org.apache.kafka.common.network ; import java.io.IOException ; /** * This interface models the in-progress sending of data. */ public interface Send { /** * Is this send complete? */ boolean completed (); /** * Write some as-yet unwritten bytes from this send to the provided channel. It may take multiple calls for the send * to be completely written * @param channel The Channel to write to * @return The number of bytes written * @throws IOException If the write fails */ long writeTo ( TransferableChannel channel ) throws IOException ; /** * Size of the send */ long size (); }","title":"Send"},{"location":"6-src/clients/common/network/Send/#send","text":"package org.apache.kafka.common.network ; import java.io.IOException ; /** * This interface models the in-progress sending of data. */ public interface Send { /** * Is this send complete? */ boolean completed (); /** * Write some as-yet unwritten bytes from this send to the provided channel. It may take multiple calls for the send * to be completely written * @param channel The Channel to write to * @return The number of bytes written * @throws IOException If the write fails */ long writeTo ( TransferableChannel channel ) throws IOException ; /** * Size of the send */ long size (); }","title":"Send"},{"location":"6-src/clients/common/network/TransportLayer/","text":"TransportLayer Transport layer for underlying communication. At very basic level it is wrapper around SocketChannel and can be used as substitute for SocketChannel and other network Channel implementations. As NetworkClient replaces BlockingChannel and other implementations we will be using KafkaChannel as a network I/O channel. public interface TransportLayer extends ScatteringByteChannel , GatheringByteChannel { /** * Returns true if the channel has handshake and authentication done. */ boolean ready (); /** * Finishes the process of connecting a socket channel. */ boolean finishConnect () throws IOException ; /** * disconnect socketChannel */ void disconnect (); /** * Tells whether or not this channel's network socket is connected. */ boolean isConnected (); /** * returns underlying socketChannel */ SocketChannel socketChannel (); /** * Get the underlying selection key */ SelectionKey selectionKey (); /** * This a no-op for the non-secure PLAINTEXT implementation. For SSL, this performs * SSL handshake. The SSL handshake includes client authentication if configured using * {@link org.apache.kafka.common.config.SslConfigs#SSL_CLIENT_AUTH_CONFIG}. * @throws AuthenticationException if handshake fails due to an {@link javax.net.ssl.SSLException}. * @throws IOException if read or write fails with an I/O error. */ void handshake () throws AuthenticationException , IOException ; /** * Returns true if there are any pending writes */ boolean hasPendingWrites (); /** * Returns `SSLSession.getPeerPrincipal()` if this is an SslTransportLayer and there is an authenticated peer, * `KafkaPrincipal.ANONYMOUS` is returned otherwise. */ Principal peerPrincipal () throws IOException ; void addInterestOps ( int ops ); void removeInterestOps ( int ops ); boolean isMute (); /** * @return true if channel has bytes to be read in any intermediate buffers * which may be processed without reading additional data from the network. */ boolean hasBytesBuffered (); /** * Transfers bytes from `fileChannel` to this `TransportLayer`. * * This method will delegate to {@link FileChannel#transferTo(long, long, java.nio.channels.WritableByteChannel)}, * but it will unwrap the destination channel, if possible, in order to benefit from zero copy. This is required * because the fast path of `transferTo` is only executed if the destination buffer inherits from an internal JDK * class. * * @param fileChannel The source channel * @param position The position within the file at which the transfer is to begin; must be non-negative * @param count The maximum number of bytes to be transferred; must be non-negative * @return The number of bytes, possibly zero, that were actually transferred * @see FileChannel#transferTo(long, long, java.nio.channels.WritableByteChannel) */ long transferFrom ( FileChannel fileChannel , long position , long count ) throws IOException ; }","title":"Transport Layer"},{"location":"6-src/clients/common/network/TransportLayer/#transportlayer","text":"Transport layer for underlying communication. At very basic level it is wrapper around SocketChannel and can be used as substitute for SocketChannel and other network Channel implementations. As NetworkClient replaces BlockingChannel and other implementations we will be using KafkaChannel as a network I/O channel. public interface TransportLayer extends ScatteringByteChannel , GatheringByteChannel { /** * Returns true if the channel has handshake and authentication done. */ boolean ready (); /** * Finishes the process of connecting a socket channel. */ boolean finishConnect () throws IOException ; /** * disconnect socketChannel */ void disconnect (); /** * Tells whether or not this channel's network socket is connected. */ boolean isConnected (); /** * returns underlying socketChannel */ SocketChannel socketChannel (); /** * Get the underlying selection key */ SelectionKey selectionKey (); /** * This a no-op for the non-secure PLAINTEXT implementation. For SSL, this performs * SSL handshake. The SSL handshake includes client authentication if configured using * {@link org.apache.kafka.common.config.SslConfigs#SSL_CLIENT_AUTH_CONFIG}. * @throws AuthenticationException if handshake fails due to an {@link javax.net.ssl.SSLException}. * @throws IOException if read or write fails with an I/O error. */ void handshake () throws AuthenticationException , IOException ; /** * Returns true if there are any pending writes */ boolean hasPendingWrites (); /** * Returns `SSLSession.getPeerPrincipal()` if this is an SslTransportLayer and there is an authenticated peer, * `KafkaPrincipal.ANONYMOUS` is returned otherwise. */ Principal peerPrincipal () throws IOException ; void addInterestOps ( int ops ); void removeInterestOps ( int ops ); boolean isMute (); /** * @return true if channel has bytes to be read in any intermediate buffers * which may be processed without reading additional data from the network. */ boolean hasBytesBuffered (); /** * Transfers bytes from `fileChannel` to this `TransportLayer`. * * This method will delegate to {@link FileChannel#transferTo(long, long, java.nio.channels.WritableByteChannel)}, * but it will unwrap the destination channel, if possible, in order to benefit from zero copy. This is required * because the fast path of `transferTo` is only executed if the destination buffer inherits from an internal JDK * class. * * @param fileChannel The source channel * @param position The position within the file at which the transfer is to begin; must be non-negative * @param count The maximum number of bytes to be transferred; must be non-negative * @return The number of bytes, possibly zero, that were actually transferred * @see FileChannel#transferTo(long, long, java.nio.channels.WritableByteChannel) */ long transferFrom ( FileChannel fileChannel , long position , long count ) throws IOException ; }","title":"TransportLayer"},{"location":"6-src/clients/common/utils/KafkaThread/","text":"","title":"Kafka Thread"},{"location":"6-src/core/kafka/","text":"admin: \u7ba1\u7406\u547d\u4ee4 TopicCommand api: cluster: \u96c6\u7fa4\u76f8\u5173\u4ee3\u7801\uff0c\u5305\u542b Partition, Replica \u7b49 Broker: Broker \u5b9a\u4e49 BrokerEndPoint Cluster: Cluster \u5b9a\u4e49\uff0cThe set of active brokers in the cluster EndPoint Partition: Data structure that represents a topic partition. The leader maintains the AR, ISR, CUR, RAR Replica common: \u4e00\u4e9b\u5e38\u7528\u7684\u516c\u5171\u7c7b(Exception, ...) consumer: \u6d88\u8d39\u8005\u4ee3\u7801\uff08\u5df2\u7ecf deprecated\uff0c\u5e76\u4e14\u632a\u5230 clients \u6a21\u5757\uff09 controller: KafkaController coordinator: group GroupCoordinator transaction log: log \u7ba1\u7406\u6a21\u5757\uff08kafka \u5b58\u50a8\u6d88\u606f\u7684\u65b9\u5f0f\uff09 message CompressionCodec: \u538b\u7f29\u65b9\u5f0f\u7684\u5b9a\u4e49 metrics: kafka \u76d1\u63a7\u7edf\u8ba1 network: \u7f51\u7edc\u5c42\u5904\u7406\uff0cnio \u7684\u4e00\u5c42\u5c01\u88c5 RequestChannel SocketServer raft security: \u6743\u9650\u7ba1\u7406 auth serializer: \u5b9a\u4e49 Encoder \u548c Docoder \u63a5\u53e3\u548c\u4e00\u4e9b\u57fa\u7840\u7684\u5982 String, Long \u7684\u5b9e\u73b0 Decoder server: kafka server \u7684\u4e3b\u8981\u5b9e\u73b0\u903b\u8f91 checkpoints epoch kafkaServer KafkaSErverStartable: \u521b\u5efa KafkaServer KafkaConfig: kafka \u914d\u7f6e\u8bbe\u7f6e\u5b9e\u73b0 object Defaults object KafkaConifg tools: \u5404\u79cd\u53ef\u4ee5\u72ec\u7acb\u8fd0\u884c\u7684\u5de5\u5177 ConsoleConsumer ConsoleProducer utils: \u5404\u79cd\u5de5\u5177\u7c7b json timer zk AdminZkClient KafkaZkClient ZkData zookeeper ZooKeeperClient Kafka: kafka \u542f\u52a8\u5165\u53e3 object Kafka extends Logging \uff1a\u83b7\u53d6\u914d\u7f6e\u9879\uff0c\u542f\u52a8\u4ee3\u7406 KafkaServerStartable","title":"Kafka"},{"location":"6-src/core/kafka/controller/ControllerState/","text":"ControllerState sealed abstract class ControllerState { def value : Byte def rateAndTimeMetricName : Option [ String ] = if ( hasRateAndTimeMetric ) Some ( s\" ${ toString } RateAndTimeMs\" ) else None protected def hasRateAndTimeMetric : Boolean = true } object ControllerState { // Note: `rateAndTimeMetricName` is based on the case object name by default. Changing a name is a breaking change // unless `rateAndTimeMetricName` is overridden. case object Idle extends ControllerState { def value = 0 override protected def hasRateAndTimeMetric : Boolean = false } case object ControllerChange extends ControllerState { def value = 1 } case object BrokerChange extends ControllerState { def value = 2 // The LeaderElectionRateAndTimeMs metric existed before `ControllerState` was introduced and we keep the name // for backwards compatibility. The alternative would be to have the same metric under two different names. override def rateAndTimeMetricName = Some ( \"LeaderElectionRateAndTimeMs\" ) } case object TopicChange extends ControllerState { def value = 3 } case object TopicDeletion extends ControllerState { def value = 4 } case object AlterPartitionReassignment extends ControllerState { def value = 5 override def rateAndTimeMetricName : Option [ String ] = Some ( \"PartitionReassignmentRateAndTimeMs\" ) } case object AutoLeaderBalance extends ControllerState { def value = 6 } case object ManualLeaderBalance extends ControllerState { def value = 7 } case object ControlledShutdown extends ControllerState { def value = 8 } case object IsrChange extends ControllerState { def value = 9 } case object LeaderAndIsrResponseReceived extends ControllerState { def value = 10 } case object LogDirChange extends ControllerState { def value = 11 } case object ControllerShutdown extends ControllerState { def value = 12 } case object UncleanLeaderElectionEnable extends ControllerState { def value = 13 } case object TopicUncleanLeaderElectionEnable extends ControllerState { def value = 14 } case object ListPartitionReassignment extends ControllerState { def value = 15 } case object UpdateMetadataResponseReceived extends ControllerState { def value = 16 override protected def hasRateAndTimeMetric : Boolean = false } case object UpdateFeatures extends ControllerState { def value = 17 } val values : Seq [ ControllerState ] = Seq ( Idle , ControllerChange , BrokerChange , TopicChange , TopicDeletion , AlterPartitionReassignment , AutoLeaderBalance , ManualLeaderBalance , ControlledShutdown , IsrChange , LeaderAndIsrResponseReceived , LogDirChange , ControllerShutdown , UncleanLeaderElectionEnable , TopicUncleanLeaderElectionEnable , ListPartitionReassignment , UpdateMetadataResponseReceived , UpdateFeatures ) }","title":"ControllerState"},{"location":"6-src/core/kafka/controller/ControllerState/#controllerstate","text":"sealed abstract class ControllerState { def value : Byte def rateAndTimeMetricName : Option [ String ] = if ( hasRateAndTimeMetric ) Some ( s\" ${ toString } RateAndTimeMs\" ) else None protected def hasRateAndTimeMetric : Boolean = true } object ControllerState { // Note: `rateAndTimeMetricName` is based on the case object name by default. Changing a name is a breaking change // unless `rateAndTimeMetricName` is overridden. case object Idle extends ControllerState { def value = 0 override protected def hasRateAndTimeMetric : Boolean = false } case object ControllerChange extends ControllerState { def value = 1 } case object BrokerChange extends ControllerState { def value = 2 // The LeaderElectionRateAndTimeMs metric existed before `ControllerState` was introduced and we keep the name // for backwards compatibility. The alternative would be to have the same metric under two different names. override def rateAndTimeMetricName = Some ( \"LeaderElectionRateAndTimeMs\" ) } case object TopicChange extends ControllerState { def value = 3 } case object TopicDeletion extends ControllerState { def value = 4 } case object AlterPartitionReassignment extends ControllerState { def value = 5 override def rateAndTimeMetricName : Option [ String ] = Some ( \"PartitionReassignmentRateAndTimeMs\" ) } case object AutoLeaderBalance extends ControllerState { def value = 6 } case object ManualLeaderBalance extends ControllerState { def value = 7 } case object ControlledShutdown extends ControllerState { def value = 8 } case object IsrChange extends ControllerState { def value = 9 } case object LeaderAndIsrResponseReceived extends ControllerState { def value = 10 } case object LogDirChange extends ControllerState { def value = 11 } case object ControllerShutdown extends ControllerState { def value = 12 } case object UncleanLeaderElectionEnable extends ControllerState { def value = 13 } case object TopicUncleanLeaderElectionEnable extends ControllerState { def value = 14 } case object ListPartitionReassignment extends ControllerState { def value = 15 } case object UpdateMetadataResponseReceived extends ControllerState { def value = 16 override protected def hasRateAndTimeMetric : Boolean = false } case object UpdateFeatures extends ControllerState { def value = 17 } val values : Seq [ ControllerState ] = Seq ( Idle , ControllerChange , BrokerChange , TopicChange , TopicDeletion , AlterPartitionReassignment , AutoLeaderBalance , ManualLeaderBalance , ControlledShutdown , IsrChange , LeaderAndIsrResponseReceived , LogDirChange , ControllerShutdown , UncleanLeaderElectionEnable , TopicUncleanLeaderElectionEnable , ListPartitionReassignment , UpdateMetadataResponseReceived , UpdateFeatures ) }","title":"ControllerState"},{"location":"6-src/core/kafka/controller/ReplicaStateMachine/","text":"ReplicaStateMachine","title":"ReplicaStateMachine"},{"location":"6-src/core/kafka/controller/ReplicaStateMachine/#replicastatemachine","text":"","title":"ReplicaStateMachine"},{"location":"6-src/core/kafka/controller/ControllerContext/","text":"ControllerContext","title":"ControllerContext"},{"location":"6-src/core/kafka/controller/ControllerContext/#controllercontext","text":"","title":"ControllerContext"},{"location":"6-src/core/kafka/controller/ControllerContext/ReplicaAssignment/","text":"ReplicaAssignment \u8bb0\u5f55\uff1a - replicas - addingReplicas - removingReplicas - originReplicas - targetReplicas object ReplicaAssignment { def apply ( replicas : Seq [ Int ]): ReplicaAssignment = { apply ( replicas , Seq . empty , Seq . empty ) } val empty : ReplicaAssignment = apply ( Seq . empty ) } /** * @param replicas the sequence of brokers assigned to the partition. It includes the set of brokers * that were added (`addingReplicas`) and removed (`removingReplicas`). * @param addingReplicas the replicas that are being added if there is a pending reassignment * @param removingReplicas the replicas that are being removed if there is a pending reassignment */ case class ReplicaAssignment private ( replicas : Seq [ Int ], addingReplicas : Seq [ Int ], removingReplicas : Seq [ Int ]) { lazy val originReplicas : Seq [ Int ] = replicas . diff ( addingReplicas ) lazy val targetReplicas : Seq [ Int ] = replicas . diff ( removingReplicas ) def isBeingReassigned : Boolean = { addingReplicas . nonEmpty || removingReplicas . nonEmpty } def reassignTo ( target : Seq [ Int ]): ReplicaAssignment = { val fullReplicaSet = ( target ++ originReplicas ). distinct ReplicaAssignment ( fullReplicaSet , fullReplicaSet . diff ( originReplicas ), fullReplicaSet . diff ( target ) ) } def removeReplica ( replica : Int ): ReplicaAssignment = { ReplicaAssignment ( replicas . filterNot ( _ == replica ), addingReplicas . filterNot ( _ == replica ), removingReplicas . filterNot ( _ == replica ) ) } override def toString : String = s\"ReplicaAssignment(\" + s\"replicas= ${ replicas . mkString ( \",\" ) } , \" + s\"addingReplicas= ${ addingReplicas . mkString ( \",\" ) } , \" + s\"removingReplicas= ${ removingReplicas . mkString ( \",\" ) } )\" }","title":"ReplicaAssignment"},{"location":"6-src/core/kafka/controller/ControllerContext/ReplicaAssignment/#replicaassignment","text":"\u8bb0\u5f55\uff1a - replicas - addingReplicas - removingReplicas - originReplicas - targetReplicas object ReplicaAssignment { def apply ( replicas : Seq [ Int ]): ReplicaAssignment = { apply ( replicas , Seq . empty , Seq . empty ) } val empty : ReplicaAssignment = apply ( Seq . empty ) } /** * @param replicas the sequence of brokers assigned to the partition. It includes the set of brokers * that were added (`addingReplicas`) and removed (`removingReplicas`). * @param addingReplicas the replicas that are being added if there is a pending reassignment * @param removingReplicas the replicas that are being removed if there is a pending reassignment */ case class ReplicaAssignment private ( replicas : Seq [ Int ], addingReplicas : Seq [ Int ], removingReplicas : Seq [ Int ]) { lazy val originReplicas : Seq [ Int ] = replicas . diff ( addingReplicas ) lazy val targetReplicas : Seq [ Int ] = replicas . diff ( removingReplicas ) def isBeingReassigned : Boolean = { addingReplicas . nonEmpty || removingReplicas . nonEmpty } def reassignTo ( target : Seq [ Int ]): ReplicaAssignment = { val fullReplicaSet = ( target ++ originReplicas ). distinct ReplicaAssignment ( fullReplicaSet , fullReplicaSet . diff ( originReplicas ), fullReplicaSet . diff ( target ) ) } def removeReplica ( replica : Int ): ReplicaAssignment = { ReplicaAssignment ( replicas . filterNot ( _ == replica ), addingReplicas . filterNot ( _ == replica ), removingReplicas . filterNot ( _ == replica ) ) } override def toString : String = s\"ReplicaAssignment(\" + s\"replicas= ${ replicas . mkString ( \",\" ) } , \" + s\"addingReplicas= ${ addingReplicas . mkString ( \",\" ) } , \" + s\"removingReplicas= ${ removingReplicas . mkString ( \",\" ) } )\" }","title":"ReplicaAssignment"},{"location":"6-src/core/kafka/controller/ControllerEventManager/","text":"ControllerEventManager ControllerEventManager \u7ef4\u62a4 QueuedEvent (\u5bf9 ControllerEvent \u7684\u5305\u88c5) \u7684\u961f\u5217\uff0c\u5176\u542f\u52a8\u7ebf\u7a0b\uff1a 1. \u4ece queue \u4e2d\u53d6\u4e8b\u4ef6 QueuedEvent 2. \u901a\u8fc7 processor \u5bf9\u4e8b\u4ef6\u8fdb\u884c\u5904\u7406 \u8fd9\u91cc\u7684 processor \u662f KafkaController \uff0c\u5176\u5b9e\u73b0\u4e86 ControllerEventProcessor \u5b9a\u4e49\u7684\u65b9\u6cd5 class ControllerEventManager ( controllerId : Int , processor : ControllerEventProcessor , time : Time , rateAndTimeMetrics : Map [ ControllerState , KafkaTimer ], eventQueueTimeTimeoutMs : Long = 300000 ) extends KafkaMetricsGroup { import ControllerEventManager . _ @volatile private var _state : ControllerState = ControllerState . Idle private val putLock = new ReentrantLock () private val queue = new LinkedBlockingQueue [ QueuedEvent ] // Visible for test private [ controller ] var thread = new ControllerEventThread ( ControllerEventThreadName ) private val eventQueueTimeHist = newHistogram ( EventQueueTimeMetricName ) newGauge ( EventQueueSizeMetricName , () => queue . size ) def state : ControllerState = _state def start (): Unit = thread . start () def close (): Unit = { try { thread . initiateShutdown () clearAndPut ( ShutdownEventThread ) thread . awaitShutdown () } finally { removeMetric ( EventQueueTimeMetricName ) removeMetric ( EventQueueSizeMetricName ) } } def put ( event : ControllerEvent ): QueuedEvent = inLock ( putLock ) { val queuedEvent = new QueuedEvent ( event , time . milliseconds ()) queue . put ( queuedEvent ) queuedEvent } def clearAndPut ( event : ControllerEvent ): QueuedEvent = inLock ( putLock ){ val preemptedEvents = new ArrayList [ QueuedEvent ]() queue . drainTo ( preemptedEvents ) preemptedEvents . forEach ( _ . preempt ( processor )) put ( event ) } def isEmpty : Boolean = queue . isEmpty class ControllerEventThread ( name : String ) extends ShutdownableThread ( name = name , isInterruptible = false ) { logIdent = s\"[ControllerEventThread controllerId= $ controllerId ] \" override def doWork (): Unit = { val dequeued = pollFromEventQueue () dequeued . event match { case ShutdownEventThread => // The shutting down of the thread has been initiated at this point. Ignore this event. case controllerEvent => _state = controllerEvent . state eventQueueTimeHist . update ( time . milliseconds () - dequeued . enqueueTimeMs ) try { def process (): Unit = dequeued . process ( processor ) rateAndTimeMetrics . get ( state ) match { case Some ( timer ) => timer . time { process () } case None => process () } } catch { case e : Throwable => error ( s\"Uncaught error processing event $ controllerEvent \" , e ) } _state = ControllerState . Idle } } } private def pollFromEventQueue (): QueuedEvent = { val count = eventQueueTimeHist . count () if ( count != 0 ) { val event = queue . poll ( eventQueueTimeTimeoutMs , TimeUnit . MILLISECONDS ) if ( event == null ) { eventQueueTimeHist . clear () queue . take () } else { event } } else { queue . take () } } }","title":"ControllerEventManager"},{"location":"6-src/core/kafka/controller/ControllerEventManager/#controllereventmanager","text":"ControllerEventManager \u7ef4\u62a4 QueuedEvent (\u5bf9 ControllerEvent \u7684\u5305\u88c5) \u7684\u961f\u5217\uff0c\u5176\u542f\u52a8\u7ebf\u7a0b\uff1a 1. \u4ece queue \u4e2d\u53d6\u4e8b\u4ef6 QueuedEvent 2. \u901a\u8fc7 processor \u5bf9\u4e8b\u4ef6\u8fdb\u884c\u5904\u7406 \u8fd9\u91cc\u7684 processor \u662f KafkaController \uff0c\u5176\u5b9e\u73b0\u4e86 ControllerEventProcessor \u5b9a\u4e49\u7684\u65b9\u6cd5 class ControllerEventManager ( controllerId : Int , processor : ControllerEventProcessor , time : Time , rateAndTimeMetrics : Map [ ControllerState , KafkaTimer ], eventQueueTimeTimeoutMs : Long = 300000 ) extends KafkaMetricsGroup { import ControllerEventManager . _ @volatile private var _state : ControllerState = ControllerState . Idle private val putLock = new ReentrantLock () private val queue = new LinkedBlockingQueue [ QueuedEvent ] // Visible for test private [ controller ] var thread = new ControllerEventThread ( ControllerEventThreadName ) private val eventQueueTimeHist = newHistogram ( EventQueueTimeMetricName ) newGauge ( EventQueueSizeMetricName , () => queue . size ) def state : ControllerState = _state def start (): Unit = thread . start () def close (): Unit = { try { thread . initiateShutdown () clearAndPut ( ShutdownEventThread ) thread . awaitShutdown () } finally { removeMetric ( EventQueueTimeMetricName ) removeMetric ( EventQueueSizeMetricName ) } } def put ( event : ControllerEvent ): QueuedEvent = inLock ( putLock ) { val queuedEvent = new QueuedEvent ( event , time . milliseconds ()) queue . put ( queuedEvent ) queuedEvent } def clearAndPut ( event : ControllerEvent ): QueuedEvent = inLock ( putLock ){ val preemptedEvents = new ArrayList [ QueuedEvent ]() queue . drainTo ( preemptedEvents ) preemptedEvents . forEach ( _ . preempt ( processor )) put ( event ) } def isEmpty : Boolean = queue . isEmpty class ControllerEventThread ( name : String ) extends ShutdownableThread ( name = name , isInterruptible = false ) { logIdent = s\"[ControllerEventThread controllerId= $ controllerId ] \" override def doWork (): Unit = { val dequeued = pollFromEventQueue () dequeued . event match { case ShutdownEventThread => // The shutting down of the thread has been initiated at this point. Ignore this event. case controllerEvent => _state = controllerEvent . state eventQueueTimeHist . update ( time . milliseconds () - dequeued . enqueueTimeMs ) try { def process (): Unit = dequeued . process ( processor ) rateAndTimeMetrics . get ( state ) match { case Some ( timer ) => timer . time { process () } case None => process () } } catch { case e : Throwable => error ( s\"Uncaught error processing event $ controllerEvent \" , e ) } _state = ControllerState . Idle } } } private def pollFromEventQueue (): QueuedEvent = { val count = eventQueueTimeHist . count () if ( count != 0 ) { val event = queue . poll ( eventQueueTimeTimeoutMs , TimeUnit . MILLISECONDS ) if ( event == null ) { eventQueueTimeHist . clear () queue . take () } else { event } } else { queue . take () } } }","title":"ControllerEventManager"},{"location":"6-src/core/kafka/controller/ControllerEventManager/ControllerEventProcessor/","text":"ControllerEventProcessor \u5904\u7406 ControllerEvent trait ControllerEventProcessor { def process ( event : ControllerEvent ): Unit def preempt ( event : ControllerEvent ): Unit }","title":"ControllerEventProcessor"},{"location":"6-src/core/kafka/controller/ControllerEventManager/ControllerEventProcessor/#controllereventprocessor","text":"\u5904\u7406 ControllerEvent trait ControllerEventProcessor { def process ( event : ControllerEvent ): Unit def preempt ( event : ControllerEvent ): Unit }","title":"ControllerEventProcessor"},{"location":"6-src/core/kafka/controller/ControllerEventManager/ControllerEventThread/","text":"ControllerEventThread \u62c9\u53d6 ControllerEventManager \u4e2d\u7684\u4e8b\u4ef6\uff0c\u8fdb\u884c\u5904\u7406 class ControllerEventThread ( name : String ) extends ShutdownableThread ( name = name , isInterruptible = false ) { logIdent = s\"[ControllerEventThread controllerId= $ controllerId ] \" override def doWork (): Unit = { val dequeued = pollFromEventQueue () dequeued . event match { case ShutdownEventThread => // The shutting down of the thread has been initiated at this point. Ignore this event. case controllerEvent => _state = controllerEvent . state eventQueueTimeHist . update ( time . milliseconds () - dequeued . enqueueTimeMs ) try { def process (): Unit = dequeued . process ( processor ) rateAndTimeMetrics . get ( state ) match { case Some ( timer ) => timer . time { process () } case None => process () } } catch { case e : Throwable => error ( s\"Uncaught error processing event $ controllerEvent \" , e ) } _state = ControllerState . Idle } } }","title":"ControllerEventThread"},{"location":"6-src/core/kafka/controller/ControllerEventManager/ControllerEventThread/#controllereventthread","text":"\u62c9\u53d6 ControllerEventManager \u4e2d\u7684\u4e8b\u4ef6\uff0c\u8fdb\u884c\u5904\u7406 class ControllerEventThread ( name : String ) extends ShutdownableThread ( name = name , isInterruptible = false ) { logIdent = s\"[ControllerEventThread controllerId= $ controllerId ] \" override def doWork (): Unit = { val dequeued = pollFromEventQueue () dequeued . event match { case ShutdownEventThread => // The shutting down of the thread has been initiated at this point. Ignore this event. case controllerEvent => _state = controllerEvent . state eventQueueTimeHist . update ( time . milliseconds () - dequeued . enqueueTimeMs ) try { def process (): Unit = dequeued . process ( processor ) rateAndTimeMetrics . get ( state ) match { case Some ( timer ) => timer . time { process () } case None => process () } } catch { case e : Throwable => error ( s\"Uncaught error processing event $ controllerEvent \" , e ) } _state = ControllerState . Idle } } }","title":"ControllerEventThread"},{"location":"6-src/core/kafka/controller/ControllerEventManager/QueueEvent/","text":"QueueEvent class QueuedEvent ( val event : ControllerEvent , val enqueueTimeMs : Long ) { val processingStarted = new CountDownLatch ( 1 ) val spent = new AtomicBoolean ( false ) def process ( processor : ControllerEventProcessor ): Unit = { if ( spent . getAndSet ( true )) return processingStarted . countDown () processor . process ( event ) } def preempt ( processor : ControllerEventProcessor ): Unit = { if ( spent . getAndSet ( true )) return processor . preempt ( event ) } def awaitProcessing (): Unit = { processingStarted . await () } override def toString : String = { s\"QueuedEvent(event= $ event , enqueueTimeMs= $ enqueueTimeMs )\" } }","title":"QueueEvent"},{"location":"6-src/core/kafka/controller/ControllerEventManager/QueueEvent/#queueevent","text":"class QueuedEvent ( val event : ControllerEvent , val enqueueTimeMs : Long ) { val processingStarted = new CountDownLatch ( 1 ) val spent = new AtomicBoolean ( false ) def process ( processor : ControllerEventProcessor ): Unit = { if ( spent . getAndSet ( true )) return processingStarted . countDown () processor . process ( event ) } def preempt ( processor : ControllerEventProcessor ): Unit = { if ( spent . getAndSet ( true )) return processor . preempt ( event ) } def awaitProcessing (): Unit = { processingStarted . await () } override def toString : String = { s\"QueuedEvent(event= $ event , enqueueTimeMs= $ enqueueTimeMs )\" } }","title":"QueueEvent"},{"location":"6-src/core/kafka/controller/KafkaController/","text":"KafkaController KafkaController \u5b9e\u73b0 ControllerEventProcessor \u7684 process() \u4e0e preempt() \u65b9\u6cd5\u3002","title":"KafkaController"},{"location":"6-src/core/kafka/controller/KafkaController/#kafkacontroller","text":"KafkaController \u5b9e\u73b0 ControllerEventProcessor \u7684 process() \u4e0e preempt() \u65b9\u6cd5\u3002","title":"KafkaController"},{"location":"6-src/core/kafka/controller/KafkaController/ControllerEvent/","text":"ControllerEvent sealed trait ControllerEvent { def state : ControllerState // preempt() is not executed by `ControllerEventThread` but by the main thread. def preempt (): Unit } case object ControllerChange extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerChange override def preempt (): Unit = {} } case object Reelect extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerChange override def preempt (): Unit = {} } case object RegisterBrokerAndReelect extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerChange override def preempt (): Unit = {} } case object Expire extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerChange override def preempt (): Unit = {} } case object ShutdownEventThread extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerShutdown override def preempt (): Unit = {} } case object AutoPreferredReplicaLeaderElection extends ControllerEvent { override def state : ControllerState = ControllerState . AutoLeaderBalance override def preempt (): Unit = {} } case object UncleanLeaderElectionEnable extends ControllerEvent { override def state : ControllerState = ControllerState . UncleanLeaderElectionEnable override def preempt (): Unit = {} } case class TopicUncleanLeaderElectionEnable ( topic : String ) extends ControllerEvent { override def state : ControllerState = ControllerState . TopicUncleanLeaderElectionEnable override def preempt (): Unit = {} } case class ControlledShutdown ( id : Int , brokerEpoch : Long , controlledShutdownCallback : Try [ Set [ TopicPartition ]] => Unit ) extends ControllerEvent { override def state : ControllerState = ControllerState . ControlledShutdown override def preempt (): Unit = controlledShutdownCallback ( Failure ( new ControllerMovedException ( \"Controller moved to another broker\" ))) } case class LeaderAndIsrResponseReceived ( leaderAndIsrResponse : LeaderAndIsrResponse , brokerId : Int ) extends ControllerEvent { override def state : ControllerState = ControllerState . LeaderAndIsrResponseReceived override def preempt (): Unit = {} } case class UpdateMetadataResponseReceived ( updateMetadataResponse : UpdateMetadataResponse , brokerId : Int ) extends ControllerEvent { override def state : ControllerState = ControllerState . UpdateMetadataResponseReceived override def preempt (): Unit = {} } case class TopicDeletionStopReplicaResponseReceived ( replicaId : Int , requestError : Errors , partitionErrors : Map [ TopicPartition , Errors ]) extends ControllerEvent { override def state : ControllerState = ControllerState . TopicDeletion override def preempt (): Unit = {} } case object Startup extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerChange override def preempt (): Unit = {} } case object BrokerChange extends ControllerEvent { override def state : ControllerState = ControllerState . BrokerChange override def preempt (): Unit = {} } case class BrokerModifications ( brokerId : Int ) extends ControllerEvent { override def state : ControllerState = ControllerState . BrokerChange override def preempt (): Unit = {} } case object TopicChange extends ControllerEvent { override def state : ControllerState = ControllerState . TopicChange override def preempt (): Unit = {} } case object LogDirEventNotification extends ControllerEvent { override def state : ControllerState = ControllerState . LogDirChange override def preempt (): Unit = {} } case class PartitionModifications ( topic : String ) extends ControllerEvent { override def state : ControllerState = ControllerState . TopicChange override def preempt (): Unit = {} } case object TopicDeletion extends ControllerEvent { override def state : ControllerState = ControllerState . TopicDeletion override def preempt (): Unit = {} } case object ZkPartitionReassignment extends ControllerEvent { override def state : ControllerState = ControllerState . AlterPartitionReassignment override def preempt (): Unit = {} } case class ApiPartitionReassignment ( reassignments : Map [ TopicPartition , Option [ Seq [ Int ]]], callback : AlterReassignmentsCallback ) extends ControllerEvent { override def state : ControllerState = ControllerState . AlterPartitionReassignment override def preempt (): Unit = callback ( Right ( new ApiError ( Errors . NOT_CONTROLLER ))) } case class PartitionReassignmentIsrChange ( partition : TopicPartition ) extends ControllerEvent { override def state : ControllerState = ControllerState . AlterPartitionReassignment override def preempt (): Unit = {} } case object IsrChangeNotification extends ControllerEvent { override def state : ControllerState = ControllerState . IsrChange override def preempt (): Unit = {} } case class AlterIsrReceived ( brokerId : Int , brokerEpoch : Long , isrsToAlter : Map [ TopicPartition , LeaderAndIsr ], callback : AlterIsrCallback ) extends ControllerEvent { override def state : ControllerState = ControllerState . IsrChange override def preempt (): Unit = {} } case class ReplicaLeaderElection ( partitionsFromAdminClientOpt : Option [ Set [ TopicPartition ]], electionType : ElectionType , electionTrigger : ElectionTrigger , callback : ElectLeadersCallback = _ => {} ) extends ControllerEvent { override def state : ControllerState = ControllerState . ManualLeaderBalance override def preempt (): Unit = callback ( partitionsFromAdminClientOpt . fold ( Map . empty [ TopicPartition , Either [ ApiError , Int ]]) { partitions => partitions . iterator . map ( partition => partition -> Left ( new ApiError ( Errors . NOT_CONTROLLER , null ))). toMap } ) } /** * @param partitionsOpt - an Optional set of partitions. If not present, all reassigning partitions are to be listed */ case class ListPartitionReassignments ( partitionsOpt : Option [ Set [ TopicPartition ]], callback : ListReassignmentsCallback ) extends ControllerEvent { override def state : ControllerState = ControllerState . ListPartitionReassignment override def preempt (): Unit = callback ( Right ( new ApiError ( Errors . NOT_CONTROLLER , null ))) } case class UpdateFeatures ( request : UpdateFeaturesRequest , callback : UpdateFeaturesCallback ) extends ControllerEvent { override def state : ControllerState = ControllerState . UpdateFeatures override def preempt (): Unit = {} } // Used only in test cases abstract class MockEvent ( val state : ControllerState ) extends ControllerEvent { def process (): Unit def preempt (): Unit }","title":"ControllerEvent"},{"location":"6-src/core/kafka/controller/KafkaController/ControllerEvent/#controllerevent","text":"sealed trait ControllerEvent { def state : ControllerState // preempt() is not executed by `ControllerEventThread` but by the main thread. def preempt (): Unit } case object ControllerChange extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerChange override def preempt (): Unit = {} } case object Reelect extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerChange override def preempt (): Unit = {} } case object RegisterBrokerAndReelect extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerChange override def preempt (): Unit = {} } case object Expire extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerChange override def preempt (): Unit = {} } case object ShutdownEventThread extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerShutdown override def preempt (): Unit = {} } case object AutoPreferredReplicaLeaderElection extends ControllerEvent { override def state : ControllerState = ControllerState . AutoLeaderBalance override def preempt (): Unit = {} } case object UncleanLeaderElectionEnable extends ControllerEvent { override def state : ControllerState = ControllerState . UncleanLeaderElectionEnable override def preempt (): Unit = {} } case class TopicUncleanLeaderElectionEnable ( topic : String ) extends ControllerEvent { override def state : ControllerState = ControllerState . TopicUncleanLeaderElectionEnable override def preempt (): Unit = {} } case class ControlledShutdown ( id : Int , brokerEpoch : Long , controlledShutdownCallback : Try [ Set [ TopicPartition ]] => Unit ) extends ControllerEvent { override def state : ControllerState = ControllerState . ControlledShutdown override def preempt (): Unit = controlledShutdownCallback ( Failure ( new ControllerMovedException ( \"Controller moved to another broker\" ))) } case class LeaderAndIsrResponseReceived ( leaderAndIsrResponse : LeaderAndIsrResponse , brokerId : Int ) extends ControllerEvent { override def state : ControllerState = ControllerState . LeaderAndIsrResponseReceived override def preempt (): Unit = {} } case class UpdateMetadataResponseReceived ( updateMetadataResponse : UpdateMetadataResponse , brokerId : Int ) extends ControllerEvent { override def state : ControllerState = ControllerState . UpdateMetadataResponseReceived override def preempt (): Unit = {} } case class TopicDeletionStopReplicaResponseReceived ( replicaId : Int , requestError : Errors , partitionErrors : Map [ TopicPartition , Errors ]) extends ControllerEvent { override def state : ControllerState = ControllerState . TopicDeletion override def preempt (): Unit = {} } case object Startup extends ControllerEvent { override def state : ControllerState = ControllerState . ControllerChange override def preempt (): Unit = {} } case object BrokerChange extends ControllerEvent { override def state : ControllerState = ControllerState . BrokerChange override def preempt (): Unit = {} } case class BrokerModifications ( brokerId : Int ) extends ControllerEvent { override def state : ControllerState = ControllerState . BrokerChange override def preempt (): Unit = {} } case object TopicChange extends ControllerEvent { override def state : ControllerState = ControllerState . TopicChange override def preempt (): Unit = {} } case object LogDirEventNotification extends ControllerEvent { override def state : ControllerState = ControllerState . LogDirChange override def preempt (): Unit = {} } case class PartitionModifications ( topic : String ) extends ControllerEvent { override def state : ControllerState = ControllerState . TopicChange override def preempt (): Unit = {} } case object TopicDeletion extends ControllerEvent { override def state : ControllerState = ControllerState . TopicDeletion override def preempt (): Unit = {} } case object ZkPartitionReassignment extends ControllerEvent { override def state : ControllerState = ControllerState . AlterPartitionReassignment override def preempt (): Unit = {} } case class ApiPartitionReassignment ( reassignments : Map [ TopicPartition , Option [ Seq [ Int ]]], callback : AlterReassignmentsCallback ) extends ControllerEvent { override def state : ControllerState = ControllerState . AlterPartitionReassignment override def preempt (): Unit = callback ( Right ( new ApiError ( Errors . NOT_CONTROLLER ))) } case class PartitionReassignmentIsrChange ( partition : TopicPartition ) extends ControllerEvent { override def state : ControllerState = ControllerState . AlterPartitionReassignment override def preempt (): Unit = {} } case object IsrChangeNotification extends ControllerEvent { override def state : ControllerState = ControllerState . IsrChange override def preempt (): Unit = {} } case class AlterIsrReceived ( brokerId : Int , brokerEpoch : Long , isrsToAlter : Map [ TopicPartition , LeaderAndIsr ], callback : AlterIsrCallback ) extends ControllerEvent { override def state : ControllerState = ControllerState . IsrChange override def preempt (): Unit = {} } case class ReplicaLeaderElection ( partitionsFromAdminClientOpt : Option [ Set [ TopicPartition ]], electionType : ElectionType , electionTrigger : ElectionTrigger , callback : ElectLeadersCallback = _ => {} ) extends ControllerEvent { override def state : ControllerState = ControllerState . ManualLeaderBalance override def preempt (): Unit = callback ( partitionsFromAdminClientOpt . fold ( Map . empty [ TopicPartition , Either [ ApiError , Int ]]) { partitions => partitions . iterator . map ( partition => partition -> Left ( new ApiError ( Errors . NOT_CONTROLLER , null ))). toMap } ) } /** * @param partitionsOpt - an Optional set of partitions. If not present, all reassigning partitions are to be listed */ case class ListPartitionReassignments ( partitionsOpt : Option [ Set [ TopicPartition ]], callback : ListReassignmentsCallback ) extends ControllerEvent { override def state : ControllerState = ControllerState . ListPartitionReassignment override def preempt (): Unit = callback ( Right ( new ApiError ( Errors . NOT_CONTROLLER , null ))) } case class UpdateFeatures ( request : UpdateFeaturesRequest , callback : UpdateFeaturesCallback ) extends ControllerEvent { override def state : ControllerState = ControllerState . UpdateFeatures override def preempt (): Unit = {} } // Used only in test cases abstract class MockEvent ( val state : ControllerState ) extends ControllerEvent { def process (): Unit def preempt (): Unit }","title":"ControllerEvent"},{"location":"6-src/core/kafka/network/RequestChannel/","text":"RequestChannel RequestChannel \u8d1f\u8d23\u5c06\u8bf7\u6c42\u4ece\u7f51\u7edc\u5c42\u8f6c\u63a5\u5230\u4e1a\u52a1\u5c42\uff0c\u4ee5\u53ca\u5c06\u4e1a\u52a1\u5c42\u7684\u5904\u7406\u7ed3\u679c\u4ea4\u4ed8\u7ed9\u7f51\u7edc\u5c42\u8fdb\u800c\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002 \u6bcf\u4e00\u4e2a SocketServer \u53ea\u6709\u4e00\u4e2a RequestChannel \u5bf9\u8c61\uff0c\u5728 SocketServer \u4e2d\u6784\u9020\u3002 RequestChannel \u6784\u9020\u65b9\u6cd5\u4e2d\u521d\u59cb\u5316\u4e86 requestQueue \uff0c\u7528\u6765\u5b58\u653e\u7f51\u7edc\u5c42\u63a5\u6536\u5230\u7684\u8bf7\u6c42\uff0c\u8fd9\u4e9b\u8bf7\u6c42\u5373\u5c06\u4ea4\u4ed8\u7ed9\u4e1a\u52a1\u5c42\u8fdb\u884c\u5904\u7406\uff08 KafkaRequestHandler \u8fdb\u884c\u5904\u7406\uff09\u3002 \u540c\u65f6\uff0c\u521d\u59cb\u5316\u4e86 processors \uff0c\u5728 SocketServer \u521b\u5efa Processor \u65f6\u4f1a\u5c06 Processor \u540c\u6b65\u8bb0\u5f55\u5230\u8fd9\u91cc\uff0c\u5f53 RequestChannel \u8c03\u7528 sendResponse() \u65f6\u4f1a\u4ece\u8fd9\u91cc\u53d6 response \u5bf9\u5e94\u7684 Processor \u5e76\u5c06 response \u8fd4\u56de\u7ed9\u5bf9\u5e94\u7684 Processor \uff0c\u8fd9\u4e9b response \u5373\u5c06\u4ea4\u4ed8\u7ed9\u7f51\u7edc\u5c42\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002 \u5c5e\u6027 private val requestQueue = new ArrayBlockingQueue [ BaseRequest ]( queueSize ) // Processor \u8bb0\u5f55 private val processors = new ConcurrentHashMap [ Int , Processor ]() \u65b9\u6cd5 addProcesor removeProcessor sendRequest /** Send a request to be handled, potentially blocking until there is room in the queue for the request */ def sendRequest ( request : RequestChannel . Request ): Unit = { requestQueue . put ( request ) } sendResponse /** Send a response back to the socket server to be sent over the network */ def sendResponse ( response : RequestChannel . Response ): Unit = { if ( isTraceEnabled ) { val requestHeader = response . request . header val message = response match { case sendResponse : SendResponse => s\"Sending ${ requestHeader . apiKey } response to client ${ requestHeader . clientId } of ${ sendResponse . responseSend . size } bytes.\" case _: NoOpResponse => s\"Not sending ${ requestHeader . apiKey } response to client ${ requestHeader . clientId } as it's not required.\" case _: CloseConnectionResponse => s\"Closing connection for client ${ requestHeader . clientId } due to error during ${ requestHeader . apiKey } .\" case _: StartThrottlingResponse => s\"Notifying channel throttling has started for client ${ requestHeader . clientId } for ${ requestHeader . apiKey } \" case _: EndThrottlingResponse => s\"Notifying channel throttling has ended for client ${ requestHeader . clientId } for ${ requestHeader . apiKey } \" } trace ( message ) } response match { // We should only send one of the following per request case _: SendResponse | _: NoOpResponse | _: CloseConnectionResponse => val request = response . request val timeNanos = time . nanoseconds () request . responseCompleteTimeNanos = timeNanos if ( request . apiLocalCompleteTimeNanos == - 1L ) request . apiLocalCompleteTimeNanos = timeNanos // For a given request, these may happen in addition to one in the previous section, skip updating the metrics case _: StartThrottlingResponse | _: EndThrottlingResponse => () } val processor = processors . get ( response . processor ) // The processor may be null if it was shutdown. In this case, the connections // are closed, so the response is dropped. if ( processor != null ) { processor . enqueueResponse ( response ) } } receiveRequest /** Get the next request or block until specified time has elapsed */ def receiveRequest ( timeout : Long ): RequestChannel . BaseRequest = requestQueue . poll ( timeout , TimeUnit . MILLISECONDS ) /** Get the next request or block until there is one */ def receiveRequest (): RequestChannel . BaseRequest = requestQueue . take ()","title":"Request Channel"},{"location":"6-src/core/kafka/network/RequestChannel/#requestchannel","text":"RequestChannel \u8d1f\u8d23\u5c06\u8bf7\u6c42\u4ece\u7f51\u7edc\u5c42\u8f6c\u63a5\u5230\u4e1a\u52a1\u5c42\uff0c\u4ee5\u53ca\u5c06\u4e1a\u52a1\u5c42\u7684\u5904\u7406\u7ed3\u679c\u4ea4\u4ed8\u7ed9\u7f51\u7edc\u5c42\u8fdb\u800c\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002 \u6bcf\u4e00\u4e2a SocketServer \u53ea\u6709\u4e00\u4e2a RequestChannel \u5bf9\u8c61\uff0c\u5728 SocketServer \u4e2d\u6784\u9020\u3002 RequestChannel \u6784\u9020\u65b9\u6cd5\u4e2d\u521d\u59cb\u5316\u4e86 requestQueue \uff0c\u7528\u6765\u5b58\u653e\u7f51\u7edc\u5c42\u63a5\u6536\u5230\u7684\u8bf7\u6c42\uff0c\u8fd9\u4e9b\u8bf7\u6c42\u5373\u5c06\u4ea4\u4ed8\u7ed9\u4e1a\u52a1\u5c42\u8fdb\u884c\u5904\u7406\uff08 KafkaRequestHandler \u8fdb\u884c\u5904\u7406\uff09\u3002 \u540c\u65f6\uff0c\u521d\u59cb\u5316\u4e86 processors \uff0c\u5728 SocketServer \u521b\u5efa Processor \u65f6\u4f1a\u5c06 Processor \u540c\u6b65\u8bb0\u5f55\u5230\u8fd9\u91cc\uff0c\u5f53 RequestChannel \u8c03\u7528 sendResponse() \u65f6\u4f1a\u4ece\u8fd9\u91cc\u53d6 response \u5bf9\u5e94\u7684 Processor \u5e76\u5c06 response \u8fd4\u56de\u7ed9\u5bf9\u5e94\u7684 Processor \uff0c\u8fd9\u4e9b response \u5373\u5c06\u4ea4\u4ed8\u7ed9\u7f51\u7edc\u5c42\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002","title":"RequestChannel"},{"location":"6-src/core/kafka/network/RequestChannel/#_1","text":"private val requestQueue = new ArrayBlockingQueue [ BaseRequest ]( queueSize ) // Processor \u8bb0\u5f55 private val processors = new ConcurrentHashMap [ Int , Processor ]()","title":"\u5c5e\u6027"},{"location":"6-src/core/kafka/network/RequestChannel/#_2","text":"","title":"\u65b9\u6cd5"},{"location":"6-src/core/kafka/network/RequestChannel/#addprocesor","text":"","title":"addProcesor"},{"location":"6-src/core/kafka/network/RequestChannel/#removeprocessor","text":"","title":"removeProcessor"},{"location":"6-src/core/kafka/network/RequestChannel/#sendrequest","text":"/** Send a request to be handled, potentially blocking until there is room in the queue for the request */ def sendRequest ( request : RequestChannel . Request ): Unit = { requestQueue . put ( request ) }","title":"sendRequest"},{"location":"6-src/core/kafka/network/RequestChannel/#sendresponse","text":"/** Send a response back to the socket server to be sent over the network */ def sendResponse ( response : RequestChannel . Response ): Unit = { if ( isTraceEnabled ) { val requestHeader = response . request . header val message = response match { case sendResponse : SendResponse => s\"Sending ${ requestHeader . apiKey } response to client ${ requestHeader . clientId } of ${ sendResponse . responseSend . size } bytes.\" case _: NoOpResponse => s\"Not sending ${ requestHeader . apiKey } response to client ${ requestHeader . clientId } as it's not required.\" case _: CloseConnectionResponse => s\"Closing connection for client ${ requestHeader . clientId } due to error during ${ requestHeader . apiKey } .\" case _: StartThrottlingResponse => s\"Notifying channel throttling has started for client ${ requestHeader . clientId } for ${ requestHeader . apiKey } \" case _: EndThrottlingResponse => s\"Notifying channel throttling has ended for client ${ requestHeader . clientId } for ${ requestHeader . apiKey } \" } trace ( message ) } response match { // We should only send one of the following per request case _: SendResponse | _: NoOpResponse | _: CloseConnectionResponse => val request = response . request val timeNanos = time . nanoseconds () request . responseCompleteTimeNanos = timeNanos if ( request . apiLocalCompleteTimeNanos == - 1L ) request . apiLocalCompleteTimeNanos = timeNanos // For a given request, these may happen in addition to one in the previous section, skip updating the metrics case _: StartThrottlingResponse | _: EndThrottlingResponse => () } val processor = processors . get ( response . processor ) // The processor may be null if it was shutdown. In this case, the connections // are closed, so the response is dropped. if ( processor != null ) { processor . enqueueResponse ( response ) } }","title":"sendResponse"},{"location":"6-src/core/kafka/network/RequestChannel/#receiverequest","text":"/** Get the next request or block until specified time has elapsed */ def receiveRequest ( timeout : Long ): RequestChannel . BaseRequest = requestQueue . poll ( timeout , TimeUnit . MILLISECONDS ) /** Get the next request or block until there is one */ def receiveRequest (): RequestChannel . BaseRequest = requestQueue . take ()","title":"receiveRequest"},{"location":"6-src/core/kafka/network/SocketServer/","text":"SocketServer \u652f\u6301 2 \u79cd\u7c7b\u578b\u7684\u8bf7\u6c42\uff1a 1. \u6570\u636e\uff1a - \u5904\u7406\u6765\u81ea client \u6216\u8005\u540c\u96c6\u7fa4\u5176\u4ed6 broker \u7684\u8bf7\u6c42 - \u7ebf\u7a0b\u6a21\u578b\u662f\uff1a - \u53ef\u4ee5\u914d\u7f6e\u591a\u4e2a listener\uff0c\u6bcf\u4e2a listener \u5bf9\u5e94 1 \u4e2a Acceptor \u7ebf\u7a0b\uff0c\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42 - Acceptor \u6709 N \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea sockets \u7684\u8bf7\u6c42 - M \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b 2. \u63a7\u5236\uff1a - \u5904\u7406\u6765\u81ea controller \u7684\u8bf7\u6c42\u3002\u8fd9\u662f\u53ef\u9009\u7684\u5e76\u4e14\u901a\u8fc7\u6307\u5b9a control.plane.listener.name \u8fdb\u884c\u914d\u7f6e\u3002\u5982\u679c\u6ca1\u6709\u914d\u7f6e\uff0ccontroller \u7684\u8bf7\u6c42\u4e5f\u4f1a\u50cf\u6570\u636e\u8bf7\u6c42\u4e00\u6837\u901a\u8fc7 listener \u5904\u7406 - \u7ebf\u7a0b\u6a21\u578b\u662f\uff1a - 1 \u4e2a Acceptor \u7ebf\u7a0b\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42 - Acceptor \u6709 1 \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea socket \u7684\u8bf7\u6c42 - 1 \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b \u5c5e\u6027 private val maxQueuedRequests = config . queuedMaxRequests private val logContext = new LogContext ( s\"[SocketServer brokerId= ${ config . brokerId } ] \" ) this . logIdent = logContext . logPrefix private val memoryPoolSensor = metrics . sensor ( \"MemoryPoolUtilization\" ) private val memoryPoolDepletedPercentMetricName = metrics . metricName ( \"MemoryPoolAvgDepletedPercent\" , MetricsGroup ) private val memoryPoolDepletedTimeMetricName = metrics . metricName ( \"MemoryPoolDepletedTimeTotal\" , MetricsGroup ) memoryPoolSensor . add ( new Meter ( TimeUnit . MILLISECONDS , memoryPoolDepletedPercentMetricName , memoryPoolDepletedTimeMetricName )) private val memoryPool = if ( config . queuedMaxBytes > 0 ) new SimpleMemoryPool ( config . queuedMaxBytes , config . socketRequestMaxBytes , false , memoryPoolSensor ) else MemoryPool . NONE // data-plane // Processor \u8bb0\u5f55 private val dataPlaneProcessors = new ConcurrentHashMap [ Int , Processor ]() private [ network ] val dataPlaneAcceptors = new ConcurrentHashMap [ EndPoint , Acceptor ]() val dataPlaneRequestChannel = new RequestChannel ( maxQueuedRequests , DataPlaneMetricPrefix , time ) // control-plane private var controlPlaneProcessorOpt : Option [ Processor ] = None private [ network ] var controlPlaneAcceptorOpt : Option [ Acceptor ] = None val controlPlaneRequestChannelOpt : Option [ RequestChannel ] = config . controlPlaneListenerName . map ( _ => new RequestChannel ( 20 , ControlPlaneMetricPrefix , time )) private var nextProcessorId = 0 private var connectionQuotas : ConnectionQuotas = _ private var startedProcessingRequests = false private var stoppedProcessingRequests = false","title":"Socket Server"},{"location":"6-src/core/kafka/network/SocketServer/#socketserver","text":"\u652f\u6301 2 \u79cd\u7c7b\u578b\u7684\u8bf7\u6c42\uff1a 1. \u6570\u636e\uff1a - \u5904\u7406\u6765\u81ea client \u6216\u8005\u540c\u96c6\u7fa4\u5176\u4ed6 broker \u7684\u8bf7\u6c42 - \u7ebf\u7a0b\u6a21\u578b\u662f\uff1a - \u53ef\u4ee5\u914d\u7f6e\u591a\u4e2a listener\uff0c\u6bcf\u4e2a listener \u5bf9\u5e94 1 \u4e2a Acceptor \u7ebf\u7a0b\uff0c\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42 - Acceptor \u6709 N \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea sockets \u7684\u8bf7\u6c42 - M \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b 2. \u63a7\u5236\uff1a - \u5904\u7406\u6765\u81ea controller \u7684\u8bf7\u6c42\u3002\u8fd9\u662f\u53ef\u9009\u7684\u5e76\u4e14\u901a\u8fc7\u6307\u5b9a control.plane.listener.name \u8fdb\u884c\u914d\u7f6e\u3002\u5982\u679c\u6ca1\u6709\u914d\u7f6e\uff0ccontroller \u7684\u8bf7\u6c42\u4e5f\u4f1a\u50cf\u6570\u636e\u8bf7\u6c42\u4e00\u6837\u901a\u8fc7 listener \u5904\u7406 - \u7ebf\u7a0b\u6a21\u578b\u662f\uff1a - 1 \u4e2a Acceptor \u7ebf\u7a0b\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42 - Acceptor \u6709 1 \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea socket \u7684\u8bf7\u6c42 - 1 \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b","title":"SocketServer"},{"location":"6-src/core/kafka/network/SocketServer/#_1","text":"private val maxQueuedRequests = config . queuedMaxRequests private val logContext = new LogContext ( s\"[SocketServer brokerId= ${ config . brokerId } ] \" ) this . logIdent = logContext . logPrefix private val memoryPoolSensor = metrics . sensor ( \"MemoryPoolUtilization\" ) private val memoryPoolDepletedPercentMetricName = metrics . metricName ( \"MemoryPoolAvgDepletedPercent\" , MetricsGroup ) private val memoryPoolDepletedTimeMetricName = metrics . metricName ( \"MemoryPoolDepletedTimeTotal\" , MetricsGroup ) memoryPoolSensor . add ( new Meter ( TimeUnit . MILLISECONDS , memoryPoolDepletedPercentMetricName , memoryPoolDepletedTimeMetricName )) private val memoryPool = if ( config . queuedMaxBytes > 0 ) new SimpleMemoryPool ( config . queuedMaxBytes , config . socketRequestMaxBytes , false , memoryPoolSensor ) else MemoryPool . NONE // data-plane // Processor \u8bb0\u5f55 private val dataPlaneProcessors = new ConcurrentHashMap [ Int , Processor ]() private [ network ] val dataPlaneAcceptors = new ConcurrentHashMap [ EndPoint , Acceptor ]() val dataPlaneRequestChannel = new RequestChannel ( maxQueuedRequests , DataPlaneMetricPrefix , time ) // control-plane private var controlPlaneProcessorOpt : Option [ Processor ] = None private [ network ] var controlPlaneAcceptorOpt : Option [ Acceptor ] = None val controlPlaneRequestChannelOpt : Option [ RequestChannel ] = config . controlPlaneListenerName . map ( _ => new RequestChannel ( 20 , ControlPlaneMetricPrefix , time )) private var nextProcessorId = 0 private var connectionQuotas : ConnectionQuotas = _ private var startedProcessingRequests = false private var stoppedProcessingRequests = false","title":"\u5c5e\u6027"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/","text":"Acceptor \u5c5e\u6027 private val nioSelector = NSelector . open () val serverChannel = openServerSocket ( endPoint . host , endPoint . port ) // \u5173\u8054\u7684 Processor \u8bb0\u5f55 private val processors = new ArrayBuffer [ Processor ]() \u65b9\u6cd5 run \u5728 run \u4e2d\u4e0d\u65ad\u5c1d\u8bd5\u5904\u7406\u8fde\u63a5\uff1a /** * Accept loop that checks for new connection attempts */ def run (): Unit = { serverChannel . register ( nioSelector , SelectionKey . OP_ACCEPT ) startupComplete () try { while ( isRunning ) { try { acceptNewConnections () closeThrottledConnections () } catch { // We catch all the throwables to prevent the acceptor thread from exiting on exceptions due // to a select operation on a specific channel or a bad request. We don't want // the broker to stop responding to requests from other clients in these scenarios. case e : ControlThrowable => throw e case e : Throwable => error ( \"Error occurred\" , e ) } } } finally { debug ( \"Closing server socket, selector, and any throttled sockets.\" ) CoreUtils . swallow ( serverChannel . close (), this , Level . ERROR ) CoreUtils . swallow ( nioSelector . close (), this , Level . ERROR ) throttledSockets . foreach ( throttledSocket => closeSocket ( throttledSocket . socket )) throttledSockets . clear () shutdownComplete () } } acceptNewConnections \u5728 acceptNewConnections \u4e2d\u901a\u8fc7 nio select \u7b5b\u9009\u53ef\u8fde\u63a5\u7684 channel\uff0caccept \u4e4b\uff0c\u5bf9\u6bcf\u4e2a accepted channel\uff0c \u5c1d\u8bd5\u5728 processors \u4e2d\u9009\u62e9\u4e00\u4e2a processor \u5e76\u5206\u914d\u4e4b /** * Listen for new connections and assign accepted connections to processors using round-robin. */ private def acceptNewConnections (): Unit = { val ready = nioSelector . select ( 500 ) if ( ready > 0 ) { val keys = nioSelector . selectedKeys () val iter = keys . iterator () while ( iter . hasNext && isRunning ) { try { val key = iter . next iter . remove () if ( key . isAcceptable ) { accept ( key ). foreach { socketChannel => // Assign the channel to the next processor (using round-robin) to which the // channel can be added without blocking. If newConnections queue is full on // all processors, block until the last one is able to accept a connection. var retriesLeft = synchronized ( processors . length ) var processor : Processor = null do { retriesLeft -= 1 processor = synchronized { // adjust the index (if necessary) and retrieve the processor atomically for // correct behaviour in case the number of processors is reduced dynamically currentProcessorIndex = currentProcessorIndex % processors . length processors ( currentProcessorIndex ) } currentProcessorIndex += 1 } while ( ! assignNewConnection ( socketChannel , processor , retriesLeft == 0 )) } } else throw new IllegalStateException ( \"Unrecognized key state for acceptor thread.\" ) } catch { case e : Throwable => error ( \"Error while accepting connection\" , e ) } } } } assignNewConnection assignNewConnection \u901a\u8fc7\u8c03\u7528 processor.accept \u5c06 socketChannel \u52a0\u5165\u5230 processor \u81ea\u8eab\u7ef4\u62a4\u7684 newConnections \u961f\u5217\u4e2d private def assignNewConnection ( socketChannel : SocketChannel , processor : Processor , mayBlock : Boolean ): Boolean = { if ( processor . accept ( socketChannel , mayBlock , blockedPercentMeter )) { debug ( s\"Accepted connection from ${ socketChannel . socket . getRemoteSocketAddress } on\" + s\" ${ socketChannel . socket . getLocalSocketAddress } and assigned it to processor ${ processor . id } ,\" + s\" sendBufferSize [actual|requested]: [ ${ socketChannel . socket . getSendBufferSize } | $ sendBufferSize ]\" + s\" recvBufferSize [actual|requested]: [ ${ socketChannel . socket . getReceiveBufferSize } | $ recvBufferSize ]\" ) true } else false }","title":"Acceptor"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#acceptor","text":"","title":"Acceptor"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#_1","text":"private val nioSelector = NSelector . open () val serverChannel = openServerSocket ( endPoint . host , endPoint . port ) // \u5173\u8054\u7684 Processor \u8bb0\u5f55 private val processors = new ArrayBuffer [ Processor ]()","title":"\u5c5e\u6027"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#_2","text":"","title":"\u65b9\u6cd5"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#run","text":"\u5728 run \u4e2d\u4e0d\u65ad\u5c1d\u8bd5\u5904\u7406\u8fde\u63a5\uff1a /** * Accept loop that checks for new connection attempts */ def run (): Unit = { serverChannel . register ( nioSelector , SelectionKey . OP_ACCEPT ) startupComplete () try { while ( isRunning ) { try { acceptNewConnections () closeThrottledConnections () } catch { // We catch all the throwables to prevent the acceptor thread from exiting on exceptions due // to a select operation on a specific channel or a bad request. We don't want // the broker to stop responding to requests from other clients in these scenarios. case e : ControlThrowable => throw e case e : Throwable => error ( \"Error occurred\" , e ) } } } finally { debug ( \"Closing server socket, selector, and any throttled sockets.\" ) CoreUtils . swallow ( serverChannel . close (), this , Level . ERROR ) CoreUtils . swallow ( nioSelector . close (), this , Level . ERROR ) throttledSockets . foreach ( throttledSocket => closeSocket ( throttledSocket . socket )) throttledSockets . clear () shutdownComplete () } }","title":"run"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#acceptnewconnections","text":"\u5728 acceptNewConnections \u4e2d\u901a\u8fc7 nio select \u7b5b\u9009\u53ef\u8fde\u63a5\u7684 channel\uff0caccept \u4e4b\uff0c\u5bf9\u6bcf\u4e2a accepted channel\uff0c \u5c1d\u8bd5\u5728 processors \u4e2d\u9009\u62e9\u4e00\u4e2a processor \u5e76\u5206\u914d\u4e4b /** * Listen for new connections and assign accepted connections to processors using round-robin. */ private def acceptNewConnections (): Unit = { val ready = nioSelector . select ( 500 ) if ( ready > 0 ) { val keys = nioSelector . selectedKeys () val iter = keys . iterator () while ( iter . hasNext && isRunning ) { try { val key = iter . next iter . remove () if ( key . isAcceptable ) { accept ( key ). foreach { socketChannel => // Assign the channel to the next processor (using round-robin) to which the // channel can be added without blocking. If newConnections queue is full on // all processors, block until the last one is able to accept a connection. var retriesLeft = synchronized ( processors . length ) var processor : Processor = null do { retriesLeft -= 1 processor = synchronized { // adjust the index (if necessary) and retrieve the processor atomically for // correct behaviour in case the number of processors is reduced dynamically currentProcessorIndex = currentProcessorIndex % processors . length processors ( currentProcessorIndex ) } currentProcessorIndex += 1 } while ( ! assignNewConnection ( socketChannel , processor , retriesLeft == 0 )) } } else throw new IllegalStateException ( \"Unrecognized key state for acceptor thread.\" ) } catch { case e : Throwable => error ( \"Error while accepting connection\" , e ) } } } }","title":"acceptNewConnections"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#assignnewconnection","text":"assignNewConnection \u901a\u8fc7\u8c03\u7528 processor.accept \u5c06 socketChannel \u52a0\u5165\u5230 processor \u81ea\u8eab\u7ef4\u62a4\u7684 newConnections \u961f\u5217\u4e2d private def assignNewConnection ( socketChannel : SocketChannel , processor : Processor , mayBlock : Boolean ): Boolean = { if ( processor . accept ( socketChannel , mayBlock , blockedPercentMeter )) { debug ( s\"Accepted connection from ${ socketChannel . socket . getRemoteSocketAddress } on\" + s\" ${ socketChannel . socket . getLocalSocketAddress } and assigned it to processor ${ processor . id } ,\" + s\" sendBufferSize [actual|requested]: [ ${ socketChannel . socket . getSendBufferSize } | $ sendBufferSize ]\" + s\" recvBufferSize [actual|requested]: [ ${ socketChannel . socket . getReceiveBufferSize } | $ recvBufferSize ]\" ) true } else false }","title":"assignNewConnection"},{"location":"6-src/core/kafka/network/SocketServer/Processor/","text":"Processor \u5c5e\u6027 // Acceptor \u5904\u7406\u5f97\u5230\u7684\u5df2\u8fde\u63a5\u8bf7\u6c42\u4f1a\u8bb0\u5f55\u5230\u8fd9\u91cc private val newConnections = new ArrayBlockingQueue [ SocketChannel ]( connectionQueueSize ) private val inflightResponses = mutable . Map [ String , RequestChannel . Response ]() // RequestChannel \u8c03\u7528 sendResponse() \u4f1a\u628a\u54cd\u5e94\u8bb0\u5f55\u5728\u8fd9\u91cc private val responseQueue = new LinkedBlockingDeque [ RequestChannel . Response ]() Acceptor \u901a\u8fc7 processor.accept \u5c06 socketChannel \u52a0\u5165\u5230 processor \u7684 newConnections \u4e2d Processor \u6570\u91cf\u7531 num.networker.threads \u51b3\u5b9a processor \u7684 run \u5904\u7406 socketChannel override def run (): Unit = { startupComplete () try { while ( isRunning ) { try { // setup any new connections that have been queued up // \u8fd9\u91cc\u4ece newConnections \u4e2d\u53d6\u51fa channel\uff0c\u5e76\u6ce8\u518c\u5230 selector configureNewConnections () // register any new responses for writing // \u4ece responseQueue \u4e2d\u53d6\u51fa response\uff0c\u8fdb\u884c \uff1f\u5904\u7406 processNewResponses () // select keys, \u8fdb\u884c\u5b9e\u9645\u7684\u8bfb\u5199\u4e8b\u4ef6\u5904\u7406 poll () // \u904d\u5386\u6bcf\u4e2a\u5df2\u7ecf\u5b8c\u6210\u63a5\u6536\u7684 receive\uff0c\u901a\u8fc7 requestChannel.sendRequest(req)\uff0c\u5c06\u8bf7\u6c42\u653e\u5230 RequestChannel \u7684 requestQueue \u4e2d\uff0c\u4f9b\u540e\u7eed\u5904\u7406 processCompletedReceives () // \u904d\u5386\u6bcf\u4e2a\u5df2\u7ecf\u5b8c\u6210\u53d1\u9001\u7684 send processCompletedSends () processDisconnected () closeExcessConnections () } catch { // We catch all the throwables here to prevent the processor thread from exiting. We do this because // letting a processor exit might cause a bigger impact on the broker. This behavior might need to be // reviewed if we see an exception that needs the entire broker to stop. Usually the exceptions thrown would // be either associated with a specific socket channel or a bad request. These exceptions are caught and // processed by the individual methods above which close the failing channel and continue processing other // channels. So this catch block should only ever see ControlThrowables. case e : Throwable => processException ( \"Processor got uncaught exception.\" , e ) } } } finally { debug ( s\"Closing selector - processor $ id \" ) CoreUtils . swallow ( closeAll (), this , Level . ERROR ) shutdownComplete () } }","title":"Processor"},{"location":"6-src/core/kafka/network/SocketServer/Processor/#processor","text":"","title":"Processor"},{"location":"6-src/core/kafka/network/SocketServer/Processor/#_1","text":"// Acceptor \u5904\u7406\u5f97\u5230\u7684\u5df2\u8fde\u63a5\u8bf7\u6c42\u4f1a\u8bb0\u5f55\u5230\u8fd9\u91cc private val newConnections = new ArrayBlockingQueue [ SocketChannel ]( connectionQueueSize ) private val inflightResponses = mutable . Map [ String , RequestChannel . Response ]() // RequestChannel \u8c03\u7528 sendResponse() \u4f1a\u628a\u54cd\u5e94\u8bb0\u5f55\u5728\u8fd9\u91cc private val responseQueue = new LinkedBlockingDeque [ RequestChannel . Response ]() Acceptor \u901a\u8fc7 processor.accept \u5c06 socketChannel \u52a0\u5165\u5230 processor \u7684 newConnections \u4e2d Processor \u6570\u91cf\u7531 num.networker.threads \u51b3\u5b9a processor \u7684 run \u5904\u7406 socketChannel override def run (): Unit = { startupComplete () try { while ( isRunning ) { try { // setup any new connections that have been queued up // \u8fd9\u91cc\u4ece newConnections \u4e2d\u53d6\u51fa channel\uff0c\u5e76\u6ce8\u518c\u5230 selector configureNewConnections () // register any new responses for writing // \u4ece responseQueue \u4e2d\u53d6\u51fa response\uff0c\u8fdb\u884c \uff1f\u5904\u7406 processNewResponses () // select keys, \u8fdb\u884c\u5b9e\u9645\u7684\u8bfb\u5199\u4e8b\u4ef6\u5904\u7406 poll () // \u904d\u5386\u6bcf\u4e2a\u5df2\u7ecf\u5b8c\u6210\u63a5\u6536\u7684 receive\uff0c\u901a\u8fc7 requestChannel.sendRequest(req)\uff0c\u5c06\u8bf7\u6c42\u653e\u5230 RequestChannel \u7684 requestQueue \u4e2d\uff0c\u4f9b\u540e\u7eed\u5904\u7406 processCompletedReceives () // \u904d\u5386\u6bcf\u4e2a\u5df2\u7ecf\u5b8c\u6210\u53d1\u9001\u7684 send processCompletedSends () processDisconnected () closeExcessConnections () } catch { // We catch all the throwables here to prevent the processor thread from exiting. We do this because // letting a processor exit might cause a bigger impact on the broker. This behavior might need to be // reviewed if we see an exception that needs the entire broker to stop. Usually the exceptions thrown would // be either associated with a specific socket channel or a bad request. These exceptions are caught and // processed by the individual methods above which close the failing channel and continue processing other // channels. So this catch block should only ever see ControlThrowables. case e : Throwable => processException ( \"Processor got uncaught exception.\" , e ) } } } finally { debug ( s\"Closing selector - processor $ id \" ) CoreUtils . swallow ( closeAll (), this , Level . ERROR ) shutdownComplete () } }","title":"\u5c5e\u6027"},{"location":"6-src/core/kafka/server/KafkaApis/","text":"KafkaApis \u5728 handle \u65b9\u6cd5\u4e2d\u5904\u7406 kafka \u8bf7\u6c42 \u4ee5 handleProduceRequest \u65b9\u6cd5\u4e3a\u4f8b\uff1a // call the replica manager to append messages to the replicas replicaManager . appendRecords ( timeout = produceRequest . timeout . toLong , requiredAcks = produceRequest . acks , internalTopicsAllowed = internalTopicsAllowed , origin = AppendOrigin . Client , entriesPerPartition = authorizedRequestInfo , responseCallback = sendResponseCallback , recordConversionStatsCallback = processingStatsCallback ) \u5c06 response \u901a\u8fc7 RequestChannel \u7684 sendResponse \u5199\u56de\u5230\u5bf9\u5e94\u7684 Processor \u7684 responseQueue \uff0c Processor \u7684 processNewResponses \u4f1a\u4ece responseQueue \u4e2d\u53d6\u56de response replicaManager","title":"Kafka Apis"},{"location":"6-src/core/kafka/server/KafkaApis/#kafkaapis","text":"\u5728 handle \u65b9\u6cd5\u4e2d\u5904\u7406 kafka \u8bf7\u6c42 \u4ee5 handleProduceRequest \u65b9\u6cd5\u4e3a\u4f8b\uff1a // call the replica manager to append messages to the replicas replicaManager . appendRecords ( timeout = produceRequest . timeout . toLong , requiredAcks = produceRequest . acks , internalTopicsAllowed = internalTopicsAllowed , origin = AppendOrigin . Client , entriesPerPartition = authorizedRequestInfo , responseCallback = sendResponseCallback , recordConversionStatsCallback = processingStatsCallback ) \u5c06 response \u901a\u8fc7 RequestChannel \u7684 sendResponse \u5199\u56de\u5230\u5bf9\u5e94\u7684 Processor \u7684 responseQueue \uff0c Processor \u7684 processNewResponses \u4f1a\u4ece responseQueue \u4e2d\u53d6\u56de response replicaManager","title":"KafkaApis"},{"location":"6-src/core/kafka/server/KafkaServer/","text":"KafkaServer \u65b9\u6cd5 startup /** * Start up API for bringing up a single instance of the Kafka server. * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers */ override def startup (): Unit = { try { info ( \"starting\" ) if ( isShuttingDown . get ) throw new IllegalStateException ( \"Kafka server is still shutting down, cannot re-start!\" ) if ( startupComplete . get ) return val canStartup = isStartingUp . compareAndSet ( false , true ) if ( canStartup ) { brokerState . set ( BrokerState . STARTING ) /* setup zookeeper */ initZkClient ( time ) configRepository = new ZkConfigRepository ( new AdminZkClient ( zkClient )) /* initialize features */ _featureChangeListener = new FinalizedFeatureChangeListener ( featureCache , _zkClient ) if ( config . isFeatureVersioningSupported ) { _featureChangeListener . initOrThrow ( config . zkConnectionTimeoutMs ) } /* Get or create cluster_id */ _clusterId = getOrGenerateClusterId ( zkClient ) info ( s\"Cluster ID = ${ clusterId } \" ) /* load metadata */ val ( preloadedBrokerMetadataCheckpoint , initialOfflineDirs ) = BrokerMetadataCheckpoint . getBrokerMetadataAndOfflineDirs ( config . logDirs , ignoreMissing = true ) if ( preloadedBrokerMetadataCheckpoint . version != 0 ) { throw new RuntimeException ( s\"Found unexpected version in loaded `meta.properties`: \" + s\" $ preloadedBrokerMetadataCheckpoint . Zk-based brokers only support version 0 \" + \"(which is implicit when the `version` field is missing).\" ) } /* check cluster id */ if ( preloadedBrokerMetadataCheckpoint . clusterId . isDefined && preloadedBrokerMetadataCheckpoint . clusterId . get != clusterId ) throw new InconsistentClusterIdException ( s\"The Cluster ID ${ clusterId } doesn't match stored clusterId ${ preloadedBrokerMetadataCheckpoint . clusterId } in meta.properties. \" + s\"The broker is trying to join the wrong cluster. Configured zookeeper.connect may be wrong.\" ) /* generate brokerId */ config . brokerId = getOrGenerateBrokerId ( preloadedBrokerMetadataCheckpoint ) logContext = new LogContext ( s\"[KafkaServer id= ${ config . brokerId } ] \" ) this . logIdent = logContext . logPrefix // initialize dynamic broker configs from ZooKeeper. Any updates made after this will be // applied after DynamicConfigManager starts. config . dynamicConfig . initialize ( zkClient ) /* start scheduler */ kafkaScheduler = new KafkaScheduler ( config . backgroundThreads ) kafkaScheduler . startup () /* create and configure metrics */ kafkaYammerMetrics = KafkaYammerMetrics . INSTANCE kafkaYammerMetrics . configure ( config . originals ) metrics = Server . initializeMetrics ( config , time , clusterId ) /* register broker metrics */ _brokerTopicStats = new BrokerTopicStats quotaManagers = QuotaFactory . instantiate ( config , metrics , time , threadNamePrefix . getOrElse ( \"\" )) KafkaBroker . notifyClusterListeners ( clusterId , kafkaMetricsReporters ++ metrics . reporters . asScala ) logDirFailureChannel = new LogDirFailureChannel ( config . logDirs . size ) /* start log manager */ logManager = LogManager ( config , initialOfflineDirs , new ZkConfigRepository ( new AdminZkClient ( zkClient )), kafkaScheduler , time , brokerTopicStats , logDirFailureChannel , config . usesTopicId ) brokerState . set ( BrokerState . RECOVERY ) logManager . startup ( zkClient . getAllTopicsInCluster ()) metadataCache = MetadataCache . zkMetadataCache ( config . brokerId ) // Enable delegation token cache for all SCRAM mechanisms to simplify dynamic update. // This keeps the cache up-to-date if new SCRAM mechanisms are enabled dynamically. tokenCache = new DelegationTokenCache ( ScramMechanism . mechanismNames ) credentialProvider = new CredentialProvider ( ScramMechanism . mechanismNames , tokenCache ) if ( enableForwarding ) { this . forwardingManager = Some ( ForwardingManager ( config , metadataCache , time , metrics , threadNamePrefix )) forwardingManager . foreach ( _ . start ()) } val apiVersionManager = ApiVersionManager ( ListenerType . ZK_BROKER , config , forwardingManager , brokerFeatures , featureCache ) // Create and start the socket server acceptor threads so that the bound port is known. // Delay starting processors until the end of the initialization sequence to ensure // that credentials have been loaded before processing authentications. // // Note that we allow the use of KRaft mode controller APIs when forwarding is enabled // so that the Envelope request is exposed. This is only used in testing currently. socketServer = new SocketServer ( config , metrics , time , credentialProvider , apiVersionManager ) socketServer . startup ( startProcessingRequests = false ) /* start replica manager */ alterIsrManager = if ( config . interBrokerProtocolVersion . isAlterIsrSupported ) { AlterIsrManager ( config = config , metadataCache = metadataCache , scheduler = kafkaScheduler , time = time , metrics = metrics , threadNamePrefix = threadNamePrefix , brokerEpochSupplier = () => kafkaController . brokerEpoch , config . brokerId ) } else { AlterIsrManager ( kafkaScheduler , time , zkClient ) } alterIsrManager . start () replicaManager = createReplicaManager ( isShuttingDown ) replicaManager . startup () val brokerInfo = createBrokerInfo val brokerEpoch = zkClient . registerBroker ( brokerInfo ) // Now that the broker is successfully registered, checkpoint its metadata checkpointBrokerMetadata ( ZkMetaProperties ( clusterId , config . brokerId )) /* start token manager */ tokenManager = new DelegationTokenManager ( config , tokenCache , time , zkClient ) tokenManager . startup () /* start kafka controller */ kafkaController = new KafkaController ( config , zkClient , time , metrics , brokerInfo , brokerEpoch , tokenManager , brokerFeatures , featureCache , threadNamePrefix ) kafkaController . startup () adminManager = new ZkAdminManager ( config , metrics , metadataCache , zkClient ) /* start group coordinator */ // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue groupCoordinator = GroupCoordinator ( config , replicaManager , Time . SYSTEM , metrics ) groupCoordinator . startup (() => zkClient . getTopicPartitionCount ( Topic . GROUP_METADATA_TOPIC_NAME ). getOrElse ( config . offsetsTopicPartitions )) /* start transaction coordinator, with a separate background thread scheduler for transaction expiration and log loading */ // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue transactionCoordinator = TransactionCoordinator ( config , replicaManager , new KafkaScheduler ( threads = 1 , threadNamePrefix = \"transaction-log-manager-\" ), () => new ProducerIdManager ( config . brokerId , zkClient ), metrics , metadataCache , Time . SYSTEM ) transactionCoordinator . startup ( () => zkClient . getTopicPartitionCount ( Topic . TRANSACTION_STATE_TOPIC_NAME ). getOrElse ( config . transactionTopicPartitions )) /* start auto topic creation manager */ this . autoTopicCreationManager = AutoTopicCreationManager ( config , metadataCache , time , metrics , threadNamePrefix , Some ( adminManager ), Some ( kafkaController ), groupCoordinator , transactionCoordinator , enableForwarding ) autoTopicCreationManager . start () /* Get the authorizer and initialize it if one is specified.*/ authorizer = config . authorizer authorizer . foreach ( _ . configure ( config . originals )) val authorizerFutures : Map [ Endpoint , CompletableFuture [ Void ]] = authorizer match { case Some ( authZ ) => authZ . start ( brokerInfo . broker . toServerInfo ( clusterId , config )). asScala . map { case ( ep , cs ) => ep -> cs . toCompletableFuture } case None => brokerInfo . broker . endPoints . map { ep => ep . toJava -> CompletableFuture . completedFuture [ Void ]( null ) }. toMap } val fetchManager = new FetchManager ( Time . SYSTEM , new FetchSessionCache ( config . maxIncrementalFetchSessionCacheSlots , KafkaServer . MIN_INCREMENTAL_FETCH_SESSION_EVICTION_MS )) /* start processing requests */ val zkSupport = ZkSupport ( adminManager , kafkaController , zkClient , forwardingManager , metadataCache ) dataPlaneRequestProcessor = new KafkaApis ( socketServer . dataPlaneRequestChannel , zkSupport , replicaManager , groupCoordinator , transactionCoordinator , autoTopicCreationManager , config . brokerId , config , configRepository , metadataCache , metrics , authorizer , quotaManagers , fetchManager , brokerTopicStats , clusterId , time , tokenManager , apiVersionManager ) dataPlaneRequestHandlerPool = new KafkaRequestHandlerPool ( config . brokerId , socketServer . dataPlaneRequestChannel , dataPlaneRequestProcessor , time , config . numIoThreads , s\" ${ SocketServer . DataPlaneMetricPrefix } RequestHandlerAvgIdlePercent\" , SocketServer . DataPlaneThreadPrefix ) socketServer . controlPlaneRequestChannelOpt . foreach { controlPlaneRequestChannel => controlPlaneRequestProcessor = new KafkaApis ( controlPlaneRequestChannel , zkSupport , replicaManager , groupCoordinator , transactionCoordinator , autoTopicCreationManager , config . brokerId , config , configRepository , metadataCache , metrics , authorizer , quotaManagers , fetchManager , brokerTopicStats , clusterId , time , tokenManager , apiVersionManager ) controlPlaneRequestHandlerPool = new KafkaRequestHandlerPool ( config . brokerId , socketServer . controlPlaneRequestChannelOpt . get , controlPlaneRequestProcessor , time , 1 , s\" ${ SocketServer . ControlPlaneMetricPrefix } RequestHandlerAvgIdlePercent\" , SocketServer . ControlPlaneThreadPrefix ) } Mx4jLoader . maybeLoad () /* Add all reconfigurables for config change notification before starting config handlers */ config . dynamicConfig . addReconfigurables ( this ) /* start dynamic config manager */ dynamicConfigHandlers = Map [ String , ConfigHandler ]( ConfigType . Topic -> new TopicConfigHandler ( logManager , config , quotaManagers , kafkaController ), ConfigType . Client -> new ClientIdConfigHandler ( quotaManagers ), ConfigType . User -> new UserConfigHandler ( quotaManagers , credentialProvider ), ConfigType . Broker -> new BrokerConfigHandler ( config , quotaManagers ), ConfigType . Ip -> new IpConfigHandler ( socketServer . connectionQuotas )) // Create the config manager. start listening to notifications dynamicConfigManager = new DynamicConfigManager ( zkClient , dynamicConfigHandlers ) dynamicConfigManager . startup () socketServer . startProcessingRequests ( authorizerFutures ) brokerState . set ( BrokerState . RUNNING ) shutdownLatch = new CountDownLatch ( 1 ) startupComplete . set ( true ) isStartingUp . set ( false ) AppInfoParser . registerAppInfo ( Server . MetricsPrefix , config . brokerId . toString , metrics , time . milliseconds ()) info ( \"started\" ) } } catch { case e : Throwable => fatal ( \"Fatal error during KafkaServer startup. Prepare to shutdown\" , e ) isStartingUp . set ( false ) shutdown () throw e } } \u521b\u5efa SocketServer \u5e76\u8c03\u7528 socketServer.startup()","title":"Kafka Server"},{"location":"6-src/core/kafka/server/KafkaServer/#kafkaserver","text":"","title":"KafkaServer"},{"location":"6-src/core/kafka/server/KafkaServer/#_1","text":"","title":"\u65b9\u6cd5"},{"location":"6-src/core/kafka/server/KafkaServer/#startup","text":"/** * Start up API for bringing up a single instance of the Kafka server. * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers */ override def startup (): Unit = { try { info ( \"starting\" ) if ( isShuttingDown . get ) throw new IllegalStateException ( \"Kafka server is still shutting down, cannot re-start!\" ) if ( startupComplete . get ) return val canStartup = isStartingUp . compareAndSet ( false , true ) if ( canStartup ) { brokerState . set ( BrokerState . STARTING ) /* setup zookeeper */ initZkClient ( time ) configRepository = new ZkConfigRepository ( new AdminZkClient ( zkClient )) /* initialize features */ _featureChangeListener = new FinalizedFeatureChangeListener ( featureCache , _zkClient ) if ( config . isFeatureVersioningSupported ) { _featureChangeListener . initOrThrow ( config . zkConnectionTimeoutMs ) } /* Get or create cluster_id */ _clusterId = getOrGenerateClusterId ( zkClient ) info ( s\"Cluster ID = ${ clusterId } \" ) /* load metadata */ val ( preloadedBrokerMetadataCheckpoint , initialOfflineDirs ) = BrokerMetadataCheckpoint . getBrokerMetadataAndOfflineDirs ( config . logDirs , ignoreMissing = true ) if ( preloadedBrokerMetadataCheckpoint . version != 0 ) { throw new RuntimeException ( s\"Found unexpected version in loaded `meta.properties`: \" + s\" $ preloadedBrokerMetadataCheckpoint . Zk-based brokers only support version 0 \" + \"(which is implicit when the `version` field is missing).\" ) } /* check cluster id */ if ( preloadedBrokerMetadataCheckpoint . clusterId . isDefined && preloadedBrokerMetadataCheckpoint . clusterId . get != clusterId ) throw new InconsistentClusterIdException ( s\"The Cluster ID ${ clusterId } doesn't match stored clusterId ${ preloadedBrokerMetadataCheckpoint . clusterId } in meta.properties. \" + s\"The broker is trying to join the wrong cluster. Configured zookeeper.connect may be wrong.\" ) /* generate brokerId */ config . brokerId = getOrGenerateBrokerId ( preloadedBrokerMetadataCheckpoint ) logContext = new LogContext ( s\"[KafkaServer id= ${ config . brokerId } ] \" ) this . logIdent = logContext . logPrefix // initialize dynamic broker configs from ZooKeeper. Any updates made after this will be // applied after DynamicConfigManager starts. config . dynamicConfig . initialize ( zkClient ) /* start scheduler */ kafkaScheduler = new KafkaScheduler ( config . backgroundThreads ) kafkaScheduler . startup () /* create and configure metrics */ kafkaYammerMetrics = KafkaYammerMetrics . INSTANCE kafkaYammerMetrics . configure ( config . originals ) metrics = Server . initializeMetrics ( config , time , clusterId ) /* register broker metrics */ _brokerTopicStats = new BrokerTopicStats quotaManagers = QuotaFactory . instantiate ( config , metrics , time , threadNamePrefix . getOrElse ( \"\" )) KafkaBroker . notifyClusterListeners ( clusterId , kafkaMetricsReporters ++ metrics . reporters . asScala ) logDirFailureChannel = new LogDirFailureChannel ( config . logDirs . size ) /* start log manager */ logManager = LogManager ( config , initialOfflineDirs , new ZkConfigRepository ( new AdminZkClient ( zkClient )), kafkaScheduler , time , brokerTopicStats , logDirFailureChannel , config . usesTopicId ) brokerState . set ( BrokerState . RECOVERY ) logManager . startup ( zkClient . getAllTopicsInCluster ()) metadataCache = MetadataCache . zkMetadataCache ( config . brokerId ) // Enable delegation token cache for all SCRAM mechanisms to simplify dynamic update. // This keeps the cache up-to-date if new SCRAM mechanisms are enabled dynamically. tokenCache = new DelegationTokenCache ( ScramMechanism . mechanismNames ) credentialProvider = new CredentialProvider ( ScramMechanism . mechanismNames , tokenCache ) if ( enableForwarding ) { this . forwardingManager = Some ( ForwardingManager ( config , metadataCache , time , metrics , threadNamePrefix )) forwardingManager . foreach ( _ . start ()) } val apiVersionManager = ApiVersionManager ( ListenerType . ZK_BROKER , config , forwardingManager , brokerFeatures , featureCache ) // Create and start the socket server acceptor threads so that the bound port is known. // Delay starting processors until the end of the initialization sequence to ensure // that credentials have been loaded before processing authentications. // // Note that we allow the use of KRaft mode controller APIs when forwarding is enabled // so that the Envelope request is exposed. This is only used in testing currently. socketServer = new SocketServer ( config , metrics , time , credentialProvider , apiVersionManager ) socketServer . startup ( startProcessingRequests = false ) /* start replica manager */ alterIsrManager = if ( config . interBrokerProtocolVersion . isAlterIsrSupported ) { AlterIsrManager ( config = config , metadataCache = metadataCache , scheduler = kafkaScheduler , time = time , metrics = metrics , threadNamePrefix = threadNamePrefix , brokerEpochSupplier = () => kafkaController . brokerEpoch , config . brokerId ) } else { AlterIsrManager ( kafkaScheduler , time , zkClient ) } alterIsrManager . start () replicaManager = createReplicaManager ( isShuttingDown ) replicaManager . startup () val brokerInfo = createBrokerInfo val brokerEpoch = zkClient . registerBroker ( brokerInfo ) // Now that the broker is successfully registered, checkpoint its metadata checkpointBrokerMetadata ( ZkMetaProperties ( clusterId , config . brokerId )) /* start token manager */ tokenManager = new DelegationTokenManager ( config , tokenCache , time , zkClient ) tokenManager . startup () /* start kafka controller */ kafkaController = new KafkaController ( config , zkClient , time , metrics , brokerInfo , brokerEpoch , tokenManager , brokerFeatures , featureCache , threadNamePrefix ) kafkaController . startup () adminManager = new ZkAdminManager ( config , metrics , metadataCache , zkClient ) /* start group coordinator */ // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue groupCoordinator = GroupCoordinator ( config , replicaManager , Time . SYSTEM , metrics ) groupCoordinator . startup (() => zkClient . getTopicPartitionCount ( Topic . GROUP_METADATA_TOPIC_NAME ). getOrElse ( config . offsetsTopicPartitions )) /* start transaction coordinator, with a separate background thread scheduler for transaction expiration and log loading */ // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue transactionCoordinator = TransactionCoordinator ( config , replicaManager , new KafkaScheduler ( threads = 1 , threadNamePrefix = \"transaction-log-manager-\" ), () => new ProducerIdManager ( config . brokerId , zkClient ), metrics , metadataCache , Time . SYSTEM ) transactionCoordinator . startup ( () => zkClient . getTopicPartitionCount ( Topic . TRANSACTION_STATE_TOPIC_NAME ). getOrElse ( config . transactionTopicPartitions )) /* start auto topic creation manager */ this . autoTopicCreationManager = AutoTopicCreationManager ( config , metadataCache , time , metrics , threadNamePrefix , Some ( adminManager ), Some ( kafkaController ), groupCoordinator , transactionCoordinator , enableForwarding ) autoTopicCreationManager . start () /* Get the authorizer and initialize it if one is specified.*/ authorizer = config . authorizer authorizer . foreach ( _ . configure ( config . originals )) val authorizerFutures : Map [ Endpoint , CompletableFuture [ Void ]] = authorizer match { case Some ( authZ ) => authZ . start ( brokerInfo . broker . toServerInfo ( clusterId , config )). asScala . map { case ( ep , cs ) => ep -> cs . toCompletableFuture } case None => brokerInfo . broker . endPoints . map { ep => ep . toJava -> CompletableFuture . completedFuture [ Void ]( null ) }. toMap } val fetchManager = new FetchManager ( Time . SYSTEM , new FetchSessionCache ( config . maxIncrementalFetchSessionCacheSlots , KafkaServer . MIN_INCREMENTAL_FETCH_SESSION_EVICTION_MS )) /* start processing requests */ val zkSupport = ZkSupport ( adminManager , kafkaController , zkClient , forwardingManager , metadataCache ) dataPlaneRequestProcessor = new KafkaApis ( socketServer . dataPlaneRequestChannel , zkSupport , replicaManager , groupCoordinator , transactionCoordinator , autoTopicCreationManager , config . brokerId , config , configRepository , metadataCache , metrics , authorizer , quotaManagers , fetchManager , brokerTopicStats , clusterId , time , tokenManager , apiVersionManager ) dataPlaneRequestHandlerPool = new KafkaRequestHandlerPool ( config . brokerId , socketServer . dataPlaneRequestChannel , dataPlaneRequestProcessor , time , config . numIoThreads , s\" ${ SocketServer . DataPlaneMetricPrefix } RequestHandlerAvgIdlePercent\" , SocketServer . DataPlaneThreadPrefix ) socketServer . controlPlaneRequestChannelOpt . foreach { controlPlaneRequestChannel => controlPlaneRequestProcessor = new KafkaApis ( controlPlaneRequestChannel , zkSupport , replicaManager , groupCoordinator , transactionCoordinator , autoTopicCreationManager , config . brokerId , config , configRepository , metadataCache , metrics , authorizer , quotaManagers , fetchManager , brokerTopicStats , clusterId , time , tokenManager , apiVersionManager ) controlPlaneRequestHandlerPool = new KafkaRequestHandlerPool ( config . brokerId , socketServer . controlPlaneRequestChannelOpt . get , controlPlaneRequestProcessor , time , 1 , s\" ${ SocketServer . ControlPlaneMetricPrefix } RequestHandlerAvgIdlePercent\" , SocketServer . ControlPlaneThreadPrefix ) } Mx4jLoader . maybeLoad () /* Add all reconfigurables for config change notification before starting config handlers */ config . dynamicConfig . addReconfigurables ( this ) /* start dynamic config manager */ dynamicConfigHandlers = Map [ String , ConfigHandler ]( ConfigType . Topic -> new TopicConfigHandler ( logManager , config , quotaManagers , kafkaController ), ConfigType . Client -> new ClientIdConfigHandler ( quotaManagers ), ConfigType . User -> new UserConfigHandler ( quotaManagers , credentialProvider ), ConfigType . Broker -> new BrokerConfigHandler ( config , quotaManagers ), ConfigType . Ip -> new IpConfigHandler ( socketServer . connectionQuotas )) // Create the config manager. start listening to notifications dynamicConfigManager = new DynamicConfigManager ( zkClient , dynamicConfigHandlers ) dynamicConfigManager . startup () socketServer . startProcessingRequests ( authorizerFutures ) brokerState . set ( BrokerState . RUNNING ) shutdownLatch = new CountDownLatch ( 1 ) startupComplete . set ( true ) isStartingUp . set ( false ) AppInfoParser . registerAppInfo ( Server . MetricsPrefix , config . brokerId . toString , metrics , time . milliseconds ()) info ( \"started\" ) } } catch { case e : Throwable => fatal ( \"Fatal error during KafkaServer startup. Prepare to shutdown\" , e ) isStartingUp . set ( false ) shutdown () throw e } } \u521b\u5efa SocketServer \u5e76\u8c03\u7528 socketServer.startup()","title":"startup"},{"location":"6-src/core/kafka/server/KafkaRequestHandler/","text":"KafkaRequestHandler \u4ece RequestChannel \u7684 requestQueue \u4e2d\u53d6\u51fa\u8bf7\u6c42\uff0c\u5e76\u901a\u8fc7 ApiRequestHandler.handle \u8fdb\u884c\u5904\u7406\uff08\u8fd9\u91cc\u7684\u5b9e\u73b0\u5728 KafkaApis \u4e2d\uff09 def run (): Unit = { while ( ! stopped ) { // We use a single meter for aggregate idle percentage for the thread pool. // Since meter is calculated as total_recorded_value / time_window and // time_window is independent of the number of threads, each recorded idle // time should be discounted by # threads. val startSelectTime = time . nanoseconds val req = requestChannel . receiveRequest ( 300 ) val endTime = time . nanoseconds val idleTime = endTime - startSelectTime aggregateIdleMeter . mark ( idleTime / totalHandlerThreads . get ) req match { case RequestChannel . ShutdownRequest => debug ( s\"Kafka request handler $ id on broker $ brokerId received shut down command\" ) shutdownComplete . countDown () return case request : RequestChannel . Request => try { request . requestDequeueTimeNanos = endTime trace ( s\"Kafka request handler $ id on broker $ brokerId handling request $ request \" ) apis . handle ( request ) } catch { case e : FatalExitError => shutdownComplete . countDown () Exit . exit ( e . statusCode ) case e : Throwable => error ( \"Exception when handling request\" , e ) } finally { request . releaseBuffer () } case null => // continue } } shutdownComplete . countDown () }","title":"Kafka Request Handler"},{"location":"6-src/core/kafka/server/KafkaRequestHandler/#kafkarequesthandler","text":"\u4ece RequestChannel \u7684 requestQueue \u4e2d\u53d6\u51fa\u8bf7\u6c42\uff0c\u5e76\u901a\u8fc7 ApiRequestHandler.handle \u8fdb\u884c\u5904\u7406\uff08\u8fd9\u91cc\u7684\u5b9e\u73b0\u5728 KafkaApis \u4e2d\uff09 def run (): Unit = { while ( ! stopped ) { // We use a single meter for aggregate idle percentage for the thread pool. // Since meter is calculated as total_recorded_value / time_window and // time_window is independent of the number of threads, each recorded idle // time should be discounted by # threads. val startSelectTime = time . nanoseconds val req = requestChannel . receiveRequest ( 300 ) val endTime = time . nanoseconds val idleTime = endTime - startSelectTime aggregateIdleMeter . mark ( idleTime / totalHandlerThreads . get ) req match { case RequestChannel . ShutdownRequest => debug ( s\"Kafka request handler $ id on broker $ brokerId received shut down command\" ) shutdownComplete . countDown () return case request : RequestChannel . Request => try { request . requestDequeueTimeNanos = endTime trace ( s\"Kafka request handler $ id on broker $ brokerId handling request $ request \" ) apis . handle ( request ) } catch { case e : FatalExitError => shutdownComplete . countDown () Exit . exit ( e . statusCode ) case e : Throwable => error ( \"Exception when handling request\" , e ) } finally { request . releaseBuffer () } case null => // continue } } shutdownComplete . countDown () }","title":"KafkaRequestHandler"},{"location":"6-src/core/kafka/server/KafkaRequestHandler/KafkaRequestHandlerPool/","text":"KafkaRequestHandlerPool \u521b\u5efa\u591a\u4e2a KafkaRequestHandler \uff08\u7531 num.io.threads \u914d\u7f6e\uff09","title":"Kafka Request Handler Pool"},{"location":"6-src/core/kafka/server/KafkaRequestHandler/KafkaRequestHandlerPool/#kafkarequesthandlerpool","text":"\u521b\u5efa\u591a\u4e2a KafkaRequestHandler \uff08\u7531 num.io.threads \u914d\u7f6e\uff09","title":"KafkaRequestHandlerPool"},{"location":"6-src/core/kafka/utils/KafkaScheduler/","text":"","title":"Kafka Scheduler"},{"location":"7-other/1-topic-name-limit/","text":"7.1 topic \u540d\u89c4\u5219 topic topic \u540d \u4e0d\u80fd\u4e3a\u7a7a\u5b57\u7b26\u4e32 topic \u540d \u4e0d\u80fd\u4e3a . \u6216\u8005 .. topic \u540d \u4e0d\u80fd\u8d85\u8fc7 249 \u4e2a\u5b57\u7b26 topic \u540d \u7531 [a-zA-Z0-9._-] \u8fd9\u4e9b\u5b57\u7b26\u7ec4\u6210 public static final String LEGAL_CHARS = \"[a-zA-Z0-9._-]\" ; private static final int MAX_NAME_LENGTH = 249 ; public static void validate ( String topic ) { if ( topic . isEmpty ()) throw new InvalidTopicException ( \"Topic name is illegal, it can't be empty\" ); if ( topic . equals ( \".\" ) || topic . equals ( \"..\" )) throw new InvalidTopicException ( \"Topic name cannot be \\\".\\\" or \\\"..\\\"\" ); if ( topic . length () > MAX_NAME_LENGTH ) throw new InvalidTopicException ( \"Topic name is illegal, it can't be longer than \" + MAX_NAME_LENGTH + \" characters, topic name: \" + topic ); if ( ! containsValidPattern ( topic )) throw new InvalidTopicException ( \"Topic name \\\"\" + topic + \"\\\" is illegal, it contains a character other than \" + \"ASCII alphanumerics, '.', '_' and '-'\" ); } /** * Valid characters for Kafka topics are the ASCII alphanumerics, '.', '_', and '-' */ static boolean containsValidPattern ( String topic ) { for ( int i = 0 ; i < topic . length (); ++ i ) { char c = topic . charAt ( i ); // We don't use Character.isLetterOrDigit(c) because it's slower boolean validChar = ( c >= 'a' && c <= 'z' ) || ( c >= '0' && c <= '9' ) || ( c >= 'A' && c <= 'Z' ) || c == '.' || c == '_' || c == '-' ; if ( ! validChar ) return false ; } return true ; } group id group id \u6ca1\u6709\u627e\u5230\u9650\u5236 private def isValidGroupId ( groupId : String , api : ApiKeys ): Boolean = { api match { case ApiKeys . OFFSET_COMMIT | ApiKeys . OFFSET_FETCH | ApiKeys . DESCRIBE_GROUPS | ApiKeys . DELETE_GROUPS => // For backwards compatibility, we support the offset commit APIs for the empty groupId, and also // in DescribeGroups and DeleteGroups so that users can view and delete state of all groups. groupId != null case _ => // The remaining APIs are groups using Kafka for group coordination and must have a non-empty groupId groupId != null && ! groupId . isEmpty } }","title":"7.1 topic \u540d\u89c4\u5219"},{"location":"7-other/1-topic-name-limit/#71-topic","text":"","title":"7.1 topic \u540d\u89c4\u5219"},{"location":"7-other/1-topic-name-limit/#topic","text":"topic \u540d \u4e0d\u80fd\u4e3a\u7a7a\u5b57\u7b26\u4e32 topic \u540d \u4e0d\u80fd\u4e3a . \u6216\u8005 .. topic \u540d \u4e0d\u80fd\u8d85\u8fc7 249 \u4e2a\u5b57\u7b26 topic \u540d \u7531 [a-zA-Z0-9._-] \u8fd9\u4e9b\u5b57\u7b26\u7ec4\u6210 public static final String LEGAL_CHARS = \"[a-zA-Z0-9._-]\" ; private static final int MAX_NAME_LENGTH = 249 ; public static void validate ( String topic ) { if ( topic . isEmpty ()) throw new InvalidTopicException ( \"Topic name is illegal, it can't be empty\" ); if ( topic . equals ( \".\" ) || topic . equals ( \"..\" )) throw new InvalidTopicException ( \"Topic name cannot be \\\".\\\" or \\\"..\\\"\" ); if ( topic . length () > MAX_NAME_LENGTH ) throw new InvalidTopicException ( \"Topic name is illegal, it can't be longer than \" + MAX_NAME_LENGTH + \" characters, topic name: \" + topic ); if ( ! containsValidPattern ( topic )) throw new InvalidTopicException ( \"Topic name \\\"\" + topic + \"\\\" is illegal, it contains a character other than \" + \"ASCII alphanumerics, '.', '_' and '-'\" ); } /** * Valid characters for Kafka topics are the ASCII alphanumerics, '.', '_', and '-' */ static boolean containsValidPattern ( String topic ) { for ( int i = 0 ; i < topic . length (); ++ i ) { char c = topic . charAt ( i ); // We don't use Character.isLetterOrDigit(c) because it's slower boolean validChar = ( c >= 'a' && c <= 'z' ) || ( c >= '0' && c <= '9' ) || ( c >= 'A' && c <= 'Z' ) || c == '.' || c == '_' || c == '-' ; if ( ! validChar ) return false ; } return true ; }","title":"topic"},{"location":"7-other/1-topic-name-limit/#group-id","text":"group id \u6ca1\u6709\u627e\u5230\u9650\u5236 private def isValidGroupId ( groupId : String , api : ApiKeys ): Boolean = { api match { case ApiKeys . OFFSET_COMMIT | ApiKeys . OFFSET_FETCH | ApiKeys . DESCRIBE_GROUPS | ApiKeys . DELETE_GROUPS => // For backwards compatibility, we support the offset commit APIs for the empty groupId, and also // in DescribeGroups and DeleteGroups so that users can view and delete state of all groups. groupId != null case _ => // The remaining APIs are groups using Kafka for group coordination and must have a non-empty groupId groupId != null && ! groupId . isEmpty } }","title":"group id"},{"location":"7-other/2-kafka-page-cache/","text":"producer \u751f\u4ea7\u6d88\u606f\u65f6\uff0c\u4f1a\u4f7f\u7528 pwrite() \u7cfb\u7edf\u8c03\u7528(\u5bf9\u5e94 Java NIO FileChannel.write() API)\u6309\u504f\u79fb\u91cf\u5199\u5165\u6570\u636e\uff0c\u5e76\u4e14\u90fd\u4f1a\u5148\u5199\u5165 page cache \u91cc consumer \u6d88\u8d39\u6d88\u606f\u65f6\uff0c\u4f1a\u4f7f\u7528 sendfile() \u7cfb\u7edf\u8c03\u7528(\u5bf9\u5e94 Java NIO FileChannel.transferTo() API)\uff0c \u96f6\u62f7\u8d1d \u5730\u5c06\u6570\u636e\u4ece page cache \u4f20\u8f93\u5230 broker \u7684 socket buffer\uff0c\u518d\u901a\u8fc7\u7f51\u7edc\u4f20\u8f93 follower \u540c\u6b65\u6d88\u606f\uff0c\u4e0e consumer \u540c\u7406 page cache \u4e2d\u7684\u6570\u636e\u4f1a\u968f\u7740\u5185\u6838\u4e2d flusher \u7ebf\u7a0b\u7684\u8c03\u5ea6\u4ee5\u53ca\u5bf9 sync()/fsync() \u7684\u8c03\u7528\u5199\u56de\u5230\u78c1\u76d8\uff0c\u5c31\u7b97\u8fdb\u7a0b\u5d29\u6e83\uff0c\u4e5f\u4e0d\u7528\u62c5\u5fc3\u6570\u636e\u4e22\u5931\u3002 \u53e6\u5916\uff0c\u5982\u679c consumer \u8981\u6d88\u8d39\u7684\u6d88\u606f\u4e0d\u5728 page cache \u91cc\uff0c\u624d\u4f1a\u53bb\u78c1\u76d8\u8bfb\u53d6\uff0c\u5e76\u4e14\u4f1a\u987a\u4fbf\u9884\u8bfb\u51fa\u4e00\u4e9b\u76f8\u90bb\u7684\u5757\u653e\u5165 page cache\uff0c\u4ee5\u65b9\u4fbf\u4e0b\u4e00\u6b21\u8bfb\u53d6\u3002 \u7531\u6b64\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u91cd\u8981\u7684\u7ed3\u8bba\uff1a\u5982\u679c Kafka producer \u7684\u751f\u4ea7\u901f\u7387\u4e0e consumer \u7684\u6d88\u8d39\u901f\u7387\u76f8\u5dee\u4e0d\u5927\uff0c\u90a3\u4e48\u5c31\u80fd\u51e0\u4e4e\u53ea\u9760\u5bf9 broker page cache \u7684\u8bfb\u5199\u5b8c\u6210\u6574\u4e2a\u751f\u4ea7-\u6d88\u8d39\u8fc7\u7a0b\uff0c\u78c1\u76d8\u8bbf\u95ee\u975e\u5e38\u5c11\u3002\u5e76\u4e14 Kafka \u6301\u4e45\u5316\u6d88\u606f\u5230\u5404\u4e2a topic \u7684 partition \u6587\u4ef6\u65f6\uff0c\u662f\u53ea\u8ffd\u52a0\u7684\u987a\u5e8f\u5199\uff0c\u5145\u5206\u5229\u7528\u4e86\u78c1\u76d8\u987a\u5e8f\u8bbf\u95ee\u5feb\u7684\u7279\u6027\uff0c\u6548\u7387\u9ad8\u3002 \u53c2\u8003 https://cloud.tencent.com/developer/article/1488144 https://www.cnblogs.com/xiaolincoding/p/13719610.html","title":"2 kafka page cache"},{"location":"7-other/2-kafka-page-cache/#_1","text":"https://cloud.tencent.com/developer/article/1488144 https://www.cnblogs.com/xiaolincoding/p/13719610.html","title":"\u53c2\u8003"}]}