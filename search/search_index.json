{"config":{"lang":["en","ja"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"kafka design and implement","text":"<p>\u57fa\u4e8e 2.7.1</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/","title":"1 server \u89e3\u6790","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/1-server%20%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/","title":"1.1 server \u542f\u52a8\u8fc7\u7a0b","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/1-server%20%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/#_1","title":"\u542f\u52a8","text":"<p>\u5728\u542f\u52a8\u811a\u672c <code>kafka-server-start.sh</code> \u4e2d\u53ef\u4ee5\u770b\u5230\uff0c\u8fd0\u884c\u7c7b\u4e3a <code>kafka.Kafka</code>\uff0c\u5176 <code>main</code> \u51fd\u6570\u5373\u4e3a\u670d\u52a1\u542f\u52a8\u7684\u5165\u53e3\u3002</p> <p>\u67e5\u770b <code>Kafka</code> \u7684 <code>main</code> \u51fd\u6570\uff0c\u5176\u5185\u5bb9\u4e3a\uff1a</p> <ol> <li>\u89e3\u6790\u53c2\u6570\uff0c\u7b2c\u4e00\u4e2a\u53c2\u6570\u4e3a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff0c\u4ece\u914d\u7f6e\u6587\u4ef6\u52a0\u8f7d\u670d\u52a1\u914d\u7f6e\u3002\u5176\u4ed6\u53c2\u6570\u53ef\u9009\uff0c\u6307\u5b9a\u670d\u52a1\u914d\u7f6e\u3002</li> <li>\u6784\u5efa <code>KafkaServer</code> (2.8.0 \u4e4b\u540e\u652f\u6301 <code>KafkaRaftServer</code>)</li> <li>\u5bf9\u6784\u9020\u7684 <code>KafkaServer</code> \u5bf9\u8c61\u8c03\u7528 <code>startup()</code> \u65b9\u6cd5</li> </ol> <p>\u67e5\u770b KafkaServer \u4ee3\u7801\uff0c\u5176 <code>startup</code> \u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ol> <li>initZkClient<ol> <li>createZkClient</li> </ol> </li> <li>get or create cluster_id</li> <li>load metadata</li> <li>check cluster id</li> <li>generate brokerId</li> <li>initialize dynamic borker configs from ZooKeeper</li> <li>start scheduler</li> <li>create and configure metrics</li> </ol>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","title":"1.2 server \u7f51\u7edc\u6a21\u578b","text":"<p>\u4ece\u4e0a\u4e00\u8282\u6211\u4eec\u5f97\u77e5 kafka \u670d\u52a1\u542f\u52a8\u7684\u52a8\u4f5c\u5bf9\u5e94\u7684\u4ee3\u7801\u662f <code>KafkaServer</code> \u7684 <code>startup()</code> \u51fd\u6570\uff0c\u5728 <code>startup()</code> \u51fd\u6570\u4e2d\uff0c \u521b\u5efa\u5e76\u542f\u52a8\u4e86\u4e00\u7cfb\u5217\u7ec4\u4ef6\uff0c\u5176\u4e2d\u5305\u62ec <code>SocketServer</code>\u3002</p> <pre><code>// Create and start the socket server acceptor threads so that the bound port is known.\n// Delay starting processors until the end of the initialization sequence to ensure\n// that credentials have been loaded before processing authentications.\n//\n// Note that we allow the use of KRaft mode controller APIs when forwarding is enabled\n// so that the Envelope request is exposed. This is only used in testing currently.\nsocketServer = new SocketServer(config, metrics, time, credentialProvider, apiVersionManager)\nsocketServer.startup(startProcessingRequests = false)\n</code></pre> <p>\u8fd9\u4e00\u8282\u89e3\u6790 server \u7aef\u7684\u7f51\u7edc\u6a21\u578b\uff0c\u5373 server \u7aef\u63a5\u6536\u8bf7\u6c42\uff0c\u5904\u7406\u5e76\u8fd4\u56de\u54cd\u5e94\u7684\u8fc7\u7a0b\u3002</p> <p>\u5bf9\u8bf7\u6c42\u7684\u76d1\u542c\u6b63\u662f\u901a\u8fc7 <code>SocketServer</code> \u7ec4\u4ef6\u5b8c\u6210\u3002</p> <p></p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#socketserver","title":"SocketServer","text":"<p><code>SocketServer</code> \u652f\u6301 2 \u79cd\u7c7b\u578b\u7684\u8bf7\u6c42\uff1a</p> <ol> <li>\u6570\u636e(data plane)\uff1a<ul> <li>\u5904\u7406\u6765\u81ea client \u6216\u8005\u540c\u96c6\u7fa4\u5176\u4ed6 broker \u7684\u8bf7\u6c42</li> <li>\u7ebf\u7a0b\u6a21\u578b\u662f\uff1a<ul> <li>\u53ef\u4ee5\u914d\u7f6e\u591a\u4e2a listener\uff0c\u6bcf\u4e2a listener \u5bf9\u5e94 1 \u4e2a Acceptor \u7ebf\u7a0b\uff0c\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42</li> <li>Acceptor \u6709 N \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea sockets \u7684\u8bf7\u6c42(<code>num.network.threads</code>)</li> <li>M \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b(<code>num.io.threads</code>)</li> </ul> </li> </ul> </li> <li>\u63a7\u5236(control plane)\uff1a<ul> <li>\u5904\u7406\u6765\u81ea controller \u7684\u8bf7\u6c42\u3002\u8fd9\u662f\u53ef\u9009\u7684\u5e76\u4e14\u901a\u8fc7\u6307\u5b9a <code>control.plane.listener.name</code> \u8fdb\u884c\u914d\u7f6e\u3002\u5982\u679c\u6ca1\u6709\u914d\u7f6e\uff0ccontroller \u7684\u8bf7\u6c42\u4e5f\u4f1a\u50cf\u6570\u636e\u8bf7\u6c42\u4e00\u6837\u901a\u8fc7 listener \u5904\u7406</li> <li>\u7ebf\u7a0b\u6a21\u578b\u662f\uff1a<ul> <li>1 \u4e2a Acceptor \u7ebf\u7a0b\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42</li> <li>Acceptor \u6709 1 \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea socket \u7684\u8bf7\u6c42</li> <li>1 \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b</li> </ul> </li> </ul> </li> </ol> <p>\u67e5\u770b SocketServer \u4ee3\u7801\uff0c</p> <p><code>SocketServer</code> \u6784\u9020\u65f6\u4f1a\u521d\u59cb\u4ee5\u4e0b\u5173\u952e\u5c5e\u6027\uff1a</p> <pre><code>// data-plane\nprivate val dataPlaneProcessors = new ConcurrentHashMap[Int, Processor]()\nprivate[network] val dataPlaneAcceptors = new ConcurrentHashMap[EndPoint, Acceptor]()\nval dataPlaneRequestChannel = new RequestChannel(maxQueuedRequests, DataPlaneMetricPrefix, time)\n// control-plane\nprivate var controlPlaneProcessorOpt : Option[Processor] = None\nprivate[network] var controlPlaneAcceptorOpt : Option[Acceptor] = None\nval controlPlaneRequestChannelOpt: Option[RequestChannel] = config.controlPlaneListenerName.map(_ =&gt;\n  new RequestChannel(20, ControlPlaneMetricPrefix, time))\n</code></pre> <p><code>SocketServer</code> \u7684 <code>startup()</code> \u51fd\u6570\u4e2d\u4e3b\u8981\u52a8\u4f5c\u4e3a\uff1a</p> <pre><code>this.synchronized {\n  connectionQuotas = new ConnectionQuotas(config, time, metrics)\n  // 1. \u521b\u5efa\u5904\u7406 controller \u8bf7\u6c42\u7684 Acceptor, Processor\n  createControlPlaneAcceptorAndProcessor(config.controlPlaneListener)\n  // 2. \u521b\u5efa\u5904\u7406\u6570\u636e\u8bf7\u6c42\u7684 Acceptor, Processor\n  createDataPlaneAcceptorsAndProcessors(config.numNetworkThreads, config.dataPlaneListeners)\n  if (startProcessingRequests) {\n    // 3. \u542f\u52a8\u7ebf\u7a0b kafkaThread\uff0c\u540e\u53f0\u8fd0\u884c Acceptor \u4e0e Processor\n    this.startProcessingRequests()\n  }\n}\n</code></pre> <p>\u4e3b\u8981\u6709 3 \u6b65\uff1a</p> <ol> <li>\u521b\u5efa\u5904\u7406 controller \u8bf7\u6c42\u7684 Acceptor, Processor</li> <li>\u521b\u5efa\u5904\u7406\u6570\u636e\u8bf7\u6c42\u7684 Acceptor, Processor</li> <li>\u542f\u52a8\u7ebf\u7a0b kafkaThread\uff0c\u540e\u53f0\u8fd0\u884c Acceptor \u4e0e Processor</li> </ol> <pre><code>private def createDataPlaneAcceptorsAndProcessors(dataProcessorsPerListener: Int,\n                                                  endpoints: Seq[EndPoint]): Unit = {\n  // \u5bf9\u6bcf\u4e2a endpoint\n  endpoints.foreach { endpoint =&gt;\n    connectionQuotas.addListener(config, endpoint.listenerName)\n    // \u521b\u5efa Acceptor\n    val dataPlaneAcceptor = createAcceptor(endpoint, DataPlaneMetricPrefix)\n    // 1. \u521b\u5efa dataProcessorsPerListener \u4e2a Processor\n    // 2. \u5c06 Processor \u52a0\u5165\u5230 dataPlaneRequestChannel \u7684 processors \u4e2d\n    // 3. \u5c06 Processor \u52a0\u5165\u5230 SocketServer \u7684 dataPlaneProcessors \u4e2d\n    // 4. \u5c06 Processor \u52a0\u5165\u5230 Acceptor \u7684 processors \u4e2d\n    addDataPlaneProcessors(dataPlaneAcceptor, endpoint, dataProcessorsPerListener)\n    // \u5c06 Acceptor \u52a0\u5165\u5230 SocketServer \u7684 dataPlaneAcceptors \u4e2d\n    dataPlaneAcceptors.put(endpoint, dataPlaneAcceptor)\n    info(s\"Created data-plane acceptor and processors for endpoint : ${endpoint.listenerName}\")\n  }\n}\n</code></pre> <pre><code>private def createControlPlaneAcceptorAndProcessor(endpointOpt: Option[EndPoint]): Unit = {\n  endpointOpt.foreach { endpoint =&gt;\n    connectionQuotas.addListener(config, endpoint.listenerName)\n    val controlPlaneAcceptor = createAcceptor(endpoint, ControlPlaneMetricPrefix)\n    val controlPlaneProcessor = newProcessor(nextProcessorId, controlPlaneRequestChannelOpt.get, connectionQuotas, endpoint.listenerName, endpoint.securityProtocol, memoryPool)\n    controlPlaneAcceptorOpt = Some(controlPlaneAcceptor)\n    controlPlaneProcessorOpt = Some(controlPlaneProcessor)\n    val listenerProcessors = new ArrayBuffer[Processor]()\n    listenerProcessors += controlPlaneProcessor\n    controlPlaneRequestChannelOpt.foreach(_.addProcessor(controlPlaneProcessor))\n    nextProcessorId += 1\n    controlPlaneAcceptor.addProcessors(listenerProcessors, ControlPlaneThreadPrefix)\n    info(s\"Created control-plane acceptor and processor for endpoint : ${endpoint.listenerName}\")\n  }\n}\n</code></pre> <p>\u8fd9\u91cc\u505a\u7684\u5de5\u4f5c\u4e3a\uff1a</p> <ol> <li>\u4e3a\u6bcf\u4e2a endpoint \u521b\u5efa 1 \u4e2a <code>Acceptor</code>\uff0c<code>Acceptor</code> \u8d1f\u8d23\u76d1\u542c\u8bf7\u6c42</li> <li>\u5bf9\u6bcf\u4e2a endpoint \u521b\u5efa N (<code>num.network.threads</code>) \u4e2a <code>Processor</code>\uff0c\u540c\u65f6\uff1a<ol> <li>\u5c06 <code>Processor</code> \u8bb0\u5f55\u5230 <code>RequestChannel</code> \u7684 <code>processors</code> hash map \u4e2d</li> <li>\u5c06 <code>Processor</code> \u8bb0\u5f55\u5230 <code>SocketServer</code> \u7684 <code>dataPlaneProcessors</code> hash map \u4e2d</li> <li>\u5c06 <code>Processor</code> \u52a0\u5165\u5230 <code>Acceptor</code> \u7684 <code>processors</code> \u6570\u7ec4\u4e2d</li> </ol> </li> <li>\u5c06 <code>Acceptor</code> \u52a0\u5165\u5230 <code>SocketServer</code> \u7684 <code>dataPlaneAcceptors</code> hash map \u4e2d</li> </ol> <p>\u8fd9\u91cc\u5b8c\u6210\u4e4b\u540e\uff0c\u5904\u7406\u8bf7\u6c42\u6240\u9700\u7684 <code>SocketServer</code>, <code>Acceptor</code>, <code>Processor</code>, <code>RequestChannel</code> \u5c31\u90fd\u521b\u5efa\u5b8c\u6210\u4e86\uff0c\u4e0b\u9762\u4e3b\u8981\u770b\u4e00\u4e0b <code>Acceptor</code>, <code>Processor</code>, <code>RequestChannel</code> \u5404\u81ea\u7684\u4f5c\u7528\u3002</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#acceptor","title":"Acceptor","text":"<p>Acceptor</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#processor","title":"Processor","text":"<p>Processor</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#requestchannel","title":"RequestChannel","text":"<p>RequestChannel</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#kafkarequesthandler","title":"KafkaRequestHandler","text":"<p>KafkaRequestHandler</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#_1","title":"\u76d1\u63a7","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#requesthandleravgidlepercent","title":"requestHandlerAvgIdlePercent","text":"<p><code>Meter</code> \u7c7b\u578b\uff0c\u5728 <code>KafkaRequestHandlerPool</code> \u4e2d\u521b\u5efa\uff0c\u521b\u5efa\u6bcf\u4e2a <code>KafkaRequestHandler</code> \u65f6\u4f20\u5165</p> <pre><code>/* a meter to track the average free capacity of the request handlers */\nprivate val aggregateIdleMeter = newMeter(requestHandlerAvgIdleMetricName, \"percent\", TimeUnit.NANOSECONDS)\n\n\nval runnables = new mutable.ArrayBuffer[KafkaRequestHandler](numThreads)\nfor (i &lt;- 0 until numThreads) {\n  createHandler(i)\n}\n\ndef createHandler(id: Int): Unit = synchronized {\n  runnables += new KafkaRequestHandler(id, brokerId, aggregateIdleMeter, threadPoolSize, requestChannel, apis, time)\n  KafkaThread.daemon(logAndThreadNamePrefix + \"-kafka-request-handler-\" + id, runnables(id)).start()\n}\n</code></pre> <p><code>KafkaRequestHandler</code> \u4f5c\u4e3a\u540e\u53f0\u7ebf\u7a0b\u542f\u52a8\uff0c\u542f\u52a8\u540e\u4e0d\u540c\u4ece <code>RequestChannel</code> \u4e2d\u83b7\u53d6\u8bf7\u6c42\uff0c<code>Meter.mark()</code> \u65f6\uff0c\u4ee5\u83b7\u53d6\u8bf7\u6c42\u7684\u8017\u65f6\u9664\u4ee5\u7ebf\u7a0b\u603b\u6570\u3002</p> <pre><code>  def run(): Unit = {\n    while (!stopped) {\n      // We use a single meter for aggregate idle percentage for the thread pool.\n      // Since meter is calculated as total_recorded_value / time_window and\n      // time_window is independent of the number of threads, each recorded idle\n      // time should be discounted by # threads.\n      val startSelectTime = time.nanoseconds\n\n      val req = requestChannel.receiveRequest(300)\n      val endTime = time.nanoseconds\n      val idleTime = endTime - startSelectTime\n\n      // NOTE: \u4ece RequestChannel \u4e2d\u83b7\u53d6\u8bf7\u6c42\u7684\u65f6\u95f4\uff0c\u662f KafkaRequestHandler \u7a7a\u95f2\u7684\u65f6\u95f4\n      aggregateIdleMeter.mark(idleTime / totalHandlerThreads.get)\n\n      // NOTE: \u5904\u7406\u8bf7\u6c42\u7684\u65f6\u95f4\u4e3a KafkaRequestHandler \u975e\u7a7a\u95f2\u7684\u65f6\u95f4\n      req match {\n        case RequestChannel.ShutdownRequest =&gt;\n          debug(s\"Kafka request handler $id on broker $brokerId received shut down command\")\n          shutdownComplete.countDown()\n          return\n\n        case request: RequestChannel.Request =&gt;\n          try {\n            request.requestDequeueTimeNanos = endTime\n            trace(s\"Kafka request handler $id on broker $brokerId handling request $request\")\n            apis.handle(request)\n          } catch {\n            case e: FatalExitError =&gt;\n              shutdownComplete.countDown()\n              Exit.exit(e.statusCode)\n            case e: Throwable =&gt; error(\"Exception when handling request\", e)\n          } finally {\n            request.releaseBuffer()\n          }\n\n        case null =&gt; // continue\n      }\n    }\n    shutdownComplete.countDown()\n  }\n</code></pre> <p>requestHandler \u7ebf\u7a0b\u4e0d\u65ad\u5faa\u73af\u505a 2 \u4ef6\u4e8b\uff1a</p> <ol> <li>\u4ece  channel \u4e2d\u83b7\u53d6\u5f85\u5904\u7406\u8bf7\u6c42</li> <li>\u5904\u7406\u8bf7\u6c42\uff08\u8bfb/\u5199\u8bf7\u6c42/\u5176\u4ed6\uff09</li> </ol> <p>\u5047\u8bbe\u6bcf\u6b21\u5faa\u73af\uff0c\u7b2c 1 \u4ef6\u4e8b\u8017\u65f6 A\uff0c\u7b2c 2 \u4ef6\u4e8b\u8017\u65f6 B</p> <p>\u7a7a\u95f2\u7387 = A / (A + B)</p> <p>\u6240\u4ee5\u7a7a\u95f2\u7387\u589e\u52a0\uff0c\u53ef\u80fd\u7684\u539f\u56e0\u662f\u8bf7\u6c42\u6570\u51cf\u5c11\uff0c\u5bfc\u81f4 A \u589e\u52a0\uff1b\u6216\u8005\u8bfb/\u5199\u78c1\u76d8\u66f4\u5feb\uff0c\u5904\u7406\u8bf7\u6c42\u8017\u65f6\u51cf\u5c11</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/2-server%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/#networkprocessoravgidlepercent","title":"NetworkProcessorAvgIdlePercent","text":"<p><code>Gauge</code> \u7c7b\u578b\uff0c\u5728 <code>SocketServer.startup()</code> \u65f6\u5b9a\u4e49</p> <pre><code>    newGauge(s\"${DataPlaneMetricPrefix}NetworkProcessorAvgIdlePercent\", () =&gt; SocketServer.this.synchronized {\n      val ioWaitRatioMetricNames = dataPlaneProcessors.values.asScala.iterator.map { p =&gt;\n        metrics.metricName(\"io-wait-ratio\", MetricsGroup, p.metricTags)\n      }\n      ioWaitRatioMetricNames.map { metricName =&gt;\n        Option(metrics.metric(metricName)).fold(0.0)(m =&gt; Math.min(m.metricValue.asInstanceOf[Double], 1.0))\n      }.sum / dataPlaneProcessors.size\n    })\n</code></pre> <p>\u8ba1\u7b97\u65b9\u6cd5\u662f\u53d6\u5f97\u6bcf\u4e2a networkProcessor \u7684 io-wait-ratio\uff0c\u9664\u4ee5 networkProcessor \u603b\u6570\u3002io-wait \u7684\u6bd4\u4f8b\uff0c\u5373\u662f networkProcessor \u7a7a\u95f2\u7684\u5360\u6bd4\u3002</p> <p>io-wait-ratio \u6307\u6807\u5728 <code>Selector</code> \u4e2d\u8ba1\u7b97\uff0c<code>Meter</code> \u7c7b\u578b</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/","title":"\u914d\u7f6e","text":"<p>Kafka \u670d\u52a1\u542f\u52a8\u65f6\uff0c\u5728 <code>main()</code> \u65b9\u6cd5\u4e2d\u901a\u8fc7 <code>getPropsFromArgs()</code> \u83b7\u53d6\u547d\u4ee4\u884c\u53c2\u6570\uff0c\u5e76\u8bfb\u53d6\u6307\u5b9a\u7684 <code>server.properties</code> \u6587\u4ef6\uff0c\u83b7\u53d6\u670d\u52a1\u7684\u914d\u7f6e\u53c2\u6570\u3002</p> <pre><code>  def main(args: Array[String]): Unit = {\n    try {\n      val serverProps = getPropsFromArgs(args)\n      val kafkaServerStartable = KafkaServerStartable.fromProps(serverProps)\n      // ...\n    }\n    catch {\n    }\n  }\n</code></pre> <pre><code>  def getPropsFromArgs(args: Array[String]): Properties = {\n    if (args.length == 0 || args.contains(\"--help\")) {\n      CommandLineUtils.printUsageAndDie(optionParser, \"USAGE: java [options] %s server.properties [--override property=value]*\".format(classOf[KafkaServer].getSimpleName()))\n    }\n\n    if (args.contains(\"--version\")) {\n      CommandLineUtils.printVersionAndDie()\n    }\n\n    // NOTE: args(0) \u5c31\u662f\u542f\u52a8\u547d\u4ee4\u6307\u5b9a\u7684 server.properties \u6587\u4ef6\n    val props = Utils.loadProps(args(0))\n\n    if (args.length &gt; 1) {\n    }\n    props\n  }\n</code></pre> <p>\u5728 <code>KafkaServerStartable.fromProps()</code> \u4e2d\uff0c\u4f1a\u8fdb\u4e00\u6b65\u628a props \u89e3\u6790\u4e3a <code>KafkaConfig</code> \u5b9e\u4f8b</p> <pre><code>object KafkaServerStartable {\n  def fromProps(serverProps: Properties): KafkaServerStartable = {\n    fromProps(serverProps, None)\n  }\n\n  def fromProps(serverProps: Properties, threadNamePrefix: Option[String]): KafkaServerStartable = {\n    val reporters = KafkaMetricsReporter.startReporters(new VerifiableProperties(serverProps))\n    // NOTE: KafkaConfig.fromProps \u65b9\u6cd5\u4f1a\u6839\u636e serverProps \u6784\u5efa KafkaConfig \u5b9e\u4f8b\n    new KafkaServerStartable(KafkaConfig.fromProps(serverProps, false), reporters, threadNamePrefix)\n  }\n}\n</code></pre> <p><code>KafkaConfig</code> <code>fromProps</code> \u65b9\u6cd5</p> <pre><code>  def fromProps(props: Properties, doLog: Boolean): KafkaConfig =\n    new KafkaConfig(props, doLog)\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/1-KafkaConfig/","title":"KafkaConfig","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/1-KafkaConfig/#_1","title":"\u914d\u7f6e\u5b9a\u4e49","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/1-KafkaConfig/#kafkaconfig-object","title":"KafkaConfig Object","text":"<p><code>KafkaConfig</code> object \u901a\u8fc7 <code>configDef</code> \u5b9a\u4e49\u4e86\u6240\u6709\u914d\u7f6e\u53c2\u6570\u7684\u540d\u79f0\u3001\u7c7b\u578b\u3001\u9ed8\u8ba4\u503c\u7b49\u5c5e\u6027\u3002</p> <pre><code>  private val configDef = {\n    import ConfigDef.Importance._\n    import ConfigDef.Range._\n    import ConfigDef.Type._\n    import ConfigDef.ValidString._\n\n    new ConfigDef()\n\n      /** ********* Zookeeper Configuration ***********/\n      .define(ZkConnectProp, STRING, HIGH, ZkConnectDoc)\n      .define(ZkSessionTimeoutMsProp, INT, Defaults.ZkSessionTimeoutMs, HIGH, ZkSessionTimeoutMsDoc)\n      .define(ZkConnectionTimeoutMsProp, INT, null, HIGH, ZkConnectionTimeoutMsDoc)\n      .define(ZkSyncTimeMsProp, INT, Defaults.ZkSyncTimeMs, LOW, ZkSyncTimeMsDoc)\n\n      ...\n      ...\n\n  }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/1-KafkaConfig/#configdefconfigkey","title":"ConfigDef/ConfigKey","text":"<p><code>configDef</code> \u662f\u4e00\u4e2a <code>ConfigDef</code> \u5b9e\u4f8b\uff0c<code>ConfigDef</code> \u5b9e\u4f8b\u7ef4\u62a4\u6240\u6709\u53c2\u6570\u5b9a\u4e49\u5230 <code>configKeys</code> \u4e2d\uff0c\u901a\u8fc7 <code>define()</code> \u65b9\u6cd5\u6dfb\u52a0\u3002</p> <pre><code>    private final Map&lt;String, ConfigKey&gt; configKeys;\n    private final List&lt;String&gt; groups;\n    private Set&lt;String&gt; configsWithNoParent;\n</code></pre> <p><code>configKeys</code> \u7531 <code>ConfigKey</code> \u5b9e\u4f8b\u7ec4\u6210\uff0c\u53ef\u4ee5\u5b9a\u4e49\u4e00\u4e2a\u914d\u7f6e\u7684\u540d\u5b57\u3001\u7c7b\u578b\u3001\u9ed8\u8ba4\u503c\u3001\u6587\u6863\u7b49\u76f8\u5173\u5c5e\u6027\u3002</p> <pre><code>        public final String name;\n        public final Type type;\n        public final String documentation;\n        public final Object defaultValue;\n        public final Validator validator;\n        public final Importance importance;\n        public final String group;\n        public final int orderInGroup;\n        public final Width width;\n        public final String displayName;\n        public final List&lt;String&gt; dependents;\n        public final Recommender recommender;\n        public final boolean internalConfig;\n</code></pre> <p>ConfigDef \u63d0\u4f9b <code>parse()</code> \u65b9\u6cd5\uff0c\u5c06 <code>props</code> \u6309\u7167 <code>configKeys</code> \u7684\u5b9a\u4e49\u89e3\u6790\u4e3a\u5bf9\u5e94\u7684 <code>Map&lt;String, Object&gt;</code> \u7c7b\u578b\u3002</p> <pre><code>    public Map&lt;String, Object&gt; parse(Map&lt;?, ?&gt; props) {\n        // Check all configurations are defined\n        List&lt;String&gt; undefinedConfigKeys = undefinedDependentConfigs();\n        if (!undefinedConfigKeys.isEmpty()) {\n            String joined = Utils.join(undefinedConfigKeys, \",\");\n            throw new ConfigException(\"Some configurations in are referred in the dependents, but not defined: \" + joined);\n        }\n        // parse all known keys\n        Map&lt;String, Object&gt; values = new HashMap&lt;&gt;();\n        for (ConfigKey key : configKeys.values())\n            values.put(key.name, parseValue(key, props.get(key.name), props.containsKey(key.name)));\n        return values;\n    }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/1-KafkaConfig/#_2","title":"\u914d\u7f6e\u8d4b\u503c","text":"<p><code>KafkaConfig</code> class \u7ee7\u627f\u81ea <code>AbstractConfig</code>\uff0c</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/1-KafkaConfig/#abstractconfig","title":"AbstractConfig","text":"<p><code>AbstractConfig</code> \u7c7b\u6709\u4ee5\u4e0b\u5c5e\u6027\uff1a</p> <pre><code>    /* configs for which values have been requested, used to detect unused configs */\n    private final Set&lt;String&gt; used;\n\n    /* the original values passed in by the user */\n    // \u539f\u59cb\u7684 props \u503c\n    private final Map&lt;String, ?&gt; originals;\n\n    /* the parsed values */\n    // \u7ecf\u8fc7 definition.parse(originals) \u540e\u5f97\u5230\u7684\u503c\n    private final Map&lt;String, Object&gt; values;\n\n    // \u914d\u7f6e\u5b9a\u4e49\uff1aKafkaConfig.configDef\n    private final ConfigDef definition;\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/1-KafkaConfig/#kafkaconfig-class","title":"KafkaConfig Class","text":"<p><code>KafkaConfig</code> class \u7ee7\u627f\u81ea <code>AbstractConfig</code>\uff0c\u5b9a\u4e49\u4e86\u6240\u6709\u914d\u7f6e\u5e76\u8d4b\u503c\u3002</p> <p><code>KafkaConfig</code> class \u8fd8\u5b9a\u4e49\u4e86 <code>dynamicConfig</code>\uff0c\u5176\u662f <code>DynamicBrokerConfig</code> \u5b9e\u4f8b</p> <pre><code>  private[server] val dynamicConfig = dynamicConfigOverride.getOrElse(new DynamicBrokerConfig(this))\n\n  def addReconfigurable(reconfigurable: Reconfigurable): Unit = {\n    dynamicConfig.addReconfigurable(reconfigurable)\n  }\n\n  def removeReconfigurable(reconfigurable: Reconfigurable): Unit = {\n    dynamicConfig.removeReconfigurable(reconfigurable)\n  }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/2-Dynamic-Config/","title":"\u52a8\u6001\u914d\u7f6e","text":"<p>\u914d\u7f6e\u5b58\u50a8\u5728 zk \u4e2d\uff0c\u5206 4 \u79cd\u7c7b\u578b\uff1a</p> <ul> <li>topics</li> <li>clients</li> <li>users</li> <li>brokers</li> </ul>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/2-Dynamic-Config/#_2","title":"\u914d\u7f6e\u5b9a\u4e49","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/2-Dynamic-Config/#dynamicbrokerconfig","title":"DynamicBrokerConfig","text":"<p>\u5c5e\u6027\uff1a</p> <pre><code>  // \u8bb0\u5f55\u5f53\u524d broker \u52a8\u6001\u914d\u7f6e\n  private val dynamicBrokerConfigs = mutable.Map[String, String]()\n\n  // \u8bb0\u5f55 &lt;default&gt; \u5373\u96c6\u7fa4\u7ea7\u522b\u52a8\u6001\u914d\u7f6e\n  private val dynamicDefaultConfigs = mutable.Map[String, String]()\n\n  // \u8bb0\u5f55\u6240\u6709\u53ef\u4ee5\u91cd\u65b0\u914d\u7f6e\u7684\u5bf9\u8c61\n  private val reconfigurables = mutable.Buffer[Reconfigurable]()\n\n  // \u8bb0\u5f55\u6240\u6709\u53ef\u4ee5\u91cd\u65b0\u914d\u7f6e\u7684 broker \u5bf9\u8c61\n  private val brokerReconfigurables = mutable.Buffer[BrokerReconfigurable]()\n\n  // \u5f53\u524d\u7684 kafkaConfig\n  private var currentConfig = kafkaConfig\n</code></pre> <p>\u8bb0\u5f55\u6240\u6709\u53ef\u4ee5\u91cd\u65b0\u914d\u7f6e\u7684\u5bf9\u8c61</p> <pre><code>  /**\n   * Add reconfigurables to be notified when a dynamic broker config is updated.\n   *\n   * `Reconfigurable` is the public API used by configurable plugins like metrics reporter\n   * and quota callbacks. These are reconfigured before `KafkaConfig` is updated so that\n   * the update can be aborted if `reconfigure()` fails with an exception.\n   *\n   * `BrokerReconfigurable` is used for internal reconfigurable classes. These are\n   * reconfigured after `KafkaConfig` is updated so that they can access `KafkaConfig`\n   * directly. They are provided both old and new configs.\n   */\n  def addReconfigurables(kafkaServer: KafkaServer): Unit = {\n    kafkaServer.authorizer match {\n      case Some(authz: Reconfigurable) =&gt; addReconfigurable(authz)\n      case _ =&gt;\n    }\n    addReconfigurable(kafkaServer.kafkaYammerMetrics)\n    addReconfigurable(new DynamicMetricsReporters(kafkaConfig.brokerId, kafkaServer))\n    addReconfigurable(new DynamicClientQuotaCallback(kafkaConfig.brokerId, kafkaServer))\n\n    addBrokerReconfigurable(new DynamicThreadPool(kafkaServer))\n    if (kafkaServer.logManager.cleaner != null)\n      addBrokerReconfigurable(kafkaServer.logManager.cleaner)\n    addBrokerReconfigurable(new DynamicLogConfig(kafkaServer.logManager, kafkaServer))\n    addBrokerReconfigurable(new DynamicListenerConfig(kafkaServer))\n    addBrokerReconfigurable(kafkaServer.socketServer)\n  }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/2-Dynamic-Config/#_3","title":"\u914d\u7f6e\u521d\u59cb\u5316","text":"<p>\u5728 <code>KafkaServer.startup()</code> \u65b9\u6cd5\u4e2d\u8c03\u7528 <code>DynamicBrokerConfig.initialize()</code> \u65b9\u6cd5\u521d\u59cb\u5316</p> <pre><code>        // NOTE: \u8fd9\u91cc initialize\uff0c\u4f46\u662f reconfigurables \u662f\u7a7a\u7684\uff0c\u6240\u4ee5\u8fd9\u91cc\u53ea\u4f1a\u66f4\u65b0 dynamicConfig \u7684 currentConfig\uff0c\n        // \u4f46\u662f\u5b9e\u9645\u7684 reconfigurable \u5bf9\u8c61\u5e76\u4e0d\u4f1a\u91cd\u65b0\u914d\u7f6e\n        // \u4f46\u662f\u4f1a\u66f4\u65b0 kafkaConfig \u7684 currentConfig\n        // initialize dynamic broker configs from ZooKeeper. Any updates made after this will be\n        // applied after DynamicConfigManager starts.\n        config.dynamicConfig.initialize(zkClient)\n</code></pre> <ol> <li>\u8fd9\u91cc\u6267\u884c <code>DynamicBrokerConfig.initialize()</code> \u65b9\u6cd5\uff0c\u4f46\u662f reconfigurables \u662f\u7a7a\u7684\uff0c\u6240\u4ee5\u8fd9\u91cc\u53ea\u4f1a\u66f4\u65b0 dynamicConfig \u7684 currentConfig\uff0c\u4f46\u662f\u5b9e\u9645\u7684 reconfigurable \u5bf9\u8c61\u5e76\u4e0d\u4f1a\u91cd\u65b0\u914d\u7f6e</li> <li>\u56e0\u4e3a\u66f4\u65b0\u4e86\u81ea\u8eab\u7684 <code>currentConfig</code>\uff0c\u52a8\u6001\u7684\u914d\u7f6e\u503c\u83b7\u53d6\u65b9\u5f0f\u53ef\u4ee5\u4ece <code>currentConfig</code> \u4e2d\u52a8\u6001\u83b7\u53d6</li> </ol> <pre><code>  private[server] def initialize(zkClient: KafkaZkClient): Unit = {\n    currentConfig = new KafkaConfig(kafkaConfig.props, false, None)\n    val adminZkClient = new AdminZkClient(zkClient)\n    updateDefaultConfig(adminZkClient.fetchEntityConfig(ConfigType.Broker, ConfigEntityName.Default))\n    val props = adminZkClient.fetchEntityConfig(ConfigType.Broker, kafkaConfig.brokerId.toString)\n    val brokerConfig = maybeReEncodePasswords(props, adminZkClient)\n    updateBrokerConfig(kafkaConfig.brokerId, brokerConfig)\n  }\n\n\n  private[server] def updateBrokerConfig(brokerId: Int, persistentProps: Properties): Unit = CoreUtils.inWriteLock(lock) {\n    try {\n      val props = fromPersistentProps(persistentProps, perBrokerConfig = true)\n      dynamicBrokerConfigs.clear()\n      dynamicBrokerConfigs ++= props.asScala\n      updateCurrentConfig()\n    } catch {\n      case e: Exception =&gt; error(s\"Per-broker configs of $brokerId could not be applied: $persistentProps\", e)\n    }\n  }\n\n  private[server] def updateDefaultConfig(persistentProps: Properties): Unit = CoreUtils.inWriteLock(lock) {\n    try {\n      val props = fromPersistentProps(persistentProps, perBrokerConfig = false)\n      dynamicDefaultConfigs.clear()\n      dynamicDefaultConfigs ++= props.asScala\n      updateCurrentConfig()\n    } catch {\n      case e: Exception =&gt; error(s\"Cluster default configs could not be applied: $persistentProps\", e)\n    }\n  }\n\n  private def updateCurrentConfig(): Unit = {\n    val newProps = mutable.Map[String, String]()\n    newProps ++= staticBrokerConfigs\n    overrideProps(newProps, dynamicDefaultConfigs)\n    overrideProps(newProps, dynamicBrokerConfigs)\n    val oldConfig = currentConfig\n    val (newConfig, brokerReconfigurablesToUpdate) = processReconfiguration(newProps, validateOnly = false)\n\n    if (newConfig ne currentConfig) {\n      currentConfig = newConfig\n      kafkaConfig.updateCurrentConfig(newConfig)\n\n      // Process BrokerReconfigurable updates after current config is updated\n      brokerReconfigurablesToUpdate.foreach(_.reconfigure(oldConfig, newConfig))\n    }\n  }\n</code></pre> <p><code>KafkaConfig</code> \u7684 <code>currentConfig</code> \u66f4\u65b0\u540e\uff0c\u81ea\u8eab\u7684 <code>originals</code>, <code>values</code> \u7b49\u503c\u90fd\u4ece <code>currentConfig</code> \u4e2d\u83b7\u53d6</p> <pre><code>  private[server] def updateCurrentConfig(newConfig: KafkaConfig): Unit = {\n    this.currentConfig = newConfig\n  }\n\n  override def originals: util.Map[String, AnyRef] =\n    if (this eq currentConfig) super.originals else currentConfig.originals\n  override def values: util.Map[String, _] =\n    if (this eq currentConfig) super.values else currentConfig.values\n  override def originalsStrings: util.Map[String, String] =\n    if (this eq currentConfig) super.originalsStrings else currentConfig.originalsStrings\n  override def originalsWithPrefix(prefix: String): util.Map[String, AnyRef] =\n    if (this eq currentConfig) super.originalsWithPrefix(prefix) else currentConfig.originalsWithPrefix(prefix)\n  override def valuesWithPrefixOverride(prefix: String): util.Map[String, AnyRef] =\n    if (this eq currentConfig) super.valuesWithPrefixOverride(prefix) else currentConfig.valuesWithPrefixOverride(prefix)\n  override def get(key: String): AnyRef =\n    if (this eq currentConfig) super.get(key) else currentConfig.get(key)\n</code></pre> <p>\u9759\u6001\u914d\u7f6e\u7528 <code>val</code> \u5b9a\u4e49\u4e3a\u5e38\u91cf\uff0c<code>KafkaConfig</code> \u5b9e\u4f8b\u5316\u65f6\u521d\u59cb\u4e00\u6b21\u3002\u52a8\u6001\u914d\u7f6e\u7528 <code>def</code> \u5b9a\u4e49\u4e3a\u65b9\u6cd5\uff0c\u6bcf\u6b21\u83b7\u53d6\u90fd\u6267\u884c\u4e00\u6b21\uff0c\u8fd4\u56de\u7684\u503c\u53ef\u4ee5\u968f\u7740 <code>currentConfig</code> \u7684\u66f4\u65b0\u800c\u66f4\u65b0</p> <pre><code>  // \u9759\u6001\u914d\u7f6e\n  val hostName = getString(KafkaConfig.HostNameProp)\n  val port = getInt(KafkaConfig.PortProp)\n  val advertisedHostName = Option(getString(KafkaConfig.AdvertisedHostNameProp)).getOrElse(hostName)\n  val advertisedPort: java.lang.Integer = Option(getInt(KafkaConfig.AdvertisedPortProp)).getOrElse(port)\n\n  // NOTE: \u52a8\u6001\u914d\u7f6e\n  def maxConnections = getInt(KafkaConfig.MaxConnectionsProp)\n  def maxConnectionCreationRate = getInt(KafkaConfig.MaxConnectionCreationRateProp)\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/2-Dynamic-Config/#_4","title":"\u914d\u7f6e\u66f4\u65b0","text":"<p>\u5728 <code>KafkaServer.startup()</code> \u65b9\u6cd5\u4e2d</p> <pre><code>        // NOTE: \u6ce8\u610f\u8fd9\u91cc\uff0caddReconfigurables \u5728 initialize \u4e4b\u540e\uff0c\u6240\u4ee5\u4e4b\u524d initialize \u65f6\u4e0d\u4f1a\u5bf9 reconfigurable \u5e94\u7528\u52a8\u6001\u914d\u7f6e\uff0c\n        // \u4f46\u662f\u5b9e\u9645\u7684 reconfigurable \u5bf9\u8c61\u5e76\u4e0d\u4f1a\u91cd\u65b0\u914d\u7f6e\n        // \u4f46\u662f\u4f1a\u66f4\u65b0 kafkaConfig \u7684 currentConfig\n\n        /* Add all reconfigurables for config change notification before starting config handlers */\n        config.dynamicConfig.addReconfigurables(this)\n\n        /* start dynamic config manager */\n        dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -&gt; new TopicConfigHandler(logManager, config, quotaManagers, kafkaController),\n                                                           ConfigType.Client -&gt; new ClientIdConfigHandler(quotaManagers),\n                                                           ConfigType.User -&gt; new UserConfigHandler(quotaManagers, credentialProvider),\n                                                           ConfigType.Broker -&gt; new BrokerConfigHandler(config, quotaManagers))\n\n        // Create the config manager. start listening to notifications\n        dynamicConfigManager = new DynamicConfigManager(zkClient, dynamicConfigHandlers)\n        dynamicConfigManager.startup()\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/2-Dynamic-Config/#dynamicconfigmanager","title":"DynamicConfigManager","text":"<p>\u914d\u7f6e\u66f4\u65b0\u65f6\uff0c\u4f1a\u5c06\u6570\u636e\u987a\u5e8f\u5199\u5230 <code>/config/changes</code> \u8def\u5f84\u7684\u8282\u70b9\u4e0b\uff0cKafkaServer \u542f\u52a8\u65f6\u4f1a\u521d\u59cb\u5316\u4e00\u4e2a <code>DynamicConfigManager</code> \u5bf9\u8c61\uff0c\u76d1\u542c\u8fd9\u4e2a\u8def\u5f84\u3002</p> <pre><code>class DynamicConfigManager(private val zkClient: KafkaZkClient,\n                           private val configHandlers: Map[String, ConfigHandler],\n                           private val changeExpirationMs: Long = 15*60*1000,\n                           private val time: Time = Time.SYSTEM) extends Logging {\n\n\n  private val configChangeListener = new ZkNodeChangeNotificationListener(zkClient, ConfigEntityChangeNotificationZNode.path,\n    ConfigEntityChangeNotificationSequenceZNode.SequenceNumberPrefix, ConfigChangedNotificationHandler)\n\n  /**\n   * Begin watching for config changes\n   */\n  def startup(): Unit = {\n    configChangeListener.init()\n\n    // Apply all existing client/user configs to the ClientIdConfigHandler/UserConfigHandler to bootstrap the overrides\n    configHandlers.foreach {\n      case (ConfigType.User, handler) =&gt;\n        adminZkClient.fetchAllEntityConfigs(ConfigType.User).foreach {\n          case (sanitizedUser, properties) =&gt; handler.processConfigChanges(sanitizedUser, properties)\n        }\n        adminZkClient.fetchAllChildEntityConfigs(ConfigType.User, ConfigType.Client).foreach {\n          case (sanitizedUserClientId, properties) =&gt; handler.processConfigChanges(sanitizedUserClientId, properties)\n        }\n      case (configType, handler) =&gt;\n        adminZkClient.fetchAllEntityConfigs(configType).foreach {\n          case (entityName, properties) =&gt; handler.processConfigChanges(entityName, properties)\n        }\n    }\n  }\n}\n</code></pre> <p>\u76d1\u542c\u8fd9\u4e2a\u8def\u5f84\u5bf9\u5e94\u7684\u5904\u7406 handler \u4e3a <code>ConfigChangedNotificationHandler</code>\uff0c\u7ee7\u627f\u81ea <code>NotificationHandler</code>\uff0c\u5b9e\u73b0 <code>processNotifiction</code> \u65b9\u6cd5\uff0c\u5904\u7406\u65b0\u589e\u7684\u52a8\u6001\u914d\u7f6e\u66f4\u65b0\u4fe1\u606f\u3002</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/2-Dynamic-Config/#confighandler","title":"ConfigHandler","text":"<p><code>DynamicConfigManager</code> \u6301\u6709 <code>configHandlers</code>\uff0c\u5305\u542b 4 \u4e2d\u7c7b\u578b\u914d\u7f6e\u5bf9\u5e94\u7684 <code>ConfigHandler</code></p> <p>4 \u79cd\u7c7b\u578b\u914d\u7f6e\u7684\u66f4\u65b0\uff0c\u5bf9\u5e94 4 \u79cd ConfigHandler</p> <ul> <li>BrokerConfigHandler</li> <li>TopicConfigHandler</li> <li>UserConfigHandler</li> <li>ClientIdConfighandler</li> </ul>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/2-Dynamic-Config/#brokerconfighandler","title":"BrokerConfigHandler","text":"<pre><code>/**\n  * The BrokerConfigHandler will process individual broker config changes in ZK.\n  * The callback provides the brokerId and the full properties set read from ZK.\n  * This implementation reports the overrides to the respective ReplicationQuotaManager objects\n  */\nclass BrokerConfigHandler(private val brokerConfig: KafkaConfig,\n                          private val quotaManagers: QuotaManagers) extends ConfigHandler with Logging {\n\n  def processConfigChanges(brokerId: String, properties: Properties): Unit = {\n    def getOrDefault(prop: String): Long = {\n      if (properties.containsKey(prop))\n        properties.getProperty(prop).toLong\n      else\n        DefaultReplicationThrottledRate\n    }\n    if (brokerId == ConfigEntityName.Default) {\n      brokerConfig.dynamicConfig.updateDefaultConfig(properties)\n    } else if (brokerConfig.brokerId == brokerId.trim.toInt) {\n      brokerConfig.dynamicConfig.updateBrokerConfig(brokerConfig.brokerId, properties)\n      quotaManagers.leader.updateQuota(upperBound(getOrDefault(LeaderReplicationThrottledRateProp).toDouble))\n      quotaManagers.follower.updateQuota(upperBound(getOrDefault(FollowerReplicationThrottledRateProp).toDouble))\n      quotaManagers.alterLogDirs.updateQuota(upperBound(getOrDefault(ReplicaAlterLogDirsIoMaxBytesPerSecondProp).toDouble))\n    }\n  }\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/2-Dynamic-Config/#topicconfighandler","title":"TopicConfigHandler","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/10-Config/2-Dynamic-Config/#reconfigurablebrokerreconfigurable","title":"Reconfigurable/BrokerReconfigurable","text":"<p>\u53ef\u4ee5\u5728\u8fd0\u884c\u4e2d\u52a8\u6001\u66f4\u65b0\u914d\u7f6e\u7684\u5b9e\u4f8b\u90fd\u7ee7\u627f <code>Reconfigurable</code>/<code>BrokerReconfigurable</code> \u63a5\u53e3</p> <ul> <li>BrokerReconfigurable<ul> <li>DynamicThreadPool(KafkaServer)<ul> <li>numIoThreads</li> <li>numNetworkThreads</li> <li>numReplicaFetchers</li> <li>numRecoveryThreadsPerDataDir</li> <li>backgroundThreads</li> </ul> </li> <li>KafkaServer.logManager.cleaner</li> <li>DynamicLogConfig</li> <li>DynamicListenerConfig</li> <li>kafkaServer.socketServer</li> </ul> </li> </ul>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/","title":"1.3 controller","text":"<p>Kafka \u96c6\u7fa4\u4e2d\u4f1a\u6709\u4e00\u4e2a broker \u4f5c\u4e3a\u96c6\u7fa4 controller\uff0c\u8d1f\u8d23\u4e00\u4e9b\u7ba1\u7406\u548c\u534f\u8c03\u7684\u5de5\u4f5c\uff1a</p> <ul> <li>\u76d1\u542c <code>/brokers/topics</code>\uff0c\u89e6\u53d1 topic \u521b\u5efa\u52a8\u4f5c</li> <li>\u76d1\u542c <code>/admin/delete_topics</code>\uff0c\u89e6\u53d1 topic \u5220\u9664\u52a8\u4f5c</li> <li>\u76d1\u542c <code>/admin/reassign_partitions</code>\uff0c\u89e6\u53d1 topic \u91cd\u5206\u533a\u64cd\u4f5c</li> <li>\u76d1\u542c <code>/admin/preferred_replica_election</code>\uff0c\u89e6\u53d1\u6700\u4f18 leader \u9009\u4e3e\u64cd\u4f5c</li> <li>\u76d1\u542c <code>/brokers/topics/&lt;topic&gt;</code>\uff0c\u89e6\u53d1 topic partition \u6269\u5bb9\u64cd\u4f5c</li> <li>\u76d1\u542c <code>/brokers/ids</code>\uff0c\u89e6\u53d1 broker \u4e0a\u7ebf/\u4e0b\u7ebf \u64cd\u4f5c</li> <li>\u5f53\u96c6\u7fa4\u5143\u4fe1\u606f\u53d8\u52a8\u65f6\uff0ccontroller \u901a\u8fc7 <code>UpdateMetadataRequest</code> \u8bf7\u6c42\u5e7f\u64ad\u4fe1\u606f\u5230\u96c6\u7fa4\u7684\u6240\u6709 broker \u4e0a\uff0c\u901a\u8fc7 <code>LeaderAndIsrRequest</code>, <code>StopReplicaRequest</code> \u53d1\u9001\u8bf7\u6c42\u5230\u76f8\u5173 broker</li> <li>broker \u81ea\u884c\u505c\u6b62\u65f6\uff0c\u5411 controller \u53d1\u9001 <code>ControlledShudownRequest</code> \u8bf7\u6c42\uff0ccontroller \u8d1f\u8d23\u6536\u5c3e\u5de5\u4f5c</li> <li>\u76d1\u542c <code>/controller</code>\uff0c\u5982\u679c\u8be5\u8282\u70b9\u6d88\u5931\uff0c\u6240\u6709 broker \u62a2\u5360 <code>/controller</code>\uff0c\u62a2\u5360\u6210\u529f\u5219\u6210\u4e3a\u65b0\u7684 controller</li> </ul> <p>\u53c2\u8003\uff1a - https://cwiki.apache.org/confluence/display/kafka/kafka+controller+redesign - https://jaceklaskowski.gitbooks.io/apache-kafka/content/kafka-controller-ControllerEventManager.html - https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/1-controller%E5%90%AF%E5%8A%A8/","title":"1.3.1 controller \u542f\u52a8","text":"<p>controller \u5728 <code>kafkaServer.startup()</code> \u4e2d\u521d\u59cb\u5316\u5e76\u542f\u52a8\uff1a</p> <pre><code>  /**\n   * Start up API for bringing up a single instance of the Kafka server.\n   * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers\n   */\n  def startup(): Unit = {\n    try {\n      info(\"starting\")\n\n      if (isShuttingDown.get)\n        throw new IllegalStateException(\"Kafka server is still shutting down, cannot re-start!\")\n\n      if (startupComplete.get)\n        return\n\n      val canStartup = isStartingUp.compareAndSet(false, true)\n      if (canStartup) {\n\n        // ...\n\n        /* start kafka controller */\n        kafkaController = new KafkaController(config, zkClient, time, metrics, brokerInfo, brokerEpoch, tokenManager, brokerFeatures, featureCache, threadNamePrefix)\n        kafkaController.startup()\n\n        // ...\n</code></pre> <p><code>KafkaController.startup()</code> \u65b9\u6cd5\u4f1a\u5411 zookeeper \u6ce8\u518c StateChangeHandler\uff0c\u5e76\u542f\u52a8 eventManager</p> <pre><code>/**\n * Invoked when the controller module of a Kafka server is started up. This does not assume that the current broker\n * is the controller. It merely registers the session expiration listener and starts the controller leader\n * elector\n */\ndef startup() = {\n  zkClient.registerStateChangeHandler(new StateChangeHandler {\n    override val name: String = StateChangeHandlers.ControllerHandler\n    override def afterInitializingSession(): Unit = {\n      eventManager.put(RegisterBrokerAndReelect)\n    }\n    override def beforeInitializingSession(): Unit = {\n      val queuedEvent = eventManager.clearAndPut(Expire)\n\n      // Block initialization of the new session until the expiration event is being handled,\n      // which ensures that all pending events have been processed before creating the new session\n      queuedEvent.awaitProcessing()\n    }\n  })\n  eventManager.put(Startup)\n  eventManager.start()\n}\n</code></pre> <p><code>eventManager</code> \u662f <code>ControllerEventManager</code> \u7684\u5b9e\u4f8b\uff0c\u5176\u7ef4\u62a4\u4e86 <code>QueuedEvent</code>(\u5bf9 <code>ControllerEvent</code> \u7684\u5305\u88c5) \u7684\u961f\u5217\uff0c\u5e76\u542f\u52a8\u7ebf\u7a0b\uff1a</p> <ol> <li>\u4ece <code>queue</code> \u4e2d\u53d6\u4e8b\u4ef6 <code>QueuedEvent</code></li> <li>\u901a\u8fc7 <code>processor</code> \u5bf9\u4e8b\u4ef6\u8fdb\u884c\u5904\u7406</li> </ol> <p>\u8fd9\u91cc\u7684 <code>processor</code> \u662f <code>KafkaController</code>\uff0c\u5176\u5b9e\u73b0\u4e86 <code>ControllerEventProcessor</code> \u5b9a\u4e49\u7684\u65b9\u6cd5 <code>process()</code> \u65b9\u6cd5</p> <p>\u5728\u542f\u52a8 <code>eventManager</code> \u4e4b\u524d\uff0c\u5411 <code>eventManager</code> \u589e\u52a0\u4e86 <code>Startup</code> \u4e8b\u4ef6\uff0c\u53ef\u4ee5\u5728 <code>KafkaController.process()</code> \u65b9\u6cd5\u4e2d\u770b\u5230\u5bf9\u6240\u6709\u4e8b\u4ef6\u7684\u5904\u7406</p> <pre><code>override def process(event: ControllerEvent): Unit = {\n  try {\n    event match {\n      case event: MockEvent =&gt;\n        // Used only in test cases\n        event.process()\n      case ShutdownEventThread =&gt;\n        error(\"Received a ShutdownEventThread event. This type of event is supposed to be handle by ControllerEventThread\")\n      // ...\n      case Startup =&gt;\n        processStartup()\n    }\n  } catch {\n    case e: ControllerMovedException =&gt;\n      info(s\"Controller moved to another broker when processing $event.\", e)\n      maybeResign()\n    case e: Throwable =&gt;\n      error(s\"Error processing event $event\", e)\n  } finally {\n    updateMetrics()\n  }\n}\n</code></pre> <p>\u53ef\u4ee5\u770b\u5230\uff0c\u5bf9\u4e8e <code>Startup</code> \u4e8b\u4ef6\uff0c\u8fd9\u91cc\u5b9e\u9645\u901a\u8fc7 <code>processStartup()</code> \u8fdb\u884c\u5904\u7406\u3002</p> <pre><code>private def processStartup(): Unit = {\n  zkClient.registerZNodeChangeHandlerAndCheckExistence(controllerChangeHandler)\n  elect()\n}\n</code></pre> <p>\u8fd9\u91cc\u9996\u5148\u5411 zookeeper \u6ce8\u518c controllerChangeHandler \u7528\u6765\u76d1\u542c <code>/controller</code>\uff0c\u8d1f\u8d23 controller \u81ea\u8eab\u7684\u9009\u4e3e\u3001\u7ba1\u7406\u64cd\u4f5c\uff1a</p> <pre><code>class ControllerChangeHandler(eventManager: ControllerEventManager) extends ZNodeChangeHandler {\n  override val path: String = ControllerZNode.path\n\n  override def handleCreation(): Unit = eventManager.put(ControllerChange)\n  override def handleDeletion(): Unit = eventManager.put(Reelect)\n  override def handleDataChange(): Unit = eventManager.put(ControllerChange)\n}\n</code></pre> <ul> <li><code>/controller</code> zookeeper \u8282\u70b9\u521b\u5efa\u65f6\u5411 eventManager \u7684 queue \u589e\u52a0 <code>ControllerChange</code> \u4e8b\u4ef6</li> <li><code>/controller</code> zookeeper \u8282\u70b9\u5220\u9664\u65f6\u5411 eventManager \u7684 queue \u589e\u52a0 <code>Reelect</code> \u4e8b\u4ef6</li> <li><code>/controller</code> zookeeper \u8282\u70b9\u5185\u5bb9\u6539\u53d8\u65f6\u5411 eventManager \u7684 queue \u589e\u52a0 <code>ControllerChange</code> \u4e8b\u4ef6</li> </ul> <p>\u7136\u540e\uff0c\u6267\u884c <code>elect()</code>\uff0c\u5c1d\u8bd5\u6210\u4e3a\u5f53\u524d\u96c6\u7fa4\u7684 controller</p> <pre><code>private def elect(): Unit = {\n  activeControllerId = zkClient.getControllerId.getOrElse(-1)\n  /*\n   * We can get here during the initial startup and the handleDeleted ZK callback. Because of the potential race condition,\n   * it's possible that the controller has already been elected when we get here. This check will prevent the following\n   * createEphemeralPath method from getting into an infinite loop if this broker is already the controller.\n   */\n  if (activeControllerId != -1) {\n    debug(s\"Broker $activeControllerId has been elected as the controller, so stopping the election process.\")\n    return\n  }\n\n  try {\n    val (epoch, epochZkVersion) = zkClient.registerControllerAndIncrementControllerEpoch(config.brokerId)\n    controllerContext.epoch = epoch\n    controllerContext.epochZkVersion = epochZkVersion\n    activeControllerId = config.brokerId\n\n    info(s\"${config.brokerId} successfully elected as the controller. Epoch incremented to ${controllerContext.epoch} \" +\n      s\"and epoch zk version is now ${controllerContext.epochZkVersion}\")\n\n    onControllerFailover()\n  } catch {\n    case e: ControllerMovedException =&gt;\n      maybeResign()\n\n      if (activeControllerId != -1)\n        debug(s\"Broker $activeControllerId was elected as controller instead of broker ${config.brokerId}\", e)\n      else\n        warn(\"A controller has been elected but just resigned, this will result in another round of election\", e)\n    case t: Throwable =&gt;\n      error(s\"Error while electing or becoming controller on broker ${config.brokerId}. \" +\n        s\"Trigger controller movement immediately\", t)\n      triggerControllerMove()\n  }\n}\n</code></pre> <ol> <li>\u9996\u5148\u83b7\u53d6\u5f53\u524d\u662f\u5426\u6709 <code>/controller</code>\uff0c\u5982\u679c\u6709\uff0c\u5373 <code>activeControllerId != -1</code>\uff0c\u5219\u8fd4\u56de\uff0c\u5426\u5219\u7ee7\u7eed</li> <li>\u5c1d\u8bd5\u4ee5\u81ea\u8eab brokerId \u521b\u5efa <code>/controller</code>\uff0c\u5982\u679c\u521b\u5efa\u6210\u529f\uff0c\u5219\u5f53\u524d broker \u4e3a\u96c6\u7fa4 controller\uff0c\u6267\u884c <code>onControllerFailover()</code> \u65b9\u6cd5\uff0c\u5982\u4e0b\uff1a<ol> <li>\u521d\u59cb\u5316 controller context \u7f13\u5b58\u6240\u6709 topics\uff0clive brokers\uff0cleaders for all existing partitions</li> <li>\u5f00\u542f controller channel manager</li> <li>\u5f00\u542f replica state machine</li> <li>\u5f00\u542f partition state machine</li> </ol> </li> </ol> <pre><code>/**\n * This callback is invoked by the zookeeper leader elector on electing the current broker as the new controller.\n * It does the following things on the become-controller state change -\n * 1. Initializes the controller's context object that holds cache objects for current topics, live brokers and\n *    leaders for all existing partitions.\n * 2. Starts the controller's channel manager\n * 3. Starts the replica state machine\n * 4. Starts the partition state machine\n * If it encounters any unexpected exception/error while becoming controller, it resigns as the current controller.\n * This ensures another controller election will be triggered and there will always be an actively serving controller\n */\nprivate def onControllerFailover(): Unit = {\n  maybeSetupFeatureVersioning()\n\n  info(\"Registering handlers\")\n\n  // before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks\n  val childChangeHandlers = Seq(brokerChangeHandler, topicChangeHandler, topicDeletionHandler, logDirEventNotificationHandler,\n    isrChangeNotificationHandler)\n  childChangeHandlers.foreach(zkClient.registerZNodeChildChangeHandler)\n\n  val nodeChangeHandlers = Seq(preferredReplicaElectionHandler, partitionReassignmentHandler)\n  nodeChangeHandlers.foreach(zkClient.registerZNodeChangeHandlerAndCheckExistence)\n\n  info(\"Deleting log dir event notifications\")\n  zkClient.deleteLogDirEventNotifications(controllerContext.epochZkVersion)\n  info(\"Deleting isr change notifications\")\n  zkClient.deleteIsrChangeNotifications(controllerContext.epochZkVersion)\n  info(\"Initializing controller context\")\n  initializeControllerContext()\n  info(\"Fetching topic deletions in progress\")\n  val (topicsToBeDeleted, topicsIneligibleForDeletion) = fetchTopicDeletionsInProgress()\n  info(\"Initializing topic deletion manager\")\n  topicDeletionManager.init(topicsToBeDeleted, topicsIneligibleForDeletion)\n\n  // We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines\n  // are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before\n  // they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and\n  // partitionStateMachine.startup().\n  info(\"Sending update metadata request\")\n  sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, Set.empty)\n\n  replicaStateMachine.startup()\n  partitionStateMachine.startup()\n\n  info(s\"Ready to serve as the new controller with epoch $epoch\")\n\n  initializePartitionReassignments()\n  topicDeletionManager.tryTopicDeletion()\n  val pendingPreferredReplicaElections = fetchPendingPreferredReplicaElections()\n  onReplicaElection(pendingPreferredReplicaElections, ElectionType.PREFERRED, ZkTriggered)\n  info(\"Starting the controller scheduler\")\n  kafkaScheduler.startup()\n  if (config.autoLeaderRebalanceEnable) {\n    scheduleAutoLeaderRebalanceTask(delay = 5, unit = TimeUnit.SECONDS)\n  }\n\n  if (config.tokenAuthEnabled) {\n    info(\"starting the token expiry check scheduler\")\n    tokenCleanScheduler.startup()\n    tokenCleanScheduler.schedule(name = \"delete-expired-tokens\",\n      fun = () =&gt; tokenManager.expireTokens(),\n      period = config.delegationTokenExpiryCheckIntervalMs,\n      unit = TimeUnit.MILLISECONDS)\n  }\n}\n</code></pre> <ol> <li>\u6ce8\u518c\u4ee5\u4e0b handler\uff0c\u76d1\u542c zookeeper\uff0c\u505a\u5bf9\u5e94\u5904\u7406\uff1a<ol> <li><code>brokerChangeHandler</code>\uff0c\u76d1\u542c <code>/brokers/ids</code>\uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 <code>BrokerChange</code> \u4e8b\u4ef6</li> <li><code>topicChangeHandler</code>\uff0c\u76d1\u542c <code>/brokers/topics</code>\uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 <code>TopicChange</code> \u4e8b\u4ef6</li> <li><code>topicDeletionHandler</code>\uff0c\u76d1\u542c <code>/admin/delete_topics</code>\uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 <code>TopicDeletion</code> \u4e8b\u4ef6</li> <li><code>logDirEventNotificationHandler</code>\uff0c\u76d1\u542c <code>/log_dir_event_notification</code>\uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 <code>LogDirEventNotification</code> \u4e8b\u4ef6</li> <li><code>isrChangeNotificationHandler</code>\uff0c\u76d1\u542c <code>/isr_change_notification</code>\uff0c\u5982\u679c child \u6709\u53d8\u5316\uff0c\u5411 eventManager \u589e\u52a0 <code>IsrChangeNotification</code> \u4e8b\u4ef6</li> <li><code>preferredReplicaElectionHandler</code>\uff0c\u76d1\u542c <code>/admin/preferred_replica_election</code>\uff0c\u5982\u679c\u65b0\u5efa\uff0c\u5411 eventManager \u589e\u52a0 <code>ReplicaLeaderElection</code> \u4e8b\u4ef6</li> <li><code>partitionReassignmentHandler</code>\uff0c\u76d1\u542c <code>/admin/reassign_partitions</code>\uff0c\u5982\u679c\u65b0\u5efa\uff0c\u5411 eventManager \u589e\u52a0 <code>ZkPartitionReassignment</code> \u4e8b\u4ef6</li> </ol> </li> <li>\u521d\u59cb\u5316 controllerContext</li> <li>\u83b7\u53d6 <code>/admin/delete_topics/</code> \u8282\u70b9\uff0c\u67e5\u770b\u662f\u5426\u6709\u5f85\u5220\u9664\u7684 topic\uff0c\u521d\u59cb TopicDeletionManager</li> <li>\u5411\u6240\u6709 broker \u53d1\u9001 UpdateMetadataRequest</li> <li>\u5f00\u542f replicaStateMachine</li> <li>\u5f00\u542f partitionStateMachine</li> <li>\u5c1d\u8bd5\u8fdb\u884c\u672a\u5b8c\u6210\u7684 partitionReassignments</li> <li>\u5c1d\u8bd5\u8fdb\u884c\u672a\u5b8c\u6210\u7684 preferredReplicaElections</li> <li>\u542f\u52a8 kafkaScheduler</li> <li>\u5982\u679c\u914d\u7f6e\u4e86 auto leader rebalance\uff0c\u5219\u5468\u671f\u8fdb\u884c leader rebalance</li> <li>\u5982\u679c\u5f00\u542f token auth\uff0c\u5219\u5468\u671f\u5220\u9664\u8fc7\u671f token</li> </ol> <p>\u81f3\u6b64\uff0ccontroller \u5df2\u7ecf\u5b8c\u6210\u542f\u52a8</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/3-reassign-partitions/","title":"1.3.3 controller","text":"<p>\u5728 controller \u542f\u52a8\u8fc7\u7a0b\u4e2d\uff0c\u5728 <code>KafkaController.onControllerFailover()</code> \u65b9\u6cd5\u4e2d\u5411 zkClient \u6ce8\u518c\u4e86 <code>PartitionReassignmentHandler</code></p> <p><code>PartitionReassignmentHandler</code> \u5982\u4e0b\uff1a</p> <pre><code>class PartitionReassignmentHandler(eventManager: ControllerEventManager) extends ZNodeChangeHandler {\n  override val path: String = ReassignPartitionsZNode.path\n\n  // Note that the event is also enqueued when the znode is deleted, but we do it explicitly instead of relying on\n  // handleDeletion(). This approach is more robust as it doesn't depend on the watcher being re-registered after\n  // it's consumed during data changes (we ensure re-registration when the znode is deleted).\n  override def handleCreation(): Unit = eventManager.put(ZkPartitionReassignment)\n}\n</code></pre> <p>\u8fd9\u91cc\u5b9e\u9645\u4e0a\u662f\u76d1\u542c zookeeper <code>/admin/reassign_partitions</code> \u8282\u70b9\uff0c\u5982\u679c\u6709\u65b0\u8282\u70b9\u521b\u5efa\uff0c\u5219\u5411 eventManager \u4e8b\u4ef6\u961f\u5217 queue \u589e\u52a0 <code>ZkPartitionReassignment</code></p> <p>eventManager \u4f1a\u542f\u52a8\u5355\u72ec\u7684\u7ebf\u7a0b\u4e0d\u65ad\u4ece\u4e8b\u4ef6\u961f\u5217\u4e2d\u53d6\u51fa\u4e8b\u4ef6\uff0c\u901a\u8fc7 processor \u8fdb\u884c\u5904\u7406</p> <p>eventManager \u7684 processor \u662f KafkaController \u7684\u5b9e\u4f8b\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728 <code>KafkaController.process()</code> \u65b9\u6cd5\u91cc\u627e\u5230\u5bf9 <code>ZkPartitionReassignment</code> \u7684\u5904\u7406</p> <pre><code>  override def process(event: ControllerEvent): Unit = {\n    try {\n      event match {\n        case event: MockEvent =&gt;\n          // Used only in test cases\n          event.process()\n\n        // ...\n\n        case ApiPartitionReassignment(reassignments, callback) =&gt;\n          processApiPartitionReassignment(reassignments, callback)\n\n        case ZkPartitionReassignment =&gt;\n          processZkPartitionReassignment()\n\n        case ListPartitionReassignments(partitions, callback) =&gt;\n          processListPartitionReassignments(partitions, callback)\n\n        case PartitionReassignmentIsrChange(partition) =&gt;\n          processPartitionReassignmentIsrChange(partition)\n\n        // ...\n\n      }\n    } catch {\n      case e: ControllerMovedException =&gt;\n        info(s\"Controller moved to another broker when processing $event.\", e)\n        maybeResign()\n      case e: Throwable =&gt;\n        error(s\"Error processing event $event\", e)\n    } finally {\n      updateMetrics()\n    }\n  }\n</code></pre> <p>\u5bf9 <code>ZkPartitionReassignment</code> \u901a\u8fc7 <code>processZkPartitionReassignment()</code> \u8fdb\u884c\u5904\u7406\uff1a</p> <pre><code>  private def processZkPartitionReassignment(): Set[TopicPartition] = {\n    // We need to register the watcher if the path doesn't exist in order to detect future\n    // reassignments and we get the `path exists` check for free\n    if (isActive &amp;&amp; zkClient.registerZNodeChangeHandlerAndCheckExistence(partitionReassignmentHandler)) {\n      val reassignmentResults = mutable.Map.empty[TopicPartition, ApiError]\n      val partitionsToReassign = mutable.Map.empty[TopicPartition, ReplicaAssignment]\n\n      // \u8fd9\u91cc\u7ec4\u5408\u4e86\u73b0\u5728\u7684 replica \u548c targetReplica\n      zkClient.getPartitionReassignment.forKeyValue { (tp, targetReplicas) =&gt;\n        maybeBuildReassignment(tp, Some(targetReplicas)) match {\n          case Some(context) =&gt; partitionsToReassign.put(tp, context)\n          case None =&gt; reassignmentResults.put(tp, new ApiError(Errors.NO_REASSIGNMENT_IN_PROGRESS))\n        }\n      }\n\n      // \u89e6\u53d1 partition reassignment\n      reassignmentResults ++= maybeTriggerPartitionReassignment(partitionsToReassign)\n      val (partitionsReassigned, partitionsFailed) = reassignmentResults.partition(_._2.error == Errors.NONE)\n      if (partitionsFailed.nonEmpty) {\n        warn(s\"Failed reassignment through zk with the following errors: $partitionsFailed\")\n        maybeRemoveFromZkReassignment((tp, _) =&gt; partitionsFailed.contains(tp))\n      }\n      partitionsReassigned.keySet\n    } else {\n      Set.empty\n    }\n  }\n</code></pre> <pre><code>/**\n * Trigger a partition reassignment provided that the topic exists and is not being deleted.\n *\n * This is called when a reassignment is initially received either through Zookeeper or through the\n * AlterPartitionReassignments API\n *\n * The `partitionsBeingReassigned` field in the controller context will be updated by this\n * call after the reassignment completes validation and is successfully stored in the topic\n * assignment zNode.\n *\n * @param reassignments The reassignments to begin processing\n * @return A map of any errors in the reassignment. If the error is NONE for a given partition,\n *         then the reassignment was submitted successfully.\n */\nprivate def maybeTriggerPartitionReassignment(reassignments: Map[TopicPartition, ReplicaAssignment]): Map[TopicPartition, ApiError] = {\n  reassignments.map { case (tp, reassignment) =&gt;\n    val topic = tp.topic\n\n    val apiError = if (topicDeletionManager.isTopicQueuedUpForDeletion(topic)) {\n      info(s\"Skipping reassignment of $tp since the topic is currently being deleted\")\n      new ApiError(Errors.UNKNOWN_TOPIC_OR_PARTITION, \"The partition does not exist.\")\n    } else {\n      val assignedReplicas = controllerContext.partitionReplicaAssignment(tp)\n      if (assignedReplicas.nonEmpty) {\n        try {\n          onPartitionReassignment(tp, reassignment)\n          ApiError.NONE\n        } catch {\n          case e: ControllerMovedException =&gt;\n            info(s\"Failed completing reassignment of partition $tp because controller has moved to another broker\")\n            throw e\n          case e: Throwable =&gt;\n            error(s\"Error completing reassignment of partition $tp\", e)\n            new ApiError(Errors.UNKNOWN_SERVER_ERROR)\n        }\n      } else {\n        new ApiError(Errors.UNKNOWN_TOPIC_OR_PARTITION, \"The partition does not exist.\")\n      }\n    }\n\n    tp -&gt; apiError\n  }\n}\n</code></pre> <p>\u5b9a\u4e49\u4ee5\u4e0b\u6982\u5ff5\uff1a - RS: \u5f53\u524d\u526f\u672c\u96c6\u5408 - ORS: \u539f\u59cb\u526f\u672c\u96c6\u5408 - TRS: \u76ee\u6807\u526f\u672c\u96c6\u5408 - AR: \u5728\u8fd9\u6b21\u91cd\u5206\u914d\u4e2d\u8981\u589e\u52a0\u7684\u526f\u672c - RR: \u5728\u8fd9\u6b21\u91cd\u5206\u914d\u4e2d\u8981\u79fb\u9664\u7684\u526f\u672c</p> <p>\u5047\u8bbe\u4e00\u4e2a topic \u5f53\u524d\u526f\u672c\u96c6\u4e3a 1,2,3 \uff0c\u5219\u5176 RS \u548c ORS \u4e3a 1,2,3</p> <p>\u5982\u679c\u8981\u5c06\u5176\u91cd\u5206\u914d\u4e3a 4,5,6 \uff0c\u5219\u5176 TRS \u4e3a 4,5,6 , AR \u4e3a 4,5,6 , RR \u4e3a 1,2,3</p> RS AR RR leader isr step 1,2,3 1 1,2,3 initial state 4,5,6,1,2,3 4,5,6 1,2,3 1 1,2,3 step A2 4,5,6,1,2,3 4,5,6 1,2,3 1 1,2,3,4,5,6 phase B 4,5,6,1,2,3 4,5,6 1,2,3 4 1,2,3,4,5,6 step B3 4,5,6,1,2,3 4,5,6 1,2,3 4 4,5,6 step B4 4,5,6 4 4,5,6 step B6 <p>\u5c06\u6574\u4e2a\u91cd\u5206\u914d\u8fc7\u7a0b\u5206\u4e3a 3 \u4e2a\u9636\u6bb5\uff0c\u6bcf\u4e2a\u9636\u6bb5\u6709\u81ea\u5df1\u7684\u6b65\u9aa4 1. Phase U (Assignment update)     1. U1: \u66f4\u65b0 Zk \u4f7f RS = ORS + TRS, AR = TRS - ORS, RR = ORS - TRS     2. U2: \u66f4\u65b0 memory \u4f7f RS = ORS + TRS, AR = TRS - ORS, RR = ORS - TRS     3. U3: \u5982\u679c\u6b63\u5728\u53d6\u6d88\u5f53\u524d\u7684\u91cd\u5206\u914d\u6216\u8005\u66ff\u6362\u5f53\u524d\u7684\u91cd\u5206\u914d\uff0c\u5411\u6240\u6709\u4e0d\u5728\u65b0\u5206\u914d\u7684 TRS \u7684 AR \u53d1\u9001 <code>StopReplica</code> \u8bf7\u6c42 2. Phase A (when TRS != ISR)     1. A1: bump the leader epoch for the partition and send LeaderAndIsr updates to RS     2. A2: \u901a\u8fc7\u5c06 AR \u4e2d\u7684 replicas \u7f6e\u4e3a <code>NewReplica</code> \u72b6\u6001\u542f\u52a8\u65b0\u7684 replicas 3. Phase B (when TRS == ISR)     1. B1: \u5c06\u6240\u6709 AR \u4e2d\u7684 replicas \u7f6e\u4e3a <code>OnlineReplica</code> \u72b6\u6001     2. B2: \u66f4\u65b0 memory \u4f7f RS = TRS, AR = [], RR = []     3. B3: \u53d1\u9001\u8868\u793a RS = TRS \u7684 <code>LeaderAndIsr</code> \u8bf7\u6c42\u3002This will prevent the leader from adding any replica in TRS - ORS back in the isr. \u5982\u679c\u5f53\u524d leader \u4e0d\u5728 TRS \u91cc\u6216\u8005\u4e0d\u662f\u5b58\u6d3b\u7684\uff0cwe move the leader to a new replica in TRS. We may send the LeaderAndIsr to more than the TRS replicas due to the way the partition state machine works (it reads replicas from ZK)     4. B4: \u5c06\u6240\u6709 RR \u4e2d\u7684 replicas \u7f6e\u4e3a <code>OfflineReplica</code> \u72b6\u6001\u3002As part of <code>OfflineReplica</code> state change, we shrink the isr to remove RR in ZooKeeper and send a <code>LeaderAndIsr</code> ONLY to the Leader to notify it of the shrunk isr. After that, we send a <code>StopReplica (delete = false)</code> to the replicas in RR.     5. B5: \u5c06\u6240\u6709 RR \u4e2d\u7684 replicas \u7f6e\u4e3a <code>NonExistentReplica</code> \u72b6\u6001\u3002This will send a <code>StopReplica (delete = true)</code> to the replicas in RR to physically delete the replicas on disk.     6. B6: \u66f4\u65b0 Zk \u4f7f RS = TRS, AR = [], RR = []     7. B7: \u79fb\u9664 ISR reassign listener\uff0c\u66f4\u65b0 Zk \u7684 <code>/admin/reassign_partitions</code> \u79fb\u9664\u8fd9\u4e2a partition     8. B8: \u7ecf\u8fc7 leader \u9009\u4e3e\uff0creplicas \u548c isr \u4fe1\u606f\u6709\u53d8\u66f4\uff0c\u5411\u6240\u6709 broker \u53d1 metadata request</p> <pre><code>/**\n * This callback is invoked:\n * 1. By the AlterPartitionReassignments API\n * 2. By the reassigned partitions listener which is triggered when the /admin/reassign/partitions znode is created\n * 3. When an ongoing reassignment finishes - this is detected by a change in the partition's ISR znode\n * 4. Whenever a new broker comes up which is part of an ongoing reassignment\n * 5. On controller startup/failover\n *\n * Reassigning replicas for a partition goes through a few steps listed in the code.\n * RS = current assigned replica set\n * ORS = Original replica set for partition\n * TRS = Reassigned (target) replica set\n * AR = The replicas we are adding as part of this reassignment\n * RR = The replicas we are removing as part of this reassignment\n *\n * A reassignment may have up to three phases, each with its own steps:\n\n * Phase U (Assignment update): Regardless of the trigger, the first step is in the reassignment process\n * is to update the existing assignment state. We always update the state in Zookeeper before\n * we update memory so that it can be resumed upon controller fail-over.\n *\n *   U1. Update ZK with RS = ORS + TRS, AR = TRS - ORS, RR = ORS - TRS.\n *   U2. Update memory with RS = ORS + TRS, AR = TRS - ORS and RR = ORS - TRS\n *   U3. If we are cancelling or replacing an existing reassignment, send StopReplica to all members\n *       of AR in the original reassignment if they are not in TRS from the new assignment\n *\n * To complete the reassignment, we need to bring the new replicas into sync, so depending on the state\n * of the ISR, we will execute one of the following steps.\n *\n * Phase A (when TRS != ISR): The reassignment is not yet complete\n *\n *   A1. Bump the leader epoch for the partition and send LeaderAndIsr updates to RS.\n *   A2. Start new replicas AR by moving replicas in AR to NewReplica state.\n *\n * Phase B (when TRS = ISR): The reassignment is complete\n *\n *   B1. Move all replicas in AR to OnlineReplica state.\n *   B2. Set RS = TRS, AR = [], RR = [] in memory.\n *   B3. Send a LeaderAndIsr request with RS = TRS. This will prevent the leader from adding any replica in TRS - ORS back in the isr.\n *       If the current leader is not in TRS or isn't alive, we move the leader to a new replica in TRS.\n *       We may send the LeaderAndIsr to more than the TRS replicas due to the\n *       way the partition state machine works (it reads replicas from ZK)\n *   B4. Move all replicas in RR to OfflineReplica state. As part of OfflineReplica state change, we shrink the\n *       isr to remove RR in ZooKeeper and send a LeaderAndIsr ONLY to the Leader to notify it of the shrunk isr.\n *       After that, we send a StopReplica (delete = false) to the replicas in RR.\n *   B5. Move all replicas in RR to NonExistentReplica state. This will send a StopReplica (delete = true) to\n *       the replicas in RR to physically delete the replicas on disk.\n *   B6. Update ZK with RS=TRS, AR=[], RR=[].\n *   B7. Remove the ISR reassign listener and maybe update the /admin/reassign_partitions path in ZK to remove this partition from it if present.\n *   B8. After electing leader, the replicas and isr information changes. So resend the update metadata request to every broker.\n *\n * In general, there are two goals we want to aim for:\n * 1. Every replica present in the replica set of a LeaderAndIsrRequest gets the request sent to it\n * 2. Replicas that are removed from a partition's assignment get StopReplica sent to them\n *\n * For example, if ORS = {1,2,3} and TRS = {4,5,6}, the values in the topic and leader/isr paths in ZK\n * may go through the following transitions.\n * RS                AR          RR          leader     isr\n * {1,2,3}           {}          {}          1          {1,2,3}           (initial state)\n * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     1          {1,2,3}           (step A2)\n * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     1          {1,2,3,4,5,6}     (phase B)\n * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     4          {1,2,3,4,5,6}     (step B3)\n * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     4          {4,5,6}           (step B4)\n * {4,5,6}           {}          {}          4          {4,5,6}           (step B6)\n *\n * Note that we have to update RS in ZK with TRS last since it's the only place where we store ORS persistently.\n * This way, if the controller crashes before that step, we can still recover.\n */\nprivate def onPartitionReassignment(topicPartition: TopicPartition, reassignment: ReplicaAssignment): Unit = {\n  // While a reassignment is in progress, deletion is not allowed\n  topicDeletionManager.markTopicIneligibleForDeletion(Set(topicPartition.topic), reason = \"topic reassignment in progress\")\n\n  updateCurrentReassignment(topicPartition, reassignment)\n\n  val addingReplicas = reassignment.addingReplicas\n  val removingReplicas = reassignment.removingReplicas\n\n  if (!isReassignmentComplete(topicPartition, reassignment)) {\n    // A1. Send LeaderAndIsr request to every replica in ORS + TRS (with the new RS, AR and RR).\n    updateLeaderEpochAndSendRequest(topicPartition, reassignment)\n    // A2. replicas in AR -&gt; NewReplica\n    startNewReplicasForReassignedPartition(topicPartition, addingReplicas)\n  } else {\n    // B1. replicas in AR -&gt; OnlineReplica\n    replicaStateMachine.handleStateChanges(addingReplicas.map(PartitionAndReplica(topicPartition, _)), OnlineReplica)\n    // B2. Set RS = TRS, AR = [], RR = [] in memory.\n    val completedReassignment = ReplicaAssignment(reassignment.targetReplicas)\n    controllerContext.updatePartitionFullReplicaAssignment(topicPartition, completedReassignment)\n    // B3. Send LeaderAndIsr request with a potential new leader (if current leader not in TRS) and\n    //   a new RS (using TRS) and same isr to every broker in ORS + TRS or TRS\n    moveReassignedPartitionLeaderIfRequired(topicPartition, completedReassignment)\n    // B4. replicas in RR -&gt; Offline (force those replicas out of isr)\n    // B5. replicas in RR -&gt; NonExistentReplica (force those replicas to be deleted)\n    stopRemovedReplicasOfReassignedPartition(topicPartition, removingReplicas)\n    // B6. Update ZK with RS = TRS, AR = [], RR = [].\n    updateReplicaAssignmentForPartition(topicPartition, completedReassignment)\n    // B7. Remove the ISR reassign listener and maybe update the /admin/reassign_partitions path in ZK to remove this partition from it.\n    removePartitionFromReassigningPartitions(topicPartition, completedReassignment)\n    // B8. After electing a leader in B3, the replicas and isr information changes, so resend the update metadata request to every broker\n    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, Set(topicPartition))\n    // signal delete topic thread if reassignment for some partitions belonging to topics being deleted just completed\n    topicDeletionManager.resumeDeletionForTopics(Set(topicPartition.topic))\n  }\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/4-delete-topic/","title":"1.3.4 delete topic","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/","title":"1.3.2 controller \u7ec4\u4ef6","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#controllerchannelmanager","title":"ControllerChannelManager","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#_1","title":"\u4e8b\u4ef6\u5904\u7406","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#controllereventprocessor","title":"ControllerEventProcessor","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#queuedevent","title":"QueuedEvent","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/#controllereventmanager","title":"ControllerEventManager","text":"<p>\u7ef4\u6301\u4e00\u4e2a <code>queue</code> \u5b58\u653e\u4e8b\u4ef6\uff1a</p> <pre><code>private val queue = new LinkedBlockingQueue[QueuedEvent]\n</code></pre> <p>\u540c\u65f6\u4f1a\u521b\u5efa\u4e00\u4e2a <code>thread</code> \u5728\u540e\u53f0\u4e0d\u65ad\u4ece <code>queue</code> \u4e2d\u53d6\u4e8b\u4ef6\u5e76\u8fdb\u884c\u5904\u7406</p> <pre><code>private[controller] var thread = new ControllerEventThread(ControllerEventThreadName)\n</code></pre> <pre><code>class ControllerEventThread(name: String) extends ShutdownableThread(name = name, isInterruptible = false) {\n  logIdent = s\"[ControllerEventThread controllerId=$controllerId] \"\n\n  override def doWork(): Unit = {\n    val dequeued = pollFromEventQueue()\n    dequeued.event match {\n      case ShutdownEventThread =&gt; // The shutting down of the thread has been initiated at this point. Ignore this event.\n      case controllerEvent =&gt;\n        _state = controllerEvent.state\n\n        eventQueueTimeHist.update(time.milliseconds() - dequeued.enqueueTimeMs)\n\n        try {\n          def process(): Unit = dequeued.process(processor)\n\n          rateAndTimeMetrics.get(state) match {\n            case Some(timer) =&gt; timer.time { process() }\n            case None =&gt; process()\n          }\n        } catch {\n          case e: Throwable =&gt; error(s\"Uncaught error processing event $controllerEvent\", e)\n        }\n\n        _state = ControllerState.Idle\n    }\n  }\n}\n</code></pre> <p><code>KafkaController</code> \u7ee7\u627f\u81ea <code>ControllerEventProcessor</code>\uff0c\u5b9a\u4e49 <code>process()</code> \u65b9\u6cd5\u5982\u4e0b\uff1a</p> <pre><code>override def process(event: ControllerEvent): Unit = {\n  try {\n    event match {\n      case event: MockEvent =&gt;\n        // Used only in test cases\n        event.process()\n      case ShutdownEventThread =&gt;\n        error(\"Received a ShutdownEventThread event. This type of event is supposed to be handle by ControllerEventThread\")\n      case AutoPreferredReplicaLeaderElection =&gt;\n        processAutoPreferredReplicaLeaderElection()\n      case ReplicaLeaderElection(partitions, electionType, electionTrigger, callback) =&gt;\n        processReplicaLeaderElection(partitions, electionType, electionTrigger, callback)\n      case UncleanLeaderElectionEnable =&gt;\n        processUncleanLeaderElectionEnable()\n      case TopicUncleanLeaderElectionEnable(topic) =&gt;\n        processTopicUncleanLeaderElectionEnable(topic)\n      case ControlledShutdown(id, brokerEpoch, callback) =&gt;\n        processControlledShutdown(id, brokerEpoch, callback)\n      case LeaderAndIsrResponseReceived(response, brokerId) =&gt;\n        processLeaderAndIsrResponseReceived(response, brokerId)\n      case UpdateMetadataResponseReceived(response, brokerId) =&gt;\n        processUpdateMetadataResponseReceived(response, brokerId)\n      case TopicDeletionStopReplicaResponseReceived(replicaId, requestError, partitionErrors) =&gt;\n        processTopicDeletionStopReplicaResponseReceived(replicaId, requestError, partitionErrors)\n      case BrokerChange =&gt;\n        processBrokerChange()\n      case BrokerModifications(brokerId) =&gt;\n        processBrokerModification(brokerId)\n      case ControllerChange =&gt;\n        processControllerChange()\n      case Reelect =&gt;\n        processReelect()\n      case RegisterBrokerAndReelect =&gt;\n        processRegisterBrokerAndReelect()\n      case Expire =&gt;\n        processExpire()\n      case TopicChange =&gt;\n        processTopicChange()\n      case LogDirEventNotification =&gt;\n        processLogDirEventNotification()\n      case PartitionModifications(topic) =&gt;\n        processPartitionModifications(topic)\n      case TopicDeletion =&gt;\n        processTopicDeletion()\n      case ApiPartitionReassignment(reassignments, callback) =&gt;\n        processApiPartitionReassignment(reassignments, callback)\n      case ZkPartitionReassignment =&gt;\n        processZkPartitionReassignment()\n      case ListPartitionReassignments(partitions, callback) =&gt;\n        processListPartitionReassignments(partitions, callback)\n      case UpdateFeatures(request, callback) =&gt;\n        processFeatureUpdates(request, callback)\n      case PartitionReassignmentIsrChange(partition) =&gt;\n        processPartitionReassignmentIsrChange(partition)\n      case IsrChangeNotification =&gt;\n        processIsrChangeNotification()\n      case AlterIsrReceived(brokerId, brokerEpoch, isrsToAlter, callback) =&gt;\n        processAlterIsr(brokerId, brokerEpoch, isrsToAlter, callback)\n      case Startup =&gt;\n        processStartup()\n    }\n  } catch {\n    case e: ControllerMovedException =&gt;\n      info(s\"Controller moved to another broker when processing $event.\", e)\n      maybeResign()\n    case e: Throwable =&gt;\n      error(s\"Error processing event $event\", e)\n  } finally {\n    updateMetrics()\n  }\n}\n</code></pre> <p>\u8fd9\u91cc\u4f1a\u5bf9\u4e8b\u4ef6\u8fdb\u884c\u5904\u7406</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/1-ControllerContext/","title":"1.3.2.1 ControllerContext","text":"<p><code>ControllerContext</code> \u5305\u542b Controller \u9700\u8981\u7ef4\u62a4\u7684\u4fe1\u606f\uff1a</p> <pre><code>// \u4e0b\u7ebf\u5206\u533a\u6570\nvar offlinePartitionCount = 0\n\nvar preferredReplicaImbalanceCount = 0\n\nval shuttingDownBrokerIds = mutable.Set.empty[Int]\n\nprivate val liveBrokers = mutable.Set.empty[Broker]\n\nprivate val liveBrokerEpochs = mutable.Map.empty[Int, Long]\n\nvar epoch: Int = KafkaController.InitialControllerEpoch\n\nvar epochZkVersion: Int = KafkaController.InitialControllerEpochZkVersion\n\nval allTopics = mutable.Set.empty[String]\n\n// \u8bb0\u5f55 topic \u5bf9\u5e94\u7684 partition \u5bf9\u5e94\u7684 replicaAssignments\nval partitionAssignments = mutable.Map.empty[String, mutable.Map[Int, ReplicaAssignment]]\n\n// \u8bb0\u5f55 topic partition \u4e0e\u5176\u5bf9\u5e94\u7684 leader isr\nprivate val partitionLeadershipInfo = mutable.Map.empty[TopicPartition, LeaderIsrAndControllerEpoch]\n\nval partitionsBeingReassigned = mutable.Set.empty[TopicPartition]\n\n// \u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 \u72b6\u6001\nval partitionStates = mutable.Map.empty[TopicPartition, PartitionState]\n\n// \u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 replica \u72b6\u6001\nval replicaStates = mutable.Map.empty[PartitionAndReplica, ReplicaState]\n\nval replicasOnOfflineDirs = mutable.Map.empty[Int, Set[TopicPartition]]\n\nval topicsToBeDeleted = mutable.Set.empty[String]\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/2-ReplicaStateMachine/","title":"1.3.2.2 ReplicaStateMachine","text":"<p>ReplicaStateMachine \u63d0\u4f9b\u63a5\u53e3\uff0c\u8d1f\u8d23\u5904\u7406 replica \u72b6\u6001\u53d8\u66f4\u3002replica \u72b6\u6001\u7f13\u5b58\u5728 ControllerContext partitionAssignments \u4e2d\uff0c\u72b6\u6001\u53d8\u66f4\u8fc7\u7a0b\u9700\u8981\u4e0e zookeeper \u4ee5\u53ca broker \u4ea4\u4e92\u3002</p> <p>replica \u6240\u6709\u72b6\u6001\uff1a</p> <ol> <li><code>NewReplica</code>: controller \u5728 partition reassignment \u671f\u95f4\u53ef\u4ee5\u521b\u5efa\u65b0\u7684 replicas\u3002\u5728\u6b64\u72b6\u6001\u4e0b\uff0creplica \u53ea\u80fd\u6210\u4e3a follower\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f <code>NonExistentReplica</code></li> <li><code>OnlineReplica</code>: \u5f53 replica \u6210\u4e3a\u5176 partition \u7684 assigned replicas \u7684\u4e00\u90e8\u5206\uff0creplica \u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u5728\u6b64\u72b6\u6001\u4e0b\uff0creplica \u53ef\u4ee5\u6210\u4e3a leader \u6216\u8005 follower\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u6709 <code>NewReplica</code>, <code>OnlineReplica</code>, <code>OfflineReplica</code></li> <li><code>OfflineReplica</code>: \u5982\u679c replica \u6240\u5904 broker \u4e0b\u7ebf\uff0creplica \u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f <code>NewReplica</code>, <code>OnlineReplica</code></li> <li><code>ReplicaDeletionStarted</code>: \u5982\u679c replica \u5f00\u59cb\u5220\u9664\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f <code>OfflineReplica</code></li> <li><code>ReplicaDeletionSuccessful</code>: \u5982\u679c delete replica \u8bf7\u6c42\u7684\u54cd\u5e94\u6ca1\u6709\u9519\u8bef\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f <code>ReplicaDeletionStarted</code></li> <li><code>ReplicaDeletionIneligible</code>: \u5982\u679c replica \u5220\u9664\u5931\u8d25\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f <code>ReplicaDeletionStarted</code>, <code>OfflineReplica</code></li> <li><code>NonExistentReplica</code>: \u5982\u679c replica \u5220\u9664\u6210\u529f\uff0creplica \u4f1a\u5904\u4e8e\u8fd9\u4e2a\u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f <code>ReplicaDeletionSuccessful</code></li> </ol> <pre><code>sealed trait ReplicaState {\n  def state: Byte\n  def validPreviousStates: Set[ReplicaState]\n}\n\ncase object NewReplica extends ReplicaState {\n  val state: Byte = 1\n  val validPreviousStates: Set[ReplicaState] = Set(NonExistentReplica)\n}\n\ncase object OnlineReplica extends ReplicaState {\n  val state: Byte = 2\n  val validPreviousStates: Set[ReplicaState] = Set(NewReplica, OnlineReplica, OfflineReplica, ReplicaDeletionIneligible)\n}\n\ncase object OfflineReplica extends ReplicaState {\n  val state: Byte = 3\n  val validPreviousStates: Set[ReplicaState] = Set(NewReplica, OnlineReplica, OfflineReplica, ReplicaDeletionIneligible)\n}\n\ncase object ReplicaDeletionStarted extends ReplicaState {\n  val state: Byte = 4\n  val validPreviousStates: Set[ReplicaState] = Set(OfflineReplica)\n}\n\ncase object ReplicaDeletionSuccessful extends ReplicaState {\n  val state: Byte = 5\n  val validPreviousStates: Set[ReplicaState] = Set(ReplicaDeletionStarted)\n}\n\ncase object ReplicaDeletionIneligible extends ReplicaState {\n  val state: Byte = 6\n  val validPreviousStates: Set[ReplicaState] = Set(OfflineReplica, ReplicaDeletionStarted)\n}\n\ncase object NonExistentReplica extends ReplicaState {\n  val state: Byte = 7\n  val validPreviousStates: Set[ReplicaState] = Set(ReplicaDeletionSuccessful)\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/3-PartitionStateMachine/","title":"1.3.2.3 PartitionStateMachine","text":"<p>partition \u6240\u6709\u72b6\u6001\uff1a</p> <ol> <li><code>NonExistentPartition</code>: \u8fd9\u4e2a\u72b6\u6001\u8868\u793a parition \u6ca1\u6709\u88ab\u521b\u5efa\uff0c\u6216\u8005\u66fe\u521b\u5efa\u8fc7\u4f46\u88ab\u5220\u9664\u4e86\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f <code>OfflinePartition</code></li> <li><code>NewPartition</code>: \u521b\u5efa\u4e4b\u540e\uff0cpartition \u5904\u4e8e <code>NewPartition</code> \u72b6\u6001\u3002\u5728\u6b64\u72b6\u6001\u4e0b\uff0cpartition \u5e94\u8be5\u6709 assigned replicas\uff0c\u4f46\u662f\u6ca1\u6709 leader \u548c isr\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f <code>NonExistentPartition</code></li> <li><code>OnlinePartition</code>: \u5f53 partition \u7684 leader \u9009\u4e3e\u51fa\u6765\uff0c\u5176\u5904\u4e8e <code>OnlinePartition</code> \u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f <code>NewPartition</code>, <code>OfflinePartition</code></li> <li><code>OfflinePartition</code>: \u5982\u679c\u7ecf\u8fc7\u6210\u529f\u7684 leader \u9009\u4e3e\u4e4b\u540e\uff0cleader \u56e0\u4e3a\u4e00\u4e9b\u539f\u56e0\u4e0b\u7ebf\u4e86\uff0cpartition \u6b64\u65f6\u5904\u4e8e <code>OfflinePartition</code> \u72b6\u6001\u3002\u6709\u6548\u7684\u524d\u7f6e\u72b6\u6001\u662f <code>NewPartition</code>, <code>OnlinePartition</code></li> </ol> <pre><code>sealed trait PartitionState {\n  def state: Byte\n  def validPreviousStates: Set[PartitionState]\n}\n\ncase object NewPartition extends PartitionState {\n  val state: Byte = 0\n  val validPreviousStates: Set[PartitionState] = Set(NonExistentPartition)\n}\n\ncase object OnlinePartition extends PartitionState {\n  val state: Byte = 1\n  val validPreviousStates: Set[PartitionState] = Set(NewPartition, OnlinePartition, OfflinePartition)\n}\n\ncase object OfflinePartition extends PartitionState {\n  val state: Byte = 2\n  val validPreviousStates: Set[PartitionState] = Set(NewPartition, OnlinePartition, OfflinePartition)\n}\n\ncase object NonExistentPartition extends PartitionState {\n  val state: Byte = 3\n  val validPreviousStates: Set[PartitionState] = Set(OfflinePartition)\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/3-KafkaController/2-controller%E7%BB%84%E4%BB%B6/4-ControllerChannelManager/","title":"1.3.2.4 ControllerChannelManager","text":"<p>Controller \u4f1a\u5411 broker \u53d1\u9001 3 \u79cd\u7c7b\u578b\u7684\u8bf7\u6c42\uff1a</p> <ul> <li>LeaderAndIsrRequest</li> <li>UpdateMetadataRequest</li> <li>StopReplicaRequest</li> </ul> <p>ControllerChannelManager \u7ef4\u62a4\u4e0e broker \u4e4b\u95f4\u7684\u8fde\u63a5</p> <pre><code>protected val brokerStateInfo = new HashMap[Int, ControllerBrokerStateInfo]\nprivate val brokerLock = new Object\n</code></pre> <p>ControllerBrokerStateInfo \u5b9a\u4e49\u5982\u4e0b\uff1a</p> <pre><code>case class ControllerBrokerStateInfo(networkClient: NetworkClient,\n                                     brokerNode: Node,\n                                     messageQueue: BlockingQueue[QueueItem],\n                                     requestSendThread: RequestSendThread,\n                                     queueSizeGauge: Gauge[Int],\n                                     requestRateAndTimeMetrics: Timer,\n                                     reconfigurableChannelBuilder: Option[Reconfigurable])\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/","title":"1.4 ReplicaManager","text":"<p>\u6bcf\u4e2a broker \u8fdb\u7a0b\u6709 1 \u4e2a ReplicaManager \u5b9e\u4f8b\uff0cReplicaManager \u8d1f\u8d23\uff1a</p> <ol> <li>\u5b9a\u65f6\u68c0\u67e5\u526f\u672c\u662f\u5426\u843d\u540e\uff0c\u5982\u679c\u843d\u540e\uff0c\u9700\u8981\u901a\u77e5 controller \u5c06\u5176\u79fb\u51fa isr</li> <li>\u63d0\u4f9b <code>fetchMessages()</code> \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea consumer \u6216\u8005 follower \u7684 <code>FetchRequest</code> \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 <code>fetchMessages()</code> \u65b9\u6cd5\uff0c<code>fetchMessages()</code> \u4f1a\u66f4\u65b0\u526f\u672c\u72b6\u6001\uff0c\u5fc5\u8981\u65f6\u901a\u77e5 controller \u5c06\u526f\u672c\u52a0\u5165 isr</li> <li>\u63d0\u4f9b <code>stopReplicas()</code> \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea controller \u7684 <code>StopReplicaRequest</code> \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 <code>stopReplicas()</code> \u65b9\u6cd5</li> <li>\u63d0\u4f9b <code>becomeLeaderOrFollower()</code> \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea controller \u7684 <code>LeaderAndIsrRequest</code> \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 <code>becomeLeaderOrFollower()</code> \u65b9\u6cd5<ol> <li>\u5bf9\u4e8e\u6210\u4e3a leader \u7684\u672c\u5730 replica\uff0c\u8c03\u7528 <code>makeLeaders()</code></li> <li>\u5bf9\u4e8e\u6210\u4e3a follower \u7684\u672c\u5730 replica\uff0c\u8c03\u7528 <code>makeFollowers()</code>\uff0c\u8fd9\u91cc\u4f1a\u521b\u5efa\u5e76\u542f\u52a8 fetcherThread</li> </ol> </li> <li>\u63d0\u4f9b <code>maybeUpdateMetadataCache()</code> \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea controller \u7684 <code>UpdateMetadataRequest</code> \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 <code>maybeUpdateMetadataCache()</code> \u65b9\u6cd5</li> <li>\u63d0\u4f9b <code>appendRecords()</code> \u65b9\u6cd5\uff0cbroker \u5904\u7406\u6765\u81ea producer \u7684 <code>ProduceRequest</code> \u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 <code>appendRecords()</code> \u65b9\u6cd5</li> <li><code>ListOffsetRequest</code><ol> <li>handleListOffsetRequestV0:  \u63d0\u4f9b <code>legacyFetchOffsetsForTimestamp</code> \u65b9\u6cd5\uff0cbroker \u5904\u7406 <code>ListOffsetRequestV0</code> \u8bf7\u6c42\u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 <code>legacyFetchOffsetsForTimestamp</code> \u65b9\u6cd5</li> <li>handleListOffsetRequestV1AndAbove:  \u63d0\u4f9b <code>fetchOffsetForTimestamp</code> \u65b9\u6cd5\uff0cbroker \u5904\u7406 <code>ListOffsetRequestV1</code> \u8bf7\u6c42\u65f6\u9700\u8981\u8c03\u7528 replica manager \u7684 <code>fetchOffsetForTimestamp</code> \u65b9\u6cd5</li> </ol> </li> <li>\u63d0\u4f9b <code>deleteRecords()</code> \u65b9\u6cd5\uff0c\u5904\u7406 <code>DeleteRecordsRequest</code> \u8bf7\u6c42</li> </ol>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/1-replica%20manager%20%E5%90%AF%E5%8A%A8/","title":"1.4.1 replica manager \u542f\u52a8","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/1-replica%20manager%20%E5%90%AF%E5%8A%A8/#_1","title":"\u521b\u5efa","text":"<p>replica manager \u5728 <code>kafkaServer.startup()</code> \u4e2d\u521d\u59cb\u5316\u5e76\u542f\u52a8\uff1a</p> <pre><code>  /**\n   * Start up API for bringing up a single instance of the Kafka server.\n   * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers\n   */\n  def startup(): Unit = {\n    try {\n      info(\"starting\")\n\n      if (isShuttingDown.get)\n        throw new IllegalStateException(\"Kafka server is still shutting down, cannot re-start!\")\n\n      if (startupComplete.get)\n        return\n\n      val canStartup = isStartingUp.compareAndSet(false, true)\n      if (canStartup) {\n\n        // ...\n\n        /* start replica manager */\n        brokerToControllerChannelManager = new BrokerToControllerChannelManagerImpl(metadataCache, time, metrics, config, threadNamePrefix)\n        replicaManager = createReplicaManager(isShuttingDown)\n        replicaManager.startup()\n        brokerToControllerChannelManager.start()\n\n        // ...\n</code></pre> <pre><code>  protected def createReplicaManager(isShuttingDown: AtomicBoolean): ReplicaManager = {\n    val alterIsrManager = new AlterIsrManagerImpl(brokerToControllerChannelManager, kafkaScheduler,\n      time, config.brokerId, () =&gt; kafkaController.brokerEpoch)\n    new ReplicaManager(config, metrics, time, zkClient, kafkaScheduler, logManager, isShuttingDown, quotaManagers,\n      brokerTopicStats, metadataCache, logDirFailureChannel, alterIsrManager)\n  }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/1-replica%20manager%20%E5%90%AF%E5%8A%A8/#startup","title":"startup","text":"<p><code>ReplicaManager.startup()</code> \u65b9\u6cd5\u4f1a\u542f\u52a8\u4ee5\u4e0b\u5b9a\u65f6\u4efb\u52a1\uff1a</p> <ol> <li>isr-expiration\uff1a\u5b9a\u65f6\u68c0\u67e5 isr \u5217\u8868\u4e2d\u662f\u5426\u6709 replica \u9700\u8981\u88ab\u4ece isr \u5217\u8868\u4e2d\u79fb\u9664\uff0c\u8fd9\u91cc\u4f1a\u5bf9 isr \u53d8\u5316\u7684 topicPartition \u8fdb\u884c\u8bb0\u5f55\uff0c\u5206\u4e3a 2 \u79cd\u60c5\u51b5\uff1a<ol> <li>\u5982\u679c api version <code>&gt;=</code> KAFKA_2_7_IV2\uff0c\u8bb0\u5f55\u5728 alterIsrManager \u7684 unsentIsrUpdates \u4e2d</li> <li>\u5426\u5219\u8bb0\u5f55\u5728\u81ea\u8eab\u7684 isrChangeSet \u4e2d</li> </ol> </li> <li>isr \u7684\u53d8\u5316\u9700\u8981\u901a\u77e5\u5230 controller\uff0c\u4e0a\u4e00\u6b65\u7684\u5b9a\u65f6\u4efb\u52a1\u53ea\u662f\u8bb0\u5f55\uff0c\u8fd9\u91cc\u4f1a\u6839\u636e\u96c6\u7fa4\u652f\u6301\u7684 API version \u5206\u522b\u901a\u8fc7 AlterIsrRequest \u8bf7\u6c42\u6216\u8005 zookeeper \u5411 controller \u8fdb\u884c\u901a\u77e5:<ol> <li>send-alter-isr: \u5982\u679c api version <code>&gt;=</code> KAFKA_2_7_IV2\uff0c\u542f\u52a8 alterIsrManager\uff0calterIsrManager \u540c\u6837\u542f\u52a8\u5b9a\u65f6\u4efb\u52a1\uff0c\u5b9a\u671f\u5411 controller \u53d1\u9001\u672a\u5b8c\u6210\u7684 AlterIsrRequest \u8bf7\u6c42</li> <li>isr-change-propagation: \u5426\u5219\u5b9a\u65f6\u68c0\u67e5 isr \u662f\u5426\u9700\u8981\u53d8\u52a8\uff0c\u521b\u5efa <code>/isr_change_notification/isr_change_&lt;&gt;</code> zookeeper \u8282\u70b9\uff0c\u89e6\u53d1 controller \u52a8\u4f5c</li> </ol> </li> <li>shutdown-idle-replica-alter-log-dirs-thread</li> </ol> <pre><code>  def startup(): Unit = {\n    // start ISR expiration thread\n    // A follower can lag behind leader for up to config.replicaLagTimeMaxMs x 1.5 before it is removed from ISR\n    scheduler.schedule(\"isr-expiration\", maybeShrinkIsr _, period = config.replicaLagTimeMaxMs / 2, unit = TimeUnit.MILLISECONDS)\n    // If using AlterIsr, we don't need the znode ISR propagation\n    if (!config.interBrokerProtocolVersion.isAlterIsrSupported) {\n      scheduler.schedule(\"isr-change-propagation\", maybePropagateIsrChanges _,\n        period = isrChangeNotificationConfig.checkIntervalMs, unit = TimeUnit.MILLISECONDS)\n    } else {\n      alterIsrManager.start()\n    }\n    scheduler.schedule(\"shutdown-idle-replica-alter-log-dirs-thread\", shutdownIdleReplicaAlterLogDirsThread _, period = 10000L, unit = TimeUnit.MILLISECONDS)\n\n    // If inter-broker protocol (IBP) &lt; 1.0, the controller will send LeaderAndIsrRequest V0 which does not include isNew field.\n    // In this case, the broker receiving the request cannot determine whether it is safe to create a partition if a log directory has failed.\n    // Thus, we choose to halt the broker on any log diretory failure if IBP &lt; 1.0\n    val haltBrokerOnFailure = config.interBrokerProtocolVersion &lt; KAFKA_1_0_IV0\n    logDirFailureHandler = new LogDirFailureHandler(\"LogDirFailureHandler\", haltBrokerOnFailure)\n    logDirFailureHandler.start()\n  }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/2-isr/","title":"isr","text":"<p>isr shrik \u548c isr expand \u90fd\u662f leader \u53d1\u8d77\u7684\u3002</p> <p>leader \u6709\u5b9a\u65f6\u4efb\u52a1\u68c0\u67e5 isr \u6bcf\u4e2a replica \u662f\u5426\u53d1\u751f\u843d\u540e\uff0creplica fetch \u8bf7\u6c42\u5230\u6765\u65f6\uff0c\u68c0\u67e5\u662f\u5426\u8981\u5c06\u8be5 replica \u52a0\u5165 isr\u3002</p> <p>zk \u4e0a\u8be5 partition \u7684 state \u4fe1\u606f\u4e5f\u7531 leader \u66f4\u65b0\u3002</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/2-isr/#isr-shrink","title":"isr shrink","text":"<p>\u5b9a\u65f6\u4efb\u52a1 maybeShrinkIsr \u4f1a\u904d\u5386 topic partition\uff0c\u8fc7\u6ee4\u51fa online \u7684 partition\uff0c\u5bf9 partition \u5b9e\u4f8b\u8c03\u7528\u5176 <code>maybeShrinkIsr()</code> \u65b9\u6cd5</p> <pre><code>  private def maybeShrinkIsr(): Unit = {\n    trace(\"Evaluating ISR list of partitions to see which replicas can be removed from the ISR\")\n\n    // Shrink ISRs for non offline partitions\n    allPartitions.keys.foreach { topicPartition =&gt;\n      nonOfflinePartition(topicPartition).foreach(_.maybeShrinkIsr())\n    }\n  }\n</code></pre> <p><code>Partition.maybeShrinkIsr()</code> \u5982\u4e0b</p> <pre><code>  def maybeShrinkIsr(): Unit = {\n    val needsIsrUpdate = !isrState.isInflight &amp;&amp; inReadLock(leaderIsrUpdateLock) {\n      needsShrinkIsr()\n    }\n    val leaderHWIncremented = needsIsrUpdate &amp;&amp; inWriteLock(leaderIsrUpdateLock) {\n      leaderLogIfLocal.exists { leaderLog =&gt;\n        val outOfSyncReplicaIds = getOutOfSyncReplicas(replicaLagTimeMaxMs)\n        if (outOfSyncReplicaIds.nonEmpty) {\n          val outOfSyncReplicaLog = outOfSyncReplicaIds.map { replicaId =&gt;\n            s\"(brokerId: $replicaId, endOffset: ${getReplicaOrException(replicaId).logEndOffset})\"\n          }.mkString(\" \")\n          val newIsrLog = (isrState.isr -- outOfSyncReplicaIds).mkString(\",\")\n          info(s\"Shrinking ISR from ${isrState.isr.mkString(\",\")} to $newIsrLog. \" +\n               s\"Leader: (highWatermark: ${leaderLog.highWatermark}, endOffset: ${leaderLog.logEndOffset}). \" +\n               s\"Out of sync replicas: $outOfSyncReplicaLog.\")\n\n          shrinkIsr(outOfSyncReplicaIds)\n\n          // we may need to increment high watermark since ISR could be down to 1\n          maybeIncrementLeaderHW(leaderLog)\n        } else {\n          false\n        }\n      }\n    }\n\n    // some delayed operations may be unblocked after HW changed\n    if (leaderHWIncremented)\n      tryCompleteDelayedRequests()\n  }\n</code></pre> <p>\u5f53\u524d\u8282\u70b9\u7684 partition \u662f leader\uff0c\u83b7\u53d6 partition \u5f53\u524d\u7684 isr follower\uff0c\u8fd4\u56de\u5df2\u7ecf\u843d\u540e\u7684 follower</p> <pre><code>  /**\n   * If the follower already has the same leo as the leader, it will not be considered as out-of-sync,\n   * otherwise there are two cases that will be handled here -\n   * 1. Stuck followers: If the leo of the replica hasn't been updated for maxLagMs ms,\n   *                     the follower is stuck and should be removed from the ISR\n   * 2. Slow followers: If the replica has not read up to the leo within the last maxLagMs ms,\n   *                    then the follower is lagging and should be removed from the ISR\n   * Both these cases are handled by checking the lastCaughtUpTimeMs which represents\n   * the last time when the replica was fully caught up. If either of the above conditions\n   * is violated, that replica is considered to be out of sync\n   *\n   * If an ISR update is in-flight, we will return an empty set here\n   **/\n  def getOutOfSyncReplicas(maxLagMs: Long): Set[Int] = {\n    val current = isrState\n    if (!current.isInflight) {\n      val candidateReplicaIds = current.isr - localBrokerId\n      val currentTimeMs = time.milliseconds()\n      val leaderEndOffset = localLogOrException.logEndOffset\n      candidateReplicaIds.filter(replicaId =&gt; isFollowerOutOfSync(replicaId, leaderEndOffset, currentTimeMs, maxLagMs))\n    } else {\n      Set.empty\n    }\n  }\n</code></pre> <p>\u5982\u679c follower \u7684 leo \u4e0d\u7b49\u4e8e leader \u7684 leo\uff0c\u4e14 follower \u7684 lastCaughtUpTimeMs \u8ddd\u5f53\u524d\u65f6\u95f4\u8d85\u8fc7 <code>replica.lag.time.max.ms</code>\uff0c\u5219\u8ba4\u4e3a\u5176\u843d\u540e</p> <pre><code>  private def isFollowerOutOfSync(replicaId: Int,\n                                  leaderEndOffset: Long,\n                                  currentTimeMs: Long,\n                                  maxLagMs: Long): Boolean = {\n    val followerReplica = getReplicaOrException(replicaId)\n    followerReplica.logEndOffset != leaderEndOffset &amp;&amp;\n      (currentTimeMs - followerReplica.lastCaughtUpTimeMs) &gt; maxLagMs\n  }\n</code></pre> <p><code>Partition.shrinkIsr()</code></p> <pre><code>  private[cluster] def shrinkIsr(outOfSyncReplicas: Set[Int]): Unit = {\n    if (useAlterIsr) {\n      shrinkIsrWithAlterIsr(outOfSyncReplicas)\n    } else {\n      shrinkIsrWithZk(isrState.isr -- outOfSyncReplicas)\n    }\n  }\n\n  private def shrinkIsrWithAlterIsr(outOfSyncReplicas: Set[Int]): Unit = {\n    // This is called from maybeShrinkIsr which holds the ISR write lock\n    if (!isrState.isInflight) {\n      // When shrinking the ISR, we cannot assume that the update will succeed as this could erroneously advance the HW\n      // We update pendingInSyncReplicaIds here simply to prevent any further ISR updates from occurring until we get\n      // the next LeaderAndIsr\n      sendAlterIsrRequest(PendingShrinkIsr(isrState.isr, outOfSyncReplicas))\n    } else {\n      trace(s\"ISR update in-flight, not removing out-of-sync replicas $outOfSyncReplicas\")\n    }\n  }\n\n  private def shrinkIsrWithZk(newIsr: Set[Int]): Unit = {\n    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n    val zkVersionOpt = stateStore.shrinkIsr(controllerEpoch, newLeaderAndIsr)\n    if (zkVersionOpt.isDefined) {\n      isrChangeListener.markShrink()\n    }\n    maybeUpdateIsrAndVersionWithZk(newIsr, zkVersionOpt)\n  }\n</code></pre> <p>\u8fd9\u91cc\u4e5f\u5206 2 \u79cd\u60c5\u51b5\uff1a</p> <ol> <li>\u5982\u679c api version <code>&gt;=</code> KAFKA_2_7_IV2\uff0c\u5373 <code>useAlterIsr</code>\uff0c\u5411 alterIsrManager \u589e\u52a0 AlterIsrRequest \u8bb0\u5f55</li> <li>\u5426\u5219\uff0c\u5728 ReplicaManager \u7684 isrChangeSet \u4e2d\u8bb0\u5f55 isr \u53d8\u5316\u7684 topicPartition</li> </ol>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/2-isr/#isr-expand-fetchmessages","title":"isr expand / fetchMessages","text":"<p>topic partition leader \u6240\u5728\u8282\u70b9\uff0c\u9700\u8981\u8bb0\u5f55\u6bcf\u4e2a replica \u7684 logEndOffset\uff0c\u6bcf\u4e2a replica \u90fd\u8981\u8bb0\u5f55\u81ea\u8eab\u7684 HW</p> <pre><code>  /**\n   * Fetch messages from a replica, and wait until enough data can be fetched and return;\n   * the callback function will be triggered either when timeout or required fetch info is satisfied.\n   * Consumers may fetch from any replica, but followers can only fetch from the leader.\n   */\n  def fetchMessages(timeout: Long,\n                    replicaId: Int,\n                    fetchMinBytes: Int,\n                    fetchMaxBytes: Int,\n                    hardMaxBytesLimit: Boolean,\n                    fetchInfos: Seq[(TopicPartition, PartitionData)],\n                    quota: ReplicaQuota,\n                    responseCallback: Seq[(TopicPartition, FetchPartitionData)] =&gt; Unit,\n                    isolationLevel: IsolationLevel,\n                    clientMetadata: Option[ClientMetadata]): Unit = {\n    val isFromFollower = Request.isValidBrokerId(replicaId)\n    val isFromConsumer = !(isFromFollower || replicaId == Request.FutureLocalReplicaId)\n    val fetchIsolation = if (!isFromConsumer)\n      FetchLogEnd\n    else if (isolationLevel == IsolationLevel.READ_COMMITTED)\n      FetchTxnCommitted\n    else\n      FetchHighWatermark\n\n    // Restrict fetching to leader if request is from follower or from a client with older version (no ClientMetadata)\n    val fetchOnlyFromLeader = isFromFollower || (isFromConsumer &amp;&amp; clientMetadata.isEmpty)\n    def readFromLog(): Seq[(TopicPartition, LogReadResult)] = {\n      val result = readFromLocalLog(\n        replicaId = replicaId,\n        fetchOnlyFromLeader = fetchOnlyFromLeader,\n        fetchIsolation = fetchIsolation,\n        fetchMaxBytes = fetchMaxBytes,\n        hardMaxBytesLimit = hardMaxBytesLimit,\n        readPartitionInfo = fetchInfos,\n        quota = quota,\n        clientMetadata = clientMetadata)\n      if (isFromFollower) updateFollowerFetchState(replicaId, result)\n      else result\n    }\n\n    val logReadResults = readFromLog()\n\n    // check if this fetch request can be satisfied right away\n    var bytesReadable: Long = 0\n    var errorReadingData = false\n    var hasDivergingEpoch = false\n    val logReadResultMap = new mutable.HashMap[TopicPartition, LogReadResult]\n    logReadResults.foreach { case (topicPartition, logReadResult) =&gt;\n      brokerTopicStats.topicStats(topicPartition.topic).totalFetchRequestRate.mark()\n      brokerTopicStats.allTopicsStats.totalFetchRequestRate.mark()\n\n      if (logReadResult.error != Errors.NONE)\n        errorReadingData = true\n      if (logReadResult.divergingEpoch.nonEmpty)\n        hasDivergingEpoch = true\n      bytesReadable = bytesReadable + logReadResult.info.records.sizeInBytes\n      logReadResultMap.put(topicPartition, logReadResult)\n    }\n\n    // respond immediately if 1) fetch request does not want to wait\n    //                        2) fetch request does not require any data\n    //                        3) has enough data to respond\n    //                        4) some error happens while reading data\n    //                        5) we found a diverging epoch\n    if (timeout &lt;= 0 || fetchInfos.isEmpty || bytesReadable &gt;= fetchMinBytes || errorReadingData || hasDivergingEpoch) {\n      val fetchPartitionData = logReadResults.map { case (tp, result) =&gt;\n        val isReassignmentFetch = isFromFollower &amp;&amp; isAddingReplica(tp, replicaId)\n        tp -&gt; FetchPartitionData(\n          result.error,\n          result.highWatermark,\n          result.leaderLogStartOffset,\n          result.info.records,\n          result.divergingEpoch,\n          result.lastStableOffset,\n          result.info.abortedTransactions,\n          result.preferredReadReplica,\n          isReassignmentFetch)\n      }\n      responseCallback(fetchPartitionData)\n    } else {\n      // construct the fetch results from the read results\n      val fetchPartitionStatus = new mutable.ArrayBuffer[(TopicPartition, FetchPartitionStatus)]\n      fetchInfos.foreach { case (topicPartition, partitionData) =&gt;\n        logReadResultMap.get(topicPartition).foreach(logReadResult =&gt; {\n          val logOffsetMetadata = logReadResult.info.fetchOffsetMetadata\n          fetchPartitionStatus += (topicPartition -&gt; FetchPartitionStatus(logOffsetMetadata, partitionData))\n        })\n      }\n      val fetchMetadata: SFetchMetadata = SFetchMetadata(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit,\n        fetchOnlyFromLeader, fetchIsolation, isFromFollower, replicaId, fetchPartitionStatus)\n      val delayedFetch = new DelayedFetch(timeout, fetchMetadata, this, quota, clientMetadata,\n        responseCallback)\n\n      // create a list of (topic, partition) pairs to use as keys for this delayed fetch operation\n      val delayedFetchKeys = fetchPartitionStatus.map { case (tp, _) =&gt; TopicPartitionOperationKey(tp) }\n\n      // try to complete the request immediately, otherwise put it into the purgatory;\n      // this is because while the delayed fetch operation is being created, new requests\n      // may arrive and hence make this operation completable.\n      delayedFetchPurgatory.tryCompleteElseWatch(delayedFetch, delayedFetchKeys)\n    }\n  }\n</code></pre> <p>updateFollowerFetchState</p> <pre><code>  /**\n   * Update the follower's fetch state on the leader based on the last fetch request and update `readResult`.\n   * If the follower replica is not recognized to be one of the assigned replicas, do not update\n   * `readResult` so that log start/end offset and high watermark is consistent with\n   * records in fetch response. Log start/end offset and high watermark may change not only due to\n   * this fetch request, e.g., rolling new log segment and removing old log segment may move log\n   * start offset further than the last offset in the fetched records. The followers will get the\n   * updated leader's state in the next fetch response.\n   */\n  private def updateFollowerFetchState(followerId: Int,\n                                       readResults: Seq[(TopicPartition, LogReadResult)]): Seq[(TopicPartition, LogReadResult)] = {\n    readResults.map { case (topicPartition, readResult) =&gt;\n      val updatedReadResult = if (readResult.error != Errors.NONE) {\n        debug(s\"Skipping update of fetch state for follower $followerId since the \" +\n          s\"log read returned error ${readResult.error}\")\n        readResult\n      } else {\n        nonOfflinePartition(topicPartition) match {\n          case Some(partition) =&gt;\n            if (partition.updateFollowerFetchState(followerId,\n              followerFetchOffsetMetadata = readResult.info.fetchOffsetMetadata,\n              followerStartOffset = readResult.followerLogStartOffset,\n              followerFetchTimeMs = readResult.fetchTimeMs,\n              leaderEndOffset = readResult.leaderLogEndOffset)) {\n              readResult\n            } else {\n              warn(s\"Leader $localBrokerId failed to record follower $followerId's position \" +\n                s\"${readResult.info.fetchOffsetMetadata.messageOffset}, and last sent HW since the replica \" +\n                s\"is not recognized to be one of the assigned replicas ${partition.assignmentState.replicas.mkString(\",\")} \" +\n                s\"for partition $topicPartition. Empty records will be returned for this partition.\")\n              readResult.withEmptyFetchInfo\n            }\n          case None =&gt;\n            warn(s\"While recording the replica LEO, the partition $topicPartition hasn't been created.\")\n            readResult\n        }\n      }\n      topicPartition -&gt; updatedReadResult\n    }\n  }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/3-become-leader-or-follower/","title":"1.4.3 LeaderAndIsrRequest","text":"<p><code>becomeLeaderOrFollower</code> \u5904\u7406 LeaderAndIsrRequest \u8bf7\u6c42\u3002</p> <p>ReplicaManager \u7ef4\u62a4\u7684 <code>allPartitions</code> \u4e2d\u7684 topic partition \u8bb0\u5f55\u662f\u5728\u8fd9\u91cc\u6dfb\u52a0\u7684\u3002</p> <ul> <li>broker \u542f\u52a8\u540e\uff0ccontroller \u4f1a\u5411 broker \u53d1\u9001 <code>LeaderAndIsrRequest</code> \u8bf7\u6c42</li> <li>isr \u53d8\u5316\u65f6\uff0c</li> </ul>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/3-become-leader-or-follower/#leader-epoch","title":"leader epoch","text":"<p>partition leader \u6bcf\u5207\u6362\u4e00\u6b21\uff0cepoch \u589e\u52a0 1\u3002</p> <p>leader-epoch-checkpoint \u8bb0\u5f55\u6bcf\u6b21 leader \u5207\u6362\u540e\u7684 epoch \u4ee5\u53ca\u8be5\u6b21\u5207\u6362\u540e leader \u5199\u5165\u7b2c\u4e00\u6761\u6d88\u606f\u7684 offset\uff08\u8be5 partition \u7684 log end offset\uff09\u3002</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/4-replica-fetcher-manager/","title":"replica fetcher manager","text":"<p><code>ReplicaManager</code> \u4f1a\u521b\u5efa\u4e00\u4e2a <code>ReplicaFetcherManager</code> \u5b9e\u4f8b\u7ba1\u7406 <code>ReplicaFetcherThread</code> \u7ebf\u7a0b\u3002</p> <p><code>ReplicaFetcherThread</code> \u7ebf\u7a0b\u6309\u62c9\u53d6\u6e90 broker id \u5206\u7c7b\uff0c\u5bf9\u6bcf\u4e2a\u6e90 broker \u53ef\u4ee5\u521b\u5efa numFetchersPerBroker \u4e2a Fetcher \u7ebf\u7a0b\u3002</p> <ul> <li>leader broker 1<ul> <li>fetcher 1 (topic partition 1, topic partition 2)</li> <li>fetcher 2 ()</li> </ul> </li> <li>leader broker 2<ul> <li>fetcher 1 ()</li> <li>fetcher 2 ()</li> </ul> </li> </ul> <pre><code>  private[server] def getFetcherId(topicPartition: TopicPartition): Int = {\n    lock synchronized {\n      Utils.abs(31 * topicPartition.topic.hashCode() + topicPartition.partition) % numFetchersPerBroker\n    }\n  }\n</code></pre> <pre><code>  def addFetcherForPartitions(partitionAndOffsets: Map[TopicPartition, InitialFetchState]): Unit = {\n    lock synchronized {\n      val partitionsPerFetcher = partitionAndOffsets.groupBy { case (topicPartition, brokerAndInitialFetchOffset) =&gt;\n        BrokerAndFetcherId(brokerAndInitialFetchOffset.leader, getFetcherId(topicPartition))\n      }\n\n      def addAndStartFetcherThread(brokerAndFetcherId: BrokerAndFetcherId,\n                                   brokerIdAndFetcherId: BrokerIdAndFetcherId): T = {\n        val fetcherThread = createFetcherThread(brokerAndFetcherId.fetcherId, brokerAndFetcherId.broker)\n        fetcherThreadMap.put(brokerIdAndFetcherId, fetcherThread)\n        fetcherThread.start()\n        fetcherThread\n      }\n\n      for ((brokerAndFetcherId, initialFetchOffsets) &lt;- partitionsPerFetcher) {\n        val brokerIdAndFetcherId = BrokerIdAndFetcherId(brokerAndFetcherId.broker.id, brokerAndFetcherId.fetcherId)\n        val fetcherThread = fetcherThreadMap.get(brokerIdAndFetcherId) match {\n          case Some(currentFetcherThread) if currentFetcherThread.sourceBroker == brokerAndFetcherId.broker =&gt;\n            // reuse the fetcher thread\n            currentFetcherThread\n          case Some(f) =&gt;\n            f.shutdown()\n            addAndStartFetcherThread(brokerAndFetcherId, brokerIdAndFetcherId)\n          case None =&gt;\n            addAndStartFetcherThread(brokerAndFetcherId, brokerIdAndFetcherId)\n        }\n\n        val initialOffsetAndEpochs = initialFetchOffsets.map { case (tp, brokerAndInitOffset) =&gt;\n          tp -&gt; OffsetAndEpoch(brokerAndInitOffset.initOffset, brokerAndInitOffset.currentLeaderEpoch)\n        }\n\n        addPartitionsToFetcherThread(fetcherThread, initialOffsetAndEpochs)\n      }\n    }\n  }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/4-replica-fetcher-manager/#replicafetcherthread","title":"ReplicaFetcherThread","text":"<p><code>processPartitionData()</code> \u5c06\u6570\u636e\u5199\u5165\u5b9e\u9645\u7684 log \u6587\u4ef6\uff0c\u5e76\u4e14\u5c1d\u8bd5\u66f4\u65b0\u81ea\u8eab\u7684 high water mark \u4e0e log start offset\u3002</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/4-replica-fetcher-manager/#_1","title":"\u9650\u6d41\u5b9e\u73b0","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/4-replica-fetcher-manager/#replicaquota-replicationquotamanager","title":"ReplicaQuota - ReplicationQuotaManager","text":"<pre><code>trait ReplicaQuota {\n  // \u8bb0\u5f55\u6d41\u91cf\n  def record(value: Long): Unit\n\n  // topicPartition \u662f\u5426\u9650\u6d41\n  def isThrottled(topicPartition: TopicPartition): Boolean\n\n  // \u8bbe\u7f6e\u7684\u9650\u6d41\u662f\u5426\u8017\u5c3d\n  def isQuotaExceeded: Boolean\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/4-replica-fetcher-manager/#follower","title":"follower \u9650\u6d41\u5b9e\u73b0","text":"<p><code>ReplicaFetcherManager</code> \u901a\u8fc7 <code>createFetcherThread()</code> \u521b\u5efa <code>ReplicaFetcherThread</code> \u5b9e\u4f8b\uff0c\u4f1a\u5411\u5176\u4f20\u9012\u4e00\u4e2a <code>quotaManager: ReplicationQuotaManager</code></p> <p>\u5728 <code>processPartitionData()</code> \u65b9\u6cd5\u4e2d\uff0c\u8bb0\u5f55\u526f\u672c\u540c\u6b65\u6570\u636e\u5927\u5c0f</p> <pre><code>// Traffic from both in-sync and out of sync replicas are accounted for in replication quota to ensure total replication\n// traffic doesn't exceed quota.\nif (quota.isThrottled(topicPartition))\n  quota.record(records.sizeInBytes)\n</code></pre> <p>\u5728 <code>buildFetch()</code> \u65b9\u6cd5\u4e2d\uff0c\u5728\u6d41\u91cf\u8d85\u8fc7\u9650\u5236\u540e\u8fc7\u6ee4\u8bbe\u7f6e\u9650\u6d41\u7684 topicPartition</p> <pre><code>val builder = fetchSessionHandler.newBuilder(partitionMap.size, false)\npartitionMap.forKeyValue { (topicPartition, fetchState) =&gt;\n  // We will not include a replica in the fetch request if it should be throttled.\n  if (fetchState.isReadyForFetch &amp;&amp; !shouldFollowerThrottle(quota, fetchState, topicPartition)) {\n    try {\n      val logStartOffset = this.logStartOffset(topicPartition)\n      builder.add(topicPartition, new FetchRequest.PartitionData(\n        fetchState.fetchOffset, logStartOffset, fetchSize, Optional.of(fetchState.currentLeaderEpoch)))\n    } catch {\n      case _: KafkaStorageException =&gt;\n        // The replica has already been marked offline due to log directory failure and the original failure should have already been logged.\n        // This partition should be removed from ReplicaFetcherThread soon by ReplicaManager.handleLogDirFailure()\n        partitionsWithError += topicPartition\n    }\n  }\n}\n\nval fetchData = builder.build()\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/4-replica-fetcher-manager/#leader","title":"leader \u9650\u6d41\u5b9e\u73b0","text":"<p>leader \u9650\u6d41\u5728 <code>KafkaApis</code> \u7684 <code>handleFetchRequest()</code> \u65b9\u6cd5\u4e2d</p> <p>\u8bb0\u5f55\u62c9\u53d6\u6d88\u606f\u5927\u5c0f</p> <pre><code>if (fetchRequest.isFromFollower) {\n  // We've already evaluated against the quota and are good to go. Just need to record it now.\n  unconvertedFetchResponse = fetchContext.updateAndGenerateResponseData(partitions)\n  val responseSize = sizeOfThrottledPartitions(versionId, unconvertedFetchResponse, quotas.leader)\n  quotas.leader.record(responseSize)\n  trace(s\"Sending Fetch response with partitions.size=${unconvertedFetchResponse.responseData.size}, \" +\n    s\"metadata=${unconvertedFetchResponse.sessionId}\")\n  sendResponseExemptThrottle(request, createResponse(0), Some(updateConversionStats))\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/7-append-records/","title":"\u6d88\u606f\u5199\u5165","text":"<p><code>KafkaApis.handleProduceRequest()</code> \u4e2d\u6700\u7ec8\u5199\u5165\u6d88\u606f\u662f\u901a\u8fc7\u8c03\u7528 <code>replicaManager.appendRecords()</code> \u5b8c\u6210\u7684</p> <pre><code>      // call the replica manager to append messages to the replicas\n      replicaManager.appendRecords(\n        timeout = produceRequest.timeout.toLong,\n        requiredAcks = produceRequest.acks,\n        internalTopicsAllowed = internalTopicsAllowed,\n        origin = AppendOrigin.Client,\n        entriesPerPartition = authorizedRequestInfo,\n        responseCallback = sendResponseCallback,\n        recordConversionStatsCallback = processingStatsCallback)\n</code></pre> <p><code>- ReplicaManager.appendRecords()     - ReplicaManager.appendToLocalLog()         - Partition.appendRecordsToLeader()             - Log.appendAsLeader()</code></p> <p><code>ReplicaManager.appendRecords()</code></p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/8-delete-records/","title":"delete records","text":"<p>\u9700\u8981\u5c06\u8bf7\u6c42\u53d1\u9001\u5230 partition leader \u6240\u5728\u7684 broker \u4e0a\uff0c\u5e76\u4e14\u7b49\u5f85\u5220\u9664\u64cd\u4f5c\u540c\u6b65\u5230\u5176\u4ed6 partition follower \u4e0a\u3002</p> <p>\u901a\u8fc7 <code>maybeIncrementLogStartOffset()</code> \u589e\u52a0 log start offset</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/4-ReplicaManager/9-replication-quota-manager/","title":"ReplicationQuotaManager","text":"<p>\u5b9e\u73b0 <code>ReplicaQuota</code> trait</p> <pre><code>trait ReplicaQuota {\n  def record(value: Long): Unit\n  def isThrottled(topicPartition: TopicPartition): Boolean\n  def isQuotaExceeded: Boolean\n}\n</code></pre> <pre><code>// key \u4e3a topic\uff0cvalue \u4e3a partition \u5217\u8868\uff0c\u901a\u8fc7 markThrottled() \u65b9\u6cd5\u6dfb\u52a0\u8bb0\u5f55\nprivate val throttledPartitions = new ConcurrentHashMap[String, Seq[Int]]()\n\nprivate var quota: Quota = null\n\nprivate val sensorAccess = new SensorAccess(lock, metrics)\n\nprivate val rateMetricName = metrics.metricName(\"byte-rate\", replicationType.toString,\n  s\"Tracking byte-rate for ${replicationType}\")\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/5-GroupCoordinator/","title":"1.5 GroupCoordinator","text":"<p>kafka \u96c6\u7fa4\u6bcf\u4e2a broker \u90fd\u4f1a\u542f\u52a8\u4e00\u4e2a GroupCoordinator\uff0c\u8d1f\u8d23 consumer group \u4e0b member \u7684\u52a0\u5165/\u9000\u51fa/\u5fc3\u8df3\u540c\u6b65\uff0c\u4e0e offset \u8bb0\u5f55\u3002\uff08\u7ec4\u5185 partition \u5206\u914d\u7531 leader consumer \u51b3\u5b9a\uff09</p> <p>\u6bcf\u4e2a GroupCoordinator \u90fd\u4f1a\u8d1f\u8d23\u4e00\u7ec4 group\uff0c\u7531 group id \u51b3\u5b9a group \u8be5\u7531\u90a3\u4e2a GroupCoordinator \u8d1f\u8d23\uff0c\u5177\u4f53\u5206\u914d\u65b9\u6cd5\u53ef\u4ee5\u67e5\u770b <code>handleFindCoordinatorRequest()</code> \u65b9\u6cd5\u3002</p> <p>\u5b9e\u9645\u662f\u901a\u8fc7 group id \u627e\u5230 <code>__consumer_offsets</code> \u8fd9\u4e2a topic \u5bf9\u5e94\u7684 partition\uff0c\u4ee5 partition leader \u6240\u5728\u8282\u70b9\u4f5c\u4e3a\u8be5 group id \u7684 GroupCoordinator\u3002</p> <pre><code>def partitionFor(groupId: String): Int = Utils.abs(groupId.hashCode) % groupMetadataTopicPartitionCount\n\npublic static int abs(int n) {\n    return (n == Integer.MIN_VALUE) ? 0 : Math.abs(n);\n}\n</code></pre> <p><code>groupMetadataTopicPartitionCount</code> \u662f <code>offsets.topic.num.partitions</code> \u53c2\u6570\u7684\u503c\uff0c\u662f <code>__consumer_offsets</code> partition \u7684\u6570\u91cf\uff0c\u9ed8\u8ba4\u662f 50</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/5-GroupCoordinator/1-coordinator%E5%90%AF%E5%8A%A8/","title":"1.5.1 coordinator \u542f\u52a8","text":"<p>coordinator \u5728 <code>kafkaServer.startup()</code> \u4e2d\u542f\u52a8\uff1a</p> <pre><code>        /* start group coordinator */\n        // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue\n        groupCoordinator = GroupCoordinator(config, zkClient, replicaManager, Time.SYSTEM, metrics)\n        groupCoordinator.startup()\n</code></pre> <p>groupCoordinator \u7684 <code>startup()</code> \u65b9\u6cd5\u5982\u4e0b\uff1a</p> <pre><code>  /**\n   * Startup logic executed at the same time when the server starts up.\n   */\n  def startup(enableMetadataExpiration: Boolean = true): Unit = {\n    info(\"Starting up.\")\n    groupManager.startup(enableMetadataExpiration)\n    isActive.set(true)\n    info(\"Startup complete.\")\n  }\n</code></pre> <p>groupManager \u7684 <code>startup()</code> \u65b9\u6cd5\u5982\u4e0b\uff1a</p> <pre><code>  def startup(enableMetadataExpiration: Boolean): Unit = {\n    scheduler.startup()\n    if (enableMetadataExpiration) {\n      scheduler.schedule(name = \"delete-expired-group-metadata\",\n        fun = () =&gt; cleanupGroupMetadata(),\n        period = config.offsetsRetentionCheckIntervalMs,\n        unit = TimeUnit.MILLISECONDS)\n    }\n  }\n</code></pre> <p>\u5b9e\u9645\u4e0a\u542f\u52a8\u4e86\u4e00\u4e2a\u540e\u53f0\u7ebf\u7a0b\uff0c\u5e76\u4e14\u5b9a\u65f6\u4efb\u52a1\uff0c\u6bcf\u9694 <code>offsets.retention.check.interval.ms</code> \u6beb\u79d2\u6e05\u7406\u8d85\u8fc7 <code>offsets.retention.minutes</code> \u7684 offset cache</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/5-GroupCoordinator/2-coordinator%E5%88%87%E6%8D%A2%E5%8F%8A%E6%9B%B4%E6%96%B0/","title":"1.5.2 \u66f4\u65b0\u6570\u636e","text":"<p>GroupCoordinator \u6709 <code>onElection()</code>, <code>onResignation()</code> \u65b9\u6cd5</p> <pre><code>  /**\n   * Load cached state from the given partition and begin handling requests for groups which map to it.\n   *\n   * @param offsetTopicPartitionId The partition we are now leading\n   */\n  def onElection(offsetTopicPartitionId: Int): Unit = {\n    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n  }\n\n\n  /**\n   * Unload cached state for the given partition and stop handling requests for groups which map to it.\n   *\n   * @param offsetTopicPartitionId The partition we are no longer leading\n   */\n  def onResignation(offsetTopicPartitionId: Int): Unit = {\n    groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n  }\n</code></pre> <p>\u4f1a\u8c03\u7528 GroupMetadataManager \u7684 <code>scheduleLoadGroupAndOffsets()</code> \u7684\u65b9\u6cd5\uff0c\u5f00\u59cb\u8c03\u5ea6 <code>loadGroupsAndOffsets</code> \u65b9\u6cd5\u3002</p> <pre><code>  /**\n   * Asynchronously read the partition from the offsets topic and populate the cache\n   */\n  def scheduleLoadGroupAndOffsets(offsetsPartition: Int, onGroupLoaded: GroupMetadata =&gt; Unit): Unit = {\n    val topicPartition = new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, offsetsPartition)\n    if (addLoadingPartition(offsetsPartition)) {\n      info(s\"Scheduling loading of offsets and group metadata from $topicPartition\")\n      val startTimeMs = time.milliseconds()\n      scheduler.schedule(topicPartition.toString, () =&gt; loadGroupsAndOffsets(topicPartition, onGroupLoaded, startTimeMs))\n    } else {\n      info(s\"Already loading offsets and group metadata from $topicPartition\")\n    }\n  }\n</code></pre> <p>KafkaApis \u7684 <code>handleLeaderAndIsrRequest</code> \u65b9\u6cd5\u5728\u5904\u7406\u65f6\uff0c\u4f1a\u5728 leader \u5173\u7cfb\u53d8\u5316\u65f6\u5224\u65ad topic \u662f\u5426\u4e3a GROUP_METADATA_TOPIC_NAME\uff0c\u5982\u679c\u6210\u4e3a leader \u5219\u8c03\u7528 <code>onElection</code> \u65b9\u6cd5\uff0c\u5982\u679c\u6210\u4e3a follower \u5219\u8c03\u7528 <code>onResignation</code> \u65b9\u6cd5</p> <pre><code>  def handleLeaderAndIsrRequest(request: RequestChannel.Request): Unit = {\n    // ensureTopicExists is only for client facing requests\n    // We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they\n    // stop serving data to clients for the topic being deleted\n    val correlationId = request.header.correlationId\n    val leaderAndIsrRequest = request.body[LeaderAndIsrRequest]\n\n    def onLeadershipChange(updatedLeaders: Iterable[Partition], updatedFollowers: Iterable[Partition]): Unit = {\n      // for each new leader or follower, call coordinator to handle consumer group migration.\n      // this callback is invoked under the replica state change lock to ensure proper order of\n      // leadership changes\n      updatedLeaders.foreach { partition =&gt;\n        if (partition.topic == GROUP_METADATA_TOPIC_NAME)\n          groupCoordinator.onElection(partition.partitionId)\n        else if (partition.topic == TRANSACTION_STATE_TOPIC_NAME)\n          txnCoordinator.onElection(partition.partitionId, partition.getLeaderEpoch)\n      }\n\n      updatedFollowers.foreach { partition =&gt;\n        if (partition.topic == GROUP_METADATA_TOPIC_NAME)\n          groupCoordinator.onResignation(partition.partitionId)\n        else if (partition.topic == TRANSACTION_STATE_TOPIC_NAME)\n          txnCoordinator.onResignation(partition.partitionId, Some(partition.getLeaderEpoch))\n      }\n    }\n\n    authorizeClusterOperation(request, CLUSTER_ACTION)\n    if (isBrokerEpochStale(leaderAndIsrRequest.brokerEpoch)) {\n      // When the broker restarts very quickly, it is possible for this broker to receive request intended\n      // for its previous generation so the broker should skip the stale request.\n      info(\"Received LeaderAndIsr request with broker epoch \" +\n        s\"${leaderAndIsrRequest.brokerEpoch} smaller than the current broker epoch ${controller.brokerEpoch}\")\n      sendResponseExemptThrottle(request, leaderAndIsrRequest.getErrorResponse(0, Errors.STALE_BROKER_EPOCH.exception))\n    } else {\n      val response = replicaManager.becomeLeaderOrFollower(correlationId, leaderAndIsrRequest, onLeadershipChange)\n      sendResponseExemptThrottle(request, response)\n    }\n  }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/","title":"1.6 LogManager","text":"<p>\u6bcf\u4e2a broker \u8fdb\u7a0b\u6709 1 \u4e2a LogManager \u5b9e\u4f8b\uff0cLogManager \u8d1f\u8d23\u7ba1\u7406\u8fd9\u4e2a broker \u6240\u6709 topic partition \u5bf9\u5e94\u7684 Log\u3002</p> <p>\u6bcf\u4e2a topic partition \u5bf9\u5e94\u4e00\u4e2a Log \u5b9e\u4f8b\uff0cLog \u5b9e\u4f8b\u4e2d\u5305\u542b\u591a\u4e2a LogSegment\u3002</p> <p><code>LogManager</code> \u8d1f\u8d23\uff1a</p> <ul> <li>log creation</li> <li>log retrieval</li> <li>log cleaning</li> </ul> <p>\u6240\u6709 read \u548c write \u64cd\u4f5c\u90fd\u4f1a\u88ab\u4ee3\u7406\u7ed9\u5355\u72ec\u7684 <code>Log</code> \u5bf9\u8c61\u8d1f\u8d23</p> <p><code>LogManager</code> \u7ef4\u62a4\u4e00\u4e2a\u6216\u591a\u4e2a\u76ee\u5f55\uff0c\u65b0\u7684 log \u4f1a\u5728 log \u6700\u5c11\u7684\u76ee\u5f55\u4e2d\u521b\u5efa</p> <p>\u4e00\u4e2a\u540e\u53f0\u7ebf\u7a0b\u901a\u8fc7\u5468\u671f\u56de\u6536 log segment \u5904\u7406 log retention</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/1-LogManager%20%E5%90%AF%E5%8A%A8/","title":"1.6.1 LogManager \u542f\u52a8","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/1-LogManager%20%E5%90%AF%E5%8A%A8/#_1","title":"\u521b\u5efa","text":"<p>\u5728 <code>kafkaServer.startup()</code> \u4e2d\uff1a</p> <pre><code>/* start log manager */\nlogManager = LogManager(config, initialOfflineDirs, zkClient, brokerState, kafkaScheduler, time, brokerTopicStats, logDirFailureChannel)\nlogManager.startup()\n</code></pre> <p>\u5b9e\u9645\u8c03\u7528 <code>LogManager.apply()</code> \u521b\u5efa <code>LogManager</code> \u5bf9\u8c61\uff1a</p> <pre><code>object LogManager {\n\n  val RecoveryPointCheckpointFile = \"recovery-point-offset-checkpoint\"\n  val LogStartOffsetCheckpointFile = \"log-start-offset-checkpoint\"\n  val ProducerIdExpirationCheckIntervalMs = 10 * 60 * 1000\n\n  def apply(config: KafkaConfig,\n            initialOfflineDirs: Seq[String],\n            zkClient: KafkaZkClient,\n            brokerState: BrokerState,\n            kafkaScheduler: KafkaScheduler,\n            time: Time,\n            brokerTopicStats: BrokerTopicStats,\n            logDirFailureChannel: LogDirFailureChannel): LogManager = {\n    val defaultProps = KafkaServer.copyKafkaConfigToLog(config)\n    val defaultLogConfig = LogConfig(defaultProps)\n\n    // read the log configurations from zookeeper\n    val (topicConfigs, failed) = zkClient.getLogConfigs(zkClient.getAllTopicsInCluster, defaultProps)\n    if (!failed.isEmpty) throw failed.head._2\n\n    val cleanerConfig = LogCleaner.cleanerConfig(config)\n\n    new LogManager(logDirs = config.logDirs.map(new File(_).getAbsoluteFile),\n      initialOfflineDirs = initialOfflineDirs.map(new File(_).getAbsoluteFile),\n      topicConfigs = topicConfigs,\n      initialDefaultConfig = defaultLogConfig,\n      cleanerConfig = cleanerConfig,\n      recoveryThreadsPerDataDir = config.numRecoveryThreadsPerDataDir,\n      flushCheckMs = config.logFlushSchedulerIntervalMs,\n      flushRecoveryOffsetCheckpointMs = config.logFlushOffsetCheckpointIntervalMs,\n      flushStartOffsetCheckpointMs = config.logFlushStartOffsetCheckpointIntervalMs,\n      retentionCheckMs = config.logCleanupIntervalMs,\n      maxPidExpirationMs = config.transactionIdExpirationMs,\n      scheduler = kafkaScheduler,\n      brokerState = brokerState,\n      brokerTopicStats = brokerTopicStats,\n      logDirFailureChannel = logDirFailureChannel,\n      time = time)\n  }\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/1-LogManager%20%E5%90%AF%E5%8A%A8/#_2","title":"\u542f\u52a8","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/1-LogManager%20%E5%90%AF%E5%8A%A8/#loadlogs","title":"loadLogs","text":"<p><code>LogManager</code> \u5b9e\u4f8b\u5316\u65f6\u4f1a\u901a\u8fc7 <code>loadLogs()</code> \u65b9\u6cd5\u904d\u5386 <code>log.dirs</code> \u914d\u7f6e\u7684\u6bcf\u4e2a\u76ee\u5f55\uff0c\u5bf9\u6bcf\u4e2a\u76ee\u524d\u542f\u52a8 <code>num.recovery.threads.per.data.dir</code> \u4e2a\u7ebf\u7a0b\uff0c\u52a0\u8f7d\u76ee\u5f55\u4e0b\u7684\u6570\u636e\u6587\u4ef6\u3002</p> <p><code>log.dirs</code> \u914d\u7f6e\u76ee\u5f55\u4e0b\u7684\u6bcf\u4e2a logDir \u76ee\u5f55\u4ee3\u8868\u4e00\u4e2a topic partition\uff0c\u4f7f\u7528 <code>loadLog()</code> \u52a0\u8f7d\u3002</p> <p><code>loadLog()</code> \u4f1a\u521b\u5efa\u4e00\u4e2a <code>Log</code> \u5bf9\u8c61\u5b9e\u4f8b\uff0c\u5e76\u5c06 topic partition \u4e0e\u5bf9\u5e94\u7684 <code>Log</code> \u5b9e\u4f8b\u4fdd\u5b58\u5230 <code>currentLogs</code> \u4e2d\u3002</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/1-LogManager%20%E5%90%AF%E5%8A%A8/#log","title":"Log \u5bf9\u8c61","text":"<p><code>Log</code> \u5bf9\u8c61\u5728\u521d\u59cb\u5316\u65f6\uff0c\u4f1a\u901a\u8fc7 <code>loadSegments()</code> \u65b9\u6cd5\u52a0\u8f7d topic partition \u76ee\u5f55\u4e0b\u6240\u6709 segments\u3002</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/1-LogManager%20%E5%90%AF%E5%8A%A8/#startup","title":"startup","text":"<p><code>LogManager.startup()</code> \u542f\u52a8\u7684\u540e\u53f0\u7ebf\u7a0b\uff1a</p> <pre><code>/**\n *  Start the background threads to flush logs and do log cleanup\n */\ndef startup() {\n  /* Schedule the cleanup task to delete old logs */\n  if (scheduler != null) {\n    info(\"Starting log cleanup with a period of %d ms.\".format(retentionCheckMs))\n    scheduler.schedule(\"kafka-log-retention\",\n                       cleanupLogs _,\n                       delay = InitialTaskDelayMs,\n                       period = retentionCheckMs,\n                       TimeUnit.MILLISECONDS)\n    info(\"Starting log flusher with a default period of %d ms.\".format(flushCheckMs))\n    scheduler.schedule(\"kafka-log-flusher\",\n                       flushDirtyLogs _,\n                       delay = InitialTaskDelayMs,\n                       period = flushCheckMs,\n                       TimeUnit.MILLISECONDS)\n    scheduler.schedule(\"kafka-recovery-point-checkpoint\",\n                       checkpointLogRecoveryOffsets _,\n                       delay = InitialTaskDelayMs,\n                       period = flushRecoveryOffsetCheckpointMs,\n                       TimeUnit.MILLISECONDS)\n    scheduler.schedule(\"kafka-log-start-offset-checkpoint\",\n                       checkpointLogStartOffsets _,\n                       delay = InitialTaskDelayMs,\n                       period = flushStartOffsetCheckpointMs,\n                       TimeUnit.MILLISECONDS)\n    scheduler.schedule(\"kafka-delete-logs\", // will be rescheduled after each delete logs with a dynamic period\n                       deleteLogs _,\n                       delay = InitialTaskDelayMs,\n                       unit = TimeUnit.MILLISECONDS)\n  }\n  // \u5982\u679c\u8bbe\u7f6e\u4e3a true\uff0c\u81ea\u52a8\u6e05\u7406 compaction \u7c7b\u578b\u7684 topic\n  if (cleanerConfig.enableCleaner)\n    cleaner.startup()\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/","title":"1.6.2 \u65e5\u5fd7\u6e05\u7406","text":"<p>\u65e5\u5fd7\u6e05\u7406\u7684\u7ebf\u7a0b\u5728 <code>LogManager.startup()</code> \u4e2d\u542f\u52a8\uff0c\u5177\u4f53\u4e3a\uff1a</p> <pre><code>def startup() {\n  /* Schedule the cleanup task to delete old logs */\n  if (scheduler != null) {\n    info(\"Starting log cleanup with a period of %d ms.\".format(retentionCheckMs))\n    scheduler.schedule(\"kafka-log-retention\",\n                       cleanupLogs _,\n                       delay = InitialTaskDelayMs,\n                       period = retentionCheckMs,\n                       TimeUnit.MILLISECONDS)\n    // ....\n  }\n}\n</code></pre> <p>\u95f4\u9694 <code>retentionCheckMs</code>(\"log.retention.check.interval.ms\") \u5468\u671f\u6027\u8fd0\u884c <code>LogManager.cleanupLogs()</code> \u65b9\u6cd5\uff1a</p> <pre><code>/**\n * Delete any eligible logs. Return the number of segments deleted.\n * Only consider logs that are not compacted.\n */\ndef cleanupLogs(): Unit = {\n  debug(\"Beginning log cleanup...\")\n  var total = 0\n  val startMs = time.milliseconds\n\n  // clean current logs.\n  val deletableLogs = {\n    if (cleaner != null) {\n      // prevent cleaner from working on same partitions when changing cleanup policy\n      cleaner.pauseCleaningForNonCompactedPartitions()\n    } else {\n      currentLogs.filter {\n        case (_, log) =&gt; !log.config.compact\n      }\n    }\n  }\n\n  try {\n    deletableLogs.foreach {\n      case (topicPartition, log) =&gt;\n        debug(s\"Garbage collecting '${log.name}'\")\n        total += log.deleteOldSegments()\n\n        val futureLog = futureLogs.get(topicPartition)\n        if (futureLog != null) {\n          // clean future logs\n          debug(s\"Garbage collecting future log '${futureLog.name}'\")\n          total += futureLog.deleteOldSegments()\n        }\n    }\n  } finally {\n    if (cleaner != null) {\n      cleaner.resumeCleaning(deletableLogs.map(_._1))\n    }\n  }\n\n  debug(s\"Log cleanup completed. $total files deleted in \" +\n                (time.milliseconds - startMs) / 1000 + \" seconds\")\n}\n</code></pre> <p>\u8c03\u7528\u6bcf\u4e00\u4e2a log \u5b9e\u4f8b\u7684 <code>Log.deleteOldSegments()</code> \u65b9\u6cd5\uff1a</p> <pre><code>  /**\n   * Delete any log segments that have either expired due to time based retention\n   * or because the log size is &gt; retentionSize\n   */\n  def deleteOldSegments(): Int = {\n    if (!config.delete) return 0\n    deleteRetentionMsBreachedSegments() + deleteRetentionSizeBreachedSegments() + deleteLogStartOffsetBreachedSegments()\n  }\n</code></pre> <p>\u8fd9\u91cc\u5b9e\u9645\u8c03\u7528\u4e86 3 \u4e2a\u65b9\u6cd5\uff0c\u5206\u522b\u662f\uff1a</p> <ul> <li>deleteRetentionMsBreachedSegments: \u6309\u4fdd\u7559\u65f6\u95f4\u6e05\u7406</li> <li>deleteRetentionSizeBreachedSegments: \u6309\u4fdd\u7559\u5927\u5c0f\u6e05\u7406</li> <li>deleteLogStartOffsetBreachedSegments: \u6e05\u7406 start offset \u4e4b\u524d\u7684\u6570\u636e\uff0ckafka \u63d0\u4f9b\u4e86 <code>DeleteRecords</code> API\uff0c\u53ef\u4ee5\u624b\u52a8\u4fee\u6539 topic partition \u7684 start offset</li> </ul>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/#deleteretentionmsbreachedsegments","title":"deleteRetentionMsBreachedSegments","text":"<p><code>Log.scala</code></p> <pre><code>private def deleteRetentionMsBreachedSegments(): Int = {\n  if (config.retentionMs &lt; 0) return 0\n  val startMs = time.milliseconds\n  deleteOldSegments((segment, _) =&gt; startMs - segment.largestTimestamp &gt; config.retentionMs,\n    reason = s\"retention time ${config.retentionMs}ms breach\")\n}\n</code></pre> <p>\u8fd9\u91cc\u5224\u65ad\u7684\u4f9d\u636e\u662f <code>startMs - segment.largestTimestamp &gt; config.retentionMs</code></p> <p>\u770b\u4e00\u4e0b largestTimestamp \u662f\u600e\u4e48\u5b9a\u4e49\u7684\uff1a</p> <p><code>LogSegment.scala</code></p> <pre><code>/**\n * The last modified time of this log segment as a unix time stamp\n */\ndef lastModified = log.file.lastModified\n\n/**\n * The largest timestamp this segment contains, if maxTimestampSoFar &gt;= 0, otherwise None.\n */\ndef largestRecordTimestamp: Option[Long] = if (maxTimestampSoFar &gt;= 0) Some(maxTimestampSoFar) else None\n\n/**\n * The largest timestamp this segment contains.\n */\ndef largestTimestamp = if (maxTimestampSoFar &gt;= 0) maxTimestampSoFar else lastModified\n</code></pre> <p>\u6d88\u606f\u7684\u6700\u5927\u65f6\u95f4\u6233\u6216\u8005\u6587\u4ef6\u7684\u6700\u540e\u4fee\u6539\u65f6\u95f4\u3002</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/#deleteretentionsizebreachedsegments","title":"deleteRetentionSizeBreachedSegments","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/#deletelogstartoffsetbreachedsegments","title":"deleteLogStartOffsetBreachedSegments","text":""},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/2-%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/#deleteoldsegments","title":"\u6536\u675f\u5230 deleteOldSegments","text":"<p>\u4e0a\u8ff0 3 \u79cd\u60c5\u51b5\uff0c\u90fd\u4f1a\u6536\u675f\u5230 <code>Log.deleteOldSegments()</code>\uff0c\u4e0d\u540c\u7684\u60c5\u51b5\u53ea\u662f\u4f20\u4e0d\u540c\u7684 <code>predicate</code> \u5224\u65ad\u65b9\u6cd5\u3002</p> <pre><code>/**\n * Delete any log segments matching the given predicate function,\n * starting with the oldest segment and moving forward until a segment doesn't match.\n *\n * @param predicate A function that takes in a candidate log segment and the next higher segment\n *                  (if there is one) and returns true iff it is deletable\n * @return The number of segments deleted\n */\nprivate def deleteOldSegments(predicate: (LogSegment, Option[LogSegment]) =&gt; Boolean, reason: String): Int = {\n  lock synchronized {\n    val deletable = deletableSegments(predicate)\n    if (deletable.nonEmpty)\n      info(s\"Found deletable segments with base offsets [${deletable.map(_.baseOffset).mkString(\",\")}] due to $reason\")\n    deleteSegments(deletable)\n  }\n}\n</code></pre> <p>\u904d\u5386\u6240\u6709 segment\uff0c\u7528 predicate \u5224\u65ad\u662f\u5426\u53ef\u4ee5\u5220\u9664\uff0c\u8fd4\u56de\u53ef\u4ee5\u5220\u9664 segment \u7684\u96c6\u5408\uff1a</p> <pre><code>/**\n * Find segments starting from the oldest until the user-supplied predicate is false or the segment\n * containing the current high watermark is reached. We do not delete segments with offsets at or beyond\n * the high watermark to ensure that the log start offset can never exceed it. If the high watermark\n * has not yet been initialized, no segments are eligible for deletion.\n *\n * A final segment that is empty will never be returned (since we would just end up re-creating it).\n *\n * @param predicate A function that takes in a candidate log segment and the next higher segment\n *                  (if there is one) and returns true iff it is deletable\n * @return the segments ready to be deleted\n */\nprivate def deletableSegments(predicate: (LogSegment, Option[LogSegment]) =&gt; Boolean): Iterable[LogSegment] = {\n  if (segments.isEmpty || replicaHighWatermark.isEmpty) {\n    Seq.empty\n  } else {\n    val highWatermark = replicaHighWatermark.get\n    val deletable = ArrayBuffer.empty[LogSegment]\n    var segmentEntry = segments.firstEntry\n    while (segmentEntry != null) {\n      val segment = segmentEntry.getValue\n      val nextSegmentEntry = segments.higherEntry(segmentEntry.getKey)\n      val (nextSegment, upperBoundOffset, isLastSegmentAndEmpty) = if (nextSegmentEntry != null)\n        (nextSegmentEntry.getValue, nextSegmentEntry.getValue.baseOffset, false)\n      else\n        (null, logEndOffset, segment.size == 0)\n\n      if (highWatermark &gt;= upperBoundOffset &amp;&amp; predicate(segment, Option(nextSegment)) &amp;&amp; !isLastSegmentAndEmpty) {\n        deletable += segment\n        segmentEntry = nextSegmentEntry\n      } else {\n        segmentEntry = null\n      }\n    }\n    deletable\n  }\n}\n</code></pre> <pre><code>private def deleteSegments(deletable: Iterable[LogSegment]): Int = {\n  maybeHandleIOException(s\"Error while deleting segments for $topicPartition in dir ${dir.getParent}\") {\n    val numToDelete = deletable.size\n    if (numToDelete &gt; 0) {\n      // we must always have at least one segment, so if we are going to delete all the segments, create a new one first\n      if (segments.size == numToDelete)\n        roll()\n      lock synchronized {\n        checkIfMemoryMappedBufferClosed()\n        // remove the segments for lookups\n        deletable.foreach(deleteSegment)\n        maybeIncrementLogStartOffset(segments.firstEntry.getValue.baseOffset)\n      }\n    }\n    numToDelete\n  }\n}\n</code></pre> <p><code>Log.scala</code></p> <pre><code> /**\n   * If topic deletion is enabled, delete any log segments that have either expired due to time based retention\n   * or because the log size is &gt; retentionSize.\n   *\n   * Whether or not deletion is enabled, delete any log segments that are before the log start offset\n   */\n  def deleteOldSegments(): Int = {\n    if (config.delete) {\n      deleteRetentionMsBreachedSegments() + deleteRetentionSizeBreachedSegments() + deleteLogStartOffsetBreachedSegments()\n    } else {\n      deleteLogStartOffsetBreachedSegments()\n    }\n  }\n\n  private def deleteRetentionMsBreachedSegments(): Int = {\n    if (config.retentionMs &lt; 0) return 0\n    val startMs = time.milliseconds\n\n    def shouldDelete(segment: LogSegment, nextSegmentOpt: Option[LogSegment]): Boolean = {\n      startMs - segment.largestTimestamp &gt; config.retentionMs\n    }\n\n    deleteOldSegments(shouldDelete, RetentionMsBreach)\n  }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/4-recovery-point/","title":"1.6.4 recovery point \u68c0\u67e5\u70b9\u6587\u4ef6","text":"<ol> <li>\u6d88\u606f\u8ffd\u52a0\u5230\u5206\u533a\u5bf9\u5e94\u7684\u65e5\u5fd7\uff0c\u5728\u5237\u65b0\u65e5\u5fd7\u65f6\uff0c\u4f1a\u5c06\u6700\u65b0\u7684\u504f\u79fb\u91cf\u4f5c\u4e3a\u65e5\u5fd7\u7684\u68c0\u67e5\u70b9</li> <li>LogManager \u4f1a\u542f\u52a8\u4e00\u4e2a\u5b9a\u65f6\u4efb\u52a1\uff0c\u8bfb\u53d6\u6240\u6709\u5206\u533a\u65e5\u5fd7\u7684\u68c0\u67e5\u70b9\uff0c\u5e76\u5199\u5165\u5168\u5c40\u7684\u68c0\u67e5\u70b9\u6587\u4ef6</li> </ol>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/7-%E5%8E%8B%E7%BC%A9%E7%AD%96%E7%95%A5%E7%9A%84%E6%97%A5%E5%BF%97%E6%B8%85%E7%90%86/","title":"1.6.7 \u538b\u7f29\u7b56\u7565\u7684\u65e5\u5fd7\u6e05\u7406","text":"<pre><code>log.cleaner.enable = true\n</code></pre> <p>\u4f1a\u5f00\u542f\u5185\u90e8\u7684 LogCleaner\uff0c\u5426\u5219 cleaner \u4e3a null</p> <pre><code>log.cleaner.min.compaction.lag.ms\n</code></pre> <p>\u4fdd\u8bc1\u5199\u5165\u65f6\u95f4\u8ddd\u5f53\u524d\u65f6\u95f4\u5c0f\u4e8e\u8fd9\u4e2a\u8303\u56f4\u7684\u6d88\u606f\u4e0d\u4f1a\u88ab compact\u3002\u5982\u679c\u4e0d\u8bbe\u7f6e\uff0c\u9664\u5f53\u524d\u6d3b\u8dc3\u7684 log segment \u5916\u6240\u6709\u7684 log segment \u90fd\u53ef\u4ee5\u88ab compact\u3002\u5f53\u524d\u6d3b\u8dc3\u7684 log segment \u5373\u4f7f\u5176\u4e2d\u5305\u542b\u7684\u6d88\u606f\u5199\u5165\u65f6\u95f4\u8ddd\u4eca\u90fd\u8d85\u8fc7\u4e86 min lag ms\uff0c\u4e5f\u4e0d\u4f1a\u88ab compact\u3002</p> <pre><code>min.cleanable.dirty.ratio\n</code></pre> <p>log \u6587\u4ef6\u4e2d\u672a compact \u7684\u5185\u5bb9\uff08dirty\uff09\u6bd4\u4f8b\u8d85\u8fc7\u8fd9\u4e2a\u503c\u7684\u6587\u4ef6\u4f1a\u88ab compact</p> <pre><code>log.cleaner.max.compaction.lag.ms\n</code></pre> <p>\u4fdd\u8bc1\u5199\u5165\u65f6\u95f4\u8ddd\u4eca\u8d85\u8fc7\u8fd9\u4e2a\u65f6\u95f4\u7684\u6d88\u606f\u88ab compact\uff0c\u5373\u4f7f\u6587\u4ef6 dirty.ratio \u6ca1\u8fbe\u5230\u8bbe\u5b9a\u7684\u503c</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/6-LogManager/8-LogSegment/","title":"1.8 LogSegment","text":"<p><code>append()</code></p> <ul> <li>\u5199\u6d88\u606f\u5230\u6587\u4ef6</li> <li>\u589e\u52a0\u81ea\u8eab\u7ef4\u62a4\u7684 offsetIndex, timeIndex</li> </ul> <p><code>flush()</code></p> <ul> <li>log.flush()</li> <li>offsetIndex.flush()</li> <li>timeIndex.flush()</li> <li>txnIndex.flush()</li> </ul> <p>nio * log \u6587\u4ef6\uff0c<code>java.nio.channels.WritableByteChannel.write()</code> * index \u6587\u4ef6\uff0c<code>java.nio.MappedByteBuffer</code> \u76f4\u63a5\u5bf9\u5e94\u4e00\u4e2a\u6587\u4ef6\uff0c\u65b9\u6cd5 <code>force()</code> \u540c\u6b65\u5185\u5b58\u5185\u5bb9\u5230\u6587\u4ef6</p>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/7-acl%20%E6%8E%A7%E5%88%B6/","title":"1.7 Acl \u6743\u9650\u63a7\u5236","text":"<p>kafka \u63d0\u4f9b\u4e86\u6743\u9650\u63a7\u5236\u63a5\u53e3 <code>org.apache.kafka.server.authorizer.Authorizer</code> \u4e0e\u5176\u9ed8\u8ba4\u5b9e\u73b0 <code>kafka.security.authorizer.AclAuthorizer</code></p> <p>\u901a\u8fc7\u914d\u7f6e\uff1a</p> <pre><code>authorizer.class.name=kafka.security.authorizer.AclAuthorizer\n</code></pre> <p>\u53ef\u4ee5\u8ba9 kafka \u521d\u59cb\u5316\u8fd9\u4e2a\u7c7b\uff0c\u5e76\u5728\u5904\u7406\u8bf7\u6c42\u662f\u8fdb\u884c acl \u5224\u65ad</p> <p>2.4 \u4e4b\u524d\u662f <code>kafka.security.auth.Authorizer</code> \u63a5\u53e3\u548c <code>kafka.security.auth.SimpleAclAuthorizer</code> \u5b9e\u73b0\u7c7b</p> <p>\u5728 KafkaServer <code>startup</code> \u65b9\u6cd5\u4e2d\uff1a</p> <pre><code>/* Get the authorizer and initialize it if one is specified.*/\nauthorizer = config.authorizer\nauthorizer.foreach(_.configure(config.originals))\nval authorizerFutures: Map[Endpoint, CompletableFuture[Void]] = authorizer match {\n  case Some(authZ) =&gt;\n    authZ.start(brokerInfo.broker.toServerInfo(clusterId, config)).asScala.map { case (ep, cs) =&gt;\n      ep -&gt; cs.toCompletableFuture\n    }\n  case None =&gt;\n    brokerInfo.broker.endPoints.map { ep =&gt;\n      ep.toJava -&gt; CompletableFuture.completedFuture[Void](null)\n    }.toMap\n}\n</code></pre> <p>\u5728 KafkaApis \u4e2d\u5b9a\u4e49\uff1a</p> <pre><code>  private[server] def authorize(requestContext: RequestContext,\n                                operation: AclOperation,\n                                resourceType: ResourceType,\n                                resourceName: String,\n                                logIfAllowed: Boolean = true,\n                                logIfDenied: Boolean = true,\n                                refCount: Int = 1): Boolean = {\n    authorizer.forall { authZ =&gt;\n      val resource = new ResourcePattern(resourceType, resourceName, PatternType.LITERAL)\n      val actions = Collections.singletonList(new Action(operation, resource, refCount, logIfAllowed, logIfDenied))\n      authZ.authorize(requestContext, actions).get(0) == AuthorizationResult.ALLOWED\n    }\n  }\n</code></pre> <p>\u5904\u7406\u51fd\u6570\u4e0e\u5bf9\u5e94\u68c0\u67e5\u7684\u6743\u9650\u5982\u4e0b\uff1a</p> <ul> <li><code>handleProduceRequest</code><ul> <li>WRITE            TRANSACTIONANL_ID produceRequest.transactionalId</li> <li>IDEMPOTENT_WRITE CLUSTER           CLUSTER_NAME</li> <li>WRITE            TOPIC</li> </ul> </li> <li><code>handleFetchRequest</code><ul> <li>CLUSTER_ACTION   CLUSTER      CLUSTER_NAME</li> <li>READ TOPIC</li> </ul> </li> <li><code>handleListOffsetRequestV0</code><ul> <li>DESCRIBE TOPIC</li> </ul> </li> <li><code>handleListOffsetRequestV1AndAbove</code><ul> <li>DESCRIBE TOPIC</li> </ul> </li> </ul> <p>\u6539\u5199 <code>kafka.security.authorizer.AclAuthorizer</code> \u7684 <code>authorize()</code> \u65b9\u6cd5\uff0c\u8ba9\u5176\u5bf9\u6240\u6709 <code>PLAINTEXT</code> \u534f\u8bae\u7684\u8bbf\u95ee\u653e\u884c\u3002</p> <pre><code>  override def authorize(requestContext: AuthorizableRequestContext, actions: util.List[Action]): util.List[AuthorizationResult] = {\n    if (requestContext.securityProtocol() == SecurityProtocol.PLAINTEXT) {\n      // PLAINTEXT \u5141\u8bb8\u4e00\u5207\u64cd\u4f5c\n      actions.asScala.map { _ =&gt; AuthorizationResult.ALLOWED }.asJava\n    } else {\n      actions.asScala.map { action =&gt; authorizeAction(requestContext, action) }.asJava\n    }\n  }\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/","title":"Metrics","text":"<p><code>Metrics</code> \u7c7b\u7ba1\u7406\u6240\u6709 metrics \u548c sensor</p> <p>KafkaServer startup() \u65b9\u6cd5\u4e2d\u4f1a\u521d\u59cb\u5316\u4e00\u4e2a <code>Metrics</code> \u5b9e\u4f8b</p> <p>\u7ba1\u7406\u6210\u5458\uff1a</p> <pre><code>// \u6240\u6709\u6ce8\u518c\u7684 metric\uff0c\u901a\u8fc7 addMetric() \u65b9\u6cd5\u6dfb\u52a0\nprivate final ConcurrentMap&lt;MetricName, KafkaMetric&gt; metrics;\n\n// \u6240\u6709\u6ce8\u518c\u7684 Sensor\uff0c\u901a\u8fc7 sensor() \u65b9\u6cd5\u6dfb\u52a0\nprivate final ConcurrentMap&lt;String, Sensor&gt; sensors;\n// key \u4e3a parent sensor\uff0cvalue \u4e3a\u8fd9\u4e2a parent \u62e5\u6709\u7684 children sensor\nprivate final ConcurrentMap&lt;Sensor, List&lt;Sensor&gt;&gt; childrenSensors;\n\nprivate final List&lt;MetricsReporter&gt; reporters;\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/#metricname","title":"MetricName","text":"<p>\u76d1\u63a7\u6807\u8bc6\uff1a</p> <ul> <li>name</li> <li>group</li> <li>description</li> <li>tags</li> </ul>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/#kafkametric","title":"KafkaMetric","text":"<p>\u76d1\u63a7\u9879\uff0c</p> <p>\u5305\u542b\u4e00\u4e2a <code>MetricConfig</code> \u5bf9\u8c61\uff0c<code>MetricConfig</code> \u5bf9\u8c61\u8bb0\u5f55:</p> <pre><code>// \u9650\u6d41\u673a\u5236\nprivate Quota quota;\n\n// \u6837\u672c\u6570\nprivate int samples;\n\n// \u5355\u4e2a\u6837\u672c\u7684\u4e8b\u4ef6\u7a97\u53e3\u5927\u5c0f\nprivate long eventWindow;\n\n// \u5355\u4e2a\u6837\u672c\u7684\u65f6\u95f4\u7a97\u53e3\u5927\u5c0f\nprivate long timeWindowMs;\n\nprivate Map&lt;String, String&gt; tags;\nprivate Sensor.RecordingLevel recordingLevel;\n</code></pre> <p>\u5305\u542b\u4e00\u4e2a <code>MetricValueProvider</code> \u5bf9\u8c61\uff0c\u7528\u6765\u63d0\u4f9b\u76d1\u63a7\u503c\u3002</p> <p><code>MetricValueProvider</code> \u662f\u4e00\u4e2a\u7a7a\u7684 <code>interface</code>\uff0c\u5176\u6709 2 \u4e2a\u7ee7\u627f <code>interface</code></p> <ul> <li><code>Measurable</code></li> <li><code>Gauge</code></li> </ul>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/#sampledstat","title":"SampledStat","text":"<p><code>SampledStat</code> \u5b9e\u73b0\u4e86 <code>Measurable</code> \u63a5\u53e3\uff0c\u5176\u4ee5\u53ca\u5176\u7ee7\u627f\u7c7b\u53ef\u4ee5\u4f5c\u4e3a <code>KafkaMetric</code> \u7684 <code>MetricValueProvider</code></p> <p><code>SampledStat</code> \u4e2d\u8bb0\u5f55\uff1a</p> <pre><code>// \u6837\u672c\u521d\u59cb\u503c\nprivate double initialValue;\n\n// \u5f53\u524d\u6837\u672c\u4e0b\u6807\u7d22\u5f15\nprivate int current = 0;\n\n// \u6837\u672c\u5217\u8868\nprotected List&lt;Sample&gt; samples;\n</code></pre> <p><code>Sample</code> \u4e2d\u8bb0\u5f55\uff1a</p> <pre><code>// \u521d\u59cb\u503c\npublic double initialValue;\n\n// \u6837\u672c\u4e8b\u4ef6\u6570\npublic long eventCount;\n\n// \u7a97\u53e3\u8d77\u59cb\u65f6\u95f4\npublic long lastWindowMs;\n\n// \u7a97\u53e3\u503c\npublic double value;\n</code></pre> <p><code>SampledStat</code> \u901a\u8fc7 <code>record()</code> \u65b9\u6cd5\u8bb0\u5f55\u6570\u636e\uff1a</p> <pre><code>@Override\npublic void record(MetricConfig config, double value, long timeMs) {\n    // 1. \u83b7\u53d6\u5f53\u524d sample\uff0c\u5982\u679c\u6ca1\u6709\u5c31\u65b0\u5efa\u4e00\u4e2a\n    Sample sample = current(timeMs);\n\n    // 2. \u68c0\u67e5\u5f53\u524d sample \u7a97\u53e3\u662f\u5426\u7ed3\u675f\n    if (sample.isComplete(timeMs, config))\n        // 2.1 \u5982\u679c\u5f53\u524d sample \u7a97\u53e3\u7ed3\u675f\uff0ccurrent +1\uff0c\u79fb\u5411\u4e0b\u4e00\u4e2a sample\n        sample = advance(config, timeMs);\n\n    // 3. \u5728\u5f53\u524d sample \u4e2d\u8bb0\u5f55 value\uff0c\u5f53\u524d sample \u7684 value \u8bb0\u5f55\uff0c\n    //    \u8fd9\u91cc\u4e0d\u540c\u7684\u7edf\u8ba1\u65b9\u6cd5\u6709\u4e0d\u540c\u7684\u5b9e\u73b0\uff0c\u4f8b\u5982 avg, max, min \u7b49\n    update(sample, config, value, timeMs);\n\n    // 4. \u5f53\u524d sample \u7684\u4e8b\u4ef6\u6570 +1\n    sample.eventCount += 1;\n}\n</code></pre> <p><code>SampledStat</code> \u901a\u8fc7 <code>measure()</code> \u65b9\u6cd5\u5f97\u5230\u6837\u672c\u7684\u7edf\u8ba1\u503c\uff1a</p> <pre><code>@Override\npublic double measure(MetricConfig config, long now) {\n    // 1. \u91cd\u7f6e\u6240\u6709\u8fc7\u671f\u7684 sample\n    purgeObsoleteSamples(config, now);\n\n    // 2. \u8ba1\u7b97\u6240\u6709 sample \u7684\u7edf\u8ba1\u503c\uff0c\u8fd9\u91cc\u4e0d\u540c\u7684\u7edf\u8ba1\u65b9\u6cd5\u6709\u4e0d\u540c\u7684\u5b9e\u73b0\uff0c\u4f8b\u5982 avg, max, min \u7b49\n    return combine(this.samples, config, now);\n}\n\n\n/* Timeout any windows that have expired in the absence of any events */\nprotected void purgeObsoleteSamples(MetricConfig config, long now) {\n    long expireAge = config.samples() * config.timeWindowMs();\n    for (Sample sample : samples) {\n        if (now - sample.lastWindowMs &gt;= expireAge)\n            sample.reset(now);\n    }\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/#avg","title":"Avg","text":"<p>\u7ee7\u627f\u81ea <code>SampledStat</code>\uff0c\u590d\u5199 <code>update()</code>, <code>combine()</code></p> <pre><code>@Override\nprotected void update(Sample sample, MetricConfig config, double value, long now) {\n    sample.value += value;\n}\n\n@Override\npublic double combine(List&lt;Sample&gt; samples, MetricConfig config, long now) {\n    double total = 0.0;\n    long count = 0;\n    for (Sample s : samples) {\n        total += s.value;\n        count += s.eventCount;\n    }\n    return count == 0 ? Double.NaN : total / count;\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/#max","title":"Max","text":"<p>\u7ee7\u627f\u81ea <code>SampledStat</code>\uff0c\u590d\u5199 <code>update()</code>, <code>combine()</code></p> <pre><code>@Override\nprotected void update(Sample sample, MetricConfig config, double value, long now) {\n    sample.value = Math.max(sample.value, value);\n}\n\n@Override\npublic double combine(List&lt;Sample&gt; samples, MetricConfig config, long now) {\n    double max = Double.NEGATIVE_INFINITY;\n    long count = 0;\n    for (Sample sample : samples) {\n        max = Math.max(max, sample.value);\n        count += sample.eventCount;\n    }\n    return count == 0 ? Double.NaN : max;\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/#rate","title":"Rate","text":"<p><code>Rate</code> \u5b9e\u73b0\u4e86 <code>Measurable</code> \u63a5\u53e3\uff0c\u5176\u4ee5\u53ca\u5176\u7ee7\u627f\u7c7b\u53ef\u4ee5\u4f5c\u4e3a <code>KafkaMetric</code> \u7684 <code>MetricValueProvider</code></p> <pre><code>protected final TimeUnit unit;\nprotected final SampledStat stat;\n</code></pre> <pre><code>@Override\npublic void record(MetricConfig config, double value, long timeMs) {\n    // \u8bb0\u5f55\u6570\u636e\u901a\u8fc7\u6301\u6709\u7684 sampledStat \u5b8c\u6210\n    this.stat.record(config, value, timeMs);\n}\n\n@Override\npublic double measure(MetricConfig config, long now) {\n    // \u83b7\u5f97\u7edf\u8ba1\u503c\uff0c\u5148\u7531\u6301\u6709\u7684 sampledStat \u7684\u5230\u6837\u672c\u7edf\u8ba1\u503c\uff0c\u5728\u9664\u4ee5\u65f6\u95f4\u5f97\u5230\u901f\u7387\n    double value = stat.measure(config, now);\n    return value / convert(windowSize(config, now), unit);\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/#sensor","title":"Sensor","text":"<p>\u901a\u8fc7 <code>Stat</code> \u8bb0\u5f55\u76d1\u63a7\u6570\u636e\u503c</p> <pre><code>   private void recordInternal(double value, long timeMs, boolean checkQuotas) {\n        this.lastRecordTime = timeMs;\n        synchronized (this) {\n            synchronized (metricLock()) {\n                // increment all the stats\n                for (StatAndConfig statAndConfig : this.stats) {\n                    statAndConfig.stat.record(statAndConfig.config(), value, timeMs);\n                }\n            }\n            if (checkQuotas)\n                checkQuotas(timeMs);\n        }\n        for (Sensor parent : parents)\n            parent.record(value, timeMs, checkQuotas);\n    }\n</code></pre> <p><code>Stat</code> \u6709\u591a\u79cd\u5b9e\u73b0\uff0c\u4f8b\u5982\u5728\u8bb0\u5f55\u6570\u636e\u65f6\u5b8c\u6210 max, min, avg \u7b49\u6570\u636e\u7684\u66f4\u65b0</p> <pre><code>/**\n * A Stat is a quantity such as average, max, etc that is computed off the stream of updates to a sensor\n */\npublic interface Stat {\n\n    /**\n     * Record the given value\n     * @param config The configuration to use for this metric\n     * @param value The value to record\n     * @param timeMs The POSIX time in milliseconds this value occurred\n     */\n    void record(MetricConfig config, double value, long timeMs);\n\n}\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/1-Yammer/","title":"Yammer metrics","text":"<p>kafka \u76d1\u63a7\u5b9e\u73b0\u4f9d\u8d56 https://github.com/dropwizard/metrics</p> <ul> <li>Gauges\uff1a\u77ac\u65f6\u503c</li> <li>Counters\uff1a\u7d2f\u8ba1\u503c\uff0c\u53ef\u4ee5\u5728\u8fc7\u53bb\u7684\u57fa\u7840\u4e0a\u589e\u52a0\u6216\u51cf\u5c11</li> <li>Meters\uff1a\u4e00\u6bb5\u65f6\u95f4\u5185\u7684\u4e8b\u4ef6\u53d1\u751f\u7684\u901f\u7387(Rate)</li> <li>Histograms\uff1a\u4e00\u4e2a\u65f6\u95f4\u5185\u7684\u7edf\u8ba1\u503c(Max, Min, Avg)</li> <li>Timers\uff1a</li> <li>Healthy checks</li> </ul>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/1-Yammer/#kafkayammermetrics","title":"KafkaYammerMetrics","text":"<pre><code>        /* create and configure metrics */\n        kafkaYammerMetrics = KafkaYammerMetrics.INSTANCE\n        kafkaYammerMetrics.configure(config.originals)\n\n        val jmxReporter = new JmxReporter()\n        jmxReporter.configure(config.originals)\n\n        val reporters = new util.ArrayList[MetricsReporter]\n        reporters.add(jmxReporter)\n\n        val metricConfig = KafkaServer.metricConfig(config)\n        val metricsContext = createKafkaMetricsContext()\n        metrics = new Metrics(metricConfig, reporters, time, true, metricsContext)\n</code></pre>"},{"location":"1-server%20%E8%A7%A3%E6%9E%90/9-Metrics/2-KafkaMetricsGroup/","title":"KafkaMetricsGroup","text":"<p><code>KafkaMetricsGroup</code> trait \u63d0\u4f9b newGauge, newMeter, newHistogram, newTimer, removeMetric \u7b49\u65b9\u6cd5</p> <p>\u7ee7\u627f\u81ea <code>KafkaMetricsGroup</code> trait \u7684 class</p>"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/","title":"2 producer \u89e3\u6790","text":""},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/","title":"2.1 producer \u53d1\u9001\u6d88\u606f","text":""},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#_1","title":"\u4ee3\u7801\u793a\u4f8b","text":"<p>\u6211\u4eec\u5148\u770b\u4e0b <code>KafkaProducer</code> \u5982\u4f55\u53d1\u9001\u6d88\u606f\uff1a</p> <pre><code>Properties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\nprops.put(\"acks\", \"all\");\nprops.put(\"retries\", 0);\nprops.put(\"linger.ms\", 1);\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\nProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);\nfor (int i = 0; i &lt; 100; i++) {\n    producer.send(new ProducerRecord&lt;String, String&gt;(\"my-topic\", Integer.toString(i), Integer.toString(i)));\n}\n\nproducer.close();\n</code></pre> <p>\u8fd9\u91cc\u53ea\u6709 2 \u90e8\u5206\uff0c\u6784\u9020 <code>KafkaProducer</code> \u5b9e\u4f8b\uff0c\u8c03\u7528 <code>send()</code> \u51fd\u6570\u3002</p>"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#kafkaproducer","title":"\u6784\u9020 KafkaProducer","text":""},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#kafkaproducer_1","title":"KafkaProducer \u5b9e\u4f8b","text":"<p>\u9700\u8981\u6ce8\u610f <code>KafkaProducer</code> \u4e2d\u4ee5\u4e0b\u51e0\u4e2a\u5c5e\u6027\uff1a</p> <ul> <li><code>partitioner</code>: <code>Partitioner</code> \u5b9e\u4f8b\uff0c\u5bf9\u5e94\u5206\u533a\u7b97\u6cd5</li> <li><code>metadata</code>: <code>ProducerMetadata</code> \u5b9e\u4f8b\uff0c\u5305\u542b topic metadata</li> <li><code>accumulator</code>: <code>RecordAccumulator</code> \u5b9e\u4f8b\uff0c\u5305\u542b\u6bcf\u4e2a topic-partition \u53d1\u9001\u6570\u636e\u7684\u961f\u5217</li> <li><code>sender</code>: <code>Sender</code> \u5b9e\u4f8b\uff0c\u4e4b\u540e\u4f1a\u5728\u72ec\u7acb\u7684\u7ebf\u7a0b\u4e2d\u8fd0\u884c\uff0c\u8d1f\u8d23\u6570\u636e\u7684\u53d1\u9001</li> <li><code>ioThread</code>: <code>KafkaThread</code> \u5b9e\u4f8b\uff0c\u72ec\u7acb\u7684\u7ebf\u7a0b\uff0c\u8fd0\u884c <code>sender</code></li> </ul>"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#sender","title":"Sender \u5b9e\u4f8b","text":"<p>\u9700\u8981\u6ce8\u610f <code>Sender</code> \u4e2d\u4ee5\u4e0b\u51e0\u4e2a\u5c5e\u6027\uff1a</p> <ul> <li><code>client</code>: <code>KafkaClient</code> \u5b9e\u4f8b(\u5b9e\u9645\u4e3a<code>NetworkClient</code>)\uff0c\u8fdb\u884c\u5b9e\u9645\u7684\u6570\u636e\u53d1\u9001</li> <li><code>accumulator</code>: <code>RecordAccumulator</code> \u5b9e\u4f8b</li> <li><code>metadata</code>: <code>ProducerMetadata</code> \u5b9e\u4f8b\uff0c\u5305\u542b topic metadata</li> </ul>"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#networkclient","title":"NetworkClient \u5b9e\u4f8b","text":"<ul> <li><code>selector</code>: kafka \u5bf9 java.nio.channels.Selector \u7684\u5305\u88c5</li> <li><code>metadataUpdater</code></li> </ul> <p>KafkaProducer \u521d\u59cb\u5316\u4e4b\u540e\uff0c\u521b\u5efa\u7684\u5bf9\u8c61\u5982\u4e0b\u6240\u793a</p> <p></p> <p><code>KafkaProducer</code> \u6784\u9020\u65b9\u6cd5\u4e2d\uff0c\u521d\u59cb\u5982\u4e0b\u5173\u952e\u5c5e\u6027</p> <ul> <li><code>partitioner</code></li> <li><code>accumulator</code></li> <li><code>metadata</code> <pre><code>this.metadata = new ProducerMetadata(retryBackoffMs,\n        config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),\n        config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),\n        logContext,\n        clusterResourceListeners,\n        Time.SYSTEM);\nthis.metadata.bootstrap(addresses);\n</code></pre></li> <li><code>this.sender = newSender(logContext, kafkaClient, this.metadata)</code></li> </ul> <p>\u540c\u65f6\u5f00\u542f\u7ebf\u7a0b\uff0c\u8fd0\u884c <code>sender</code></p> <pre><code>this.ioThread = new KafkaThread(ioThreadName, this.sender, true);\nthis.ioThread.start();\n</code></pre>"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#send","title":"send() \u505a\u4e86\u4ec0\u4e48\uff1f","text":"<p><code>send()</code> \u662f\u5f02\u6b65\u7684\uff0c\u8c03\u7528\u4f1a\u7acb\u5373\u8fd4\u56de\uff0crecord \u4f1a\u88ab\u5b58\u50a8\u5230 the buffer of records \u7b49\u5f85\u88ab\u53d1\u9001\u3002This allows sending many records in parallel without blocking to wait for the response after each one.</p> <ol> <li>\u4f7f\u7528 <code>ProducerInterceptors</code> \u5bf9 record \u8fdb\u884c\u5904\u7406</li> <li>\u8c03\u7528 <code>doSend</code><ol> <li><code>throwIfProducerClosed</code>\uff0c\u68c0\u67e5 <code>sender</code> \u662f\u5426\u5b58\u5728\u4e14\u6b63\u5728\u8fd0\u884c</li> <li>make sure the metadata for the topic is available\uff0c\u8fd9\u91cc\u4f1a\u4e00\u76f4\u5c1d\u8bd5\u83b7\u53d6 metadata\uff0c\u76f4\u5230\u8d85\u8fc7 maxBlockTimeMs     <pre><code>long nowMs = time.milliseconds();\nClusterAndWaitTime clusterAndWaitTime;\ntry {\n    clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);\n} catch (KafkaException e) {\n    if (metadata.isClosed())\n        throw new KafkaException(\"Producer closed while send in progress\", e);\n    throw e;\n}\nnowMs += clusterAndWaitTime.waitedOnMetadataMs;\nlong remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);\nCluster cluster = clusterAndWaitTime.cluster;\n</code></pre></li> <li>\u5e8f\u5217\u5316 key, value</li> <li>\u8ba1\u7b97\u6d88\u606f\u6240\u5904\u5206\u533a</li> <li>\u8ba1\u7b97\u6d88\u606f\u5927\u5c0f\uff0c\u786e\u5b9a\u4e0d\u8d85\u8fc7 <code>max.request.size</code>, <code>buffer.memory</code></li> <li>\u83b7\u53d6\u6d88\u606f timestamp\uff0c\u5982\u679c\u6ca1\u6709\u5c31\u7528\u5f53\u524d\u65f6\u95f4</li> <li>\u5c06\u6d88\u606f\u8ffd\u52a0\u5230 accumulator     <pre><code>RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs, true, nowMs);\n</code></pre></li> <li>\u5982\u679c <code>batch</code> \u6ee1\u4e86\u6216\u8005\u65b0\u521b\u5efa\u4e86 <code>batch</code>\uff0c\u5524\u9192 sender     <pre><code>if (result.batchIsFull || result.newBatchCreated) {\n    log.trace(\"Waking up the sender since topic {} partition {} is either full or getting a new batch\", record.topic(), partition);\n    this.sender.wakeup();\n}\n</code></pre></li> </ol> </li> </ol> <p>\u4e4b\u540e\u4f1a\u7531\u72ec\u7acb\u7ebf\u7a0b\u7684 <code>Sender</code> \u4ece <code>RecordAccumulator</code> \u4e2d\u53d6\u5f97\u6d88\u606f\u7136\u540e\u53d1\u9001\u5230 Kafka</p>"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#sender_1","title":"Sender \u505a\u4e86\u4ec0\u4e48\uff1f","text":"<p>Sender \u662f\u4f5c\u4e3a\u72ec\u7acb\u7ebf\u7a0b\u8fd0\u884c\uff0c\u4e3b\u8981\u903b\u8f91\u5728 <code>run</code> \u51fd\u6570\u4e2d:</p> <pre><code>// main loop, runs until close is called\nwhile (running) {\n    try {\n        runOnce();\n    } catch (Exception e) {\n        log.error(\"Uncaught error in kafka producer I/O thread: \", e);\n    }\n}\n</code></pre> <p><code>run</code> \u5faa\u73af\u8fd0\u884c <code>runOnce</code>\uff0c<code>runOnce</code> \u4e3b\u8981\u6709 2 \u6b65\uff08\u5148\u4e0d\u8ba8\u8bba\u4e8b\u52a1\u6027\u652f\u6301\uff09\uff1a</p> <ul> <li>sendProducerData  // \u5c06 record batch \u8f6c\u79fb\u5230\u6bcf\u4e2a\u8282\u70b9\u7684\u751f\u4ea7\u8bf7\u6c42\u5217\u8868\u4e2d<ol> <li>\u83b7\u53d6 metadata</li> <li>\u4ece accumulator \u7684 batches \u4e2d\u53d6\u51fa\u53ef\u4ee5\u53d1\u9001\u7684\u6570\u636e\uff0c<ol> <li>\u5f97\u5230 batches \u5bf9\u5e94\u7684 leader \u8282\u70b9\u5217\u8868 \uff08<code>RecordAccumulator.ReadyCheckResult result.readyNodes</code>\uff09</li> <li>\u5f97\u5230\u5728 metadata \u91cc\u627e\u4e0d\u5230 leader \u7684 topic \u5217\u8868\uff08<code>RecordAccumulator.ReadyCheckResult result.unknownLeaderTopics</code>\uff09</li> </ol> </li> <li>\u5982\u679c\u6709\u4efb\u4f55 partition \u7684 leader \u627e\u4e0d\u5230\uff0c\u66f4\u65b0 metadata</li> <li>\u904d\u5386 leader node \u5217\u8868(<code>result.readyNodes</code>)<ol> <li>\u5224\u65ad leader node \u662f\u5426\u5df2\u7ecf\u8fde\u63a5\u5e76\u53ef\u4ee5\u53d1\u9001\u6570\u636e<ol> <li>\u5982\u679c\u4e0d\u662f\uff0c\u5219 \u5f00\u59cb\u8fde\u63a5\uff0c\u5e76\u5728 <code>readyNodes</code> \u4e2d\u79fb\u9664\u8fd9\u4e2a leader node</li> </ol> </li> </ol> </li> <li>\u83b7\u53d6\u6bcf\u4e2a leader \u5bf9\u5e94\u7684\u53ef\u4ee5\u53d1\u9001\u7684 <code>ProducerBatch</code> \u5217\u8868</li> <li>\u5728 Sender \u7684 inFlightBatches \u91cc\u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 batch \u5217\u8868</li> <li>\u5982\u679c\u9700\u8981\u4fdd\u8bc1\u6d88\u606f\u7684\u987a\u5e8f\uff0c\u5c06 topicPartition \u52a0\u5165\u5230 accumulator \u7684 muted \u8bb0\u5f55</li> <li><code>sendProduceRequests(batches, now)</code> <pre><code>/**\n * Transfer the record batches into a list of produce requests on a per-node basis\n */\nprivate void sendProduceRequests(Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; collated, long now) {\n    for (Map.Entry&lt;Integer, List&lt;ProducerBatch&gt;&gt; entry : collated.entrySet())\n        sendProduceRequest(now, entry.getKey(), acks, requestTimeoutMs, entry.getValue());\n}\n</code></pre></li> <li><code>sendProduceRequest</code> \u91cc\u6784\u9020 <code>ProduceRequest.Builder</code>\uff0c\u6784\u9020 <code>ClientRequest</code>\uff0c\u5e76\u8c03\u7528 <code>NetworkClient</code> \u7684 <code>send()</code> \u51fd\u6570\u53d1\u9001</li> </ol> </li> <li>client.poll  // <code>NetworkClient.poll</code> Do actual reads and writes to sockets</li> </ul> <p>\u8fd9\u91cc\u6709\u4e3b\u8981\u7684\u51e0\u70b9\uff1a</p> <ol> <li>\u4f55\u65f6\u4e0e broker \u5efa\u7acb\u8fde\u63a5\uff1a<ol> <li>\u83b7\u53d6 <code>readyNodes</code> \u4e4b\u540e\uff0c\u4f1a\u904d\u5386 <code>readyNodes</code> \u68c0\u67e5\u68c0\u67e5 node \u662f\u5426\u5df2\u7ecf\u8fde\u63a5\uff0c\u8fd9\u91cc\u8c03\u7528 <code>NetworkClient</code>.<code>ready()</code></li> <li><code>NetworkClient</code>.<code>ready()</code> \u4e2d\u5982\u679c\u5224\u65ad node \u5df2\u8fde\u63a5\uff0c\u4f1a\u76f4\u63a5\u8fd4\u56de <code>true</code>\uff0c\u5982\u679c\u6ca1\u6709\u8fde\u63a5\uff0c\u5219\u8fdb\u884c\u8fde\u63a5\uff0c\u5e76\u8fd4\u56de <code>false</code> <pre><code>@Override\npublic boolean ready(Node node, long now) {\n    if (node.isEmpty())\n        throw new IllegalArgumentException(\"Cannot connect to empty node \" + node);\n\n    if (isReady(node, now))\n        return true;\n\n    if (connectionStates.canConnect(node.idString(), now))\n        // if we are interested in sending to a node and we don't have a connection to it, initiate one\n        initiateConnect(node, now);\n\n    return false;\n}\n</code></pre></li> <li><code>NetworkClient</code>.<code>initiateConnect()</code> \u4e2d\u901a\u8fc7 <code>Selector</code>.<code>connect</code> \u4e0e node \u8fdb\u884c\u8fde\u63a5     <pre><code>private void initiateConnect(Node node, long now) {\n    String nodeConnectionId = node.idString();\n    try {\n        connectionStates.connecting(nodeConnectionId, now, node.host(), clientDnsLookup);\n        InetAddress address = connectionStates.currentAddress(nodeConnectionId);\n        log.debug(\"Initiating connection to node {} using address {}\", node, address);\n        selector.connect(nodeConnectionId,\n                new InetSocketAddress(address, node.port()),\n                this.socketSendBuffer,\n                this.socketReceiveBuffer);\n    } catch (IOException e) {\n        log.warn(\"Error connecting to node {}\", node, e);\n        // Attempt failed, we'll try again after the backoff\n        connectionStates.disconnected(nodeConnectionId, now);\n        // Notify metadata updater of the connection failure\n        metadataUpdater.handleServerDisconnect(now, nodeConnectionId, Optional.empty());\n    }\n}\n</code></pre></li> <li><code>Selector</code>.<code>connect</code>:<ol> <li>\u521b\u5efa <code>SocketChannel</code>\uff0c\u8fdb\u884c\u8fde\u63a5 node \u64cd\u4f5c</li> <li>\u6ce8\u518c <code>SocketChannel</code> \u5230 <code>Selector</code>.<code>nioSelector</code></li> <li>\u6784\u5efa <code>KafkaChannel</code>\uff08\u901a\u8fc7 (Ssl/Sasl/Plaintext)ChannelBuilder \u6784\u5efa\uff09</li> <li>\u5c06 <code>KafakChannel</code> \u8bb0\u5f55\u5230 <code>Selector</code>.<code>channels</code> \u4e2d</li> </ol> </li> </ol> </li> </ol> <p>kafka \u53d1\u9001\u6d88\u606f\u65f6\u4ee5\u8282\u70b9\u7ec4\u7ec7\u53ef\u53d1\u9001\u7684\u6d88\u606f\uff0c\u5c06\u53d1\u5f80\u540c\u4e00\u4e2a\u8282\u70b9\u7684\u4e0d\u540c topicPartion \u7684\u6570\u636e\u653e\u5728\u4e00\u4e2a\u8bf7\u6c42\u4e2d\u53d1\u9001</p>"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#networkclient_1","title":"NetworkClient \u662f\u5982\u4f55\u53d1\u9001\u6570\u636e\u7684\uff1f","text":""},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#selector-poll","title":"Selector \u7684 poll","text":"<ul> <li><code>poll</code><ul> <li><code>pollSelectionKeys(Set&lt;SelectionKey&gt; selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos)</code><ul> <li><code>attempRead(KafkaChannel channel)</code>: <ol> <li>\u8c03\u7528 <code>channel.read()</code>\uff0c\u8fd4\u56de\u8bfb\u53d6\u7684 byte \u6570\uff0c<ul> <li><code>KafkaChannel</code>.<code>read()</code><ul> <li><code>KafkaChannel</code>.<code>receive()</code> \u901a\u8fc7\u8c03\u7528\u81ea\u8eab\u6240\u6301\u6709\u7684 <code>NetworkReceive</code> \u7684 <code>readFrom()</code> \u51fd\u6570\u8fdb\u884c\u6570\u636e\u8bfb\u53d6</li> </ul> </li> </ul> </li> <li>\u901a\u8fc7 <code>channel.maybeCompleteReceive()</code> \u5224\u65ad\u662f\u5426\u6536\u5230\u5b8c\u6574\u7684 <code>NetworkReceive</code>\uff0c\u5982\u679c\u662f\uff0c\u5c06 receive \u52a0\u5165\u5230\u81ea\u8eab\u7684 <code>completedReceives</code></li> </ol> </li> <li><code>attemptWrite(SelectionKey key, KafkaChannel channel, long nowNanos)</code><ul> <li><code>write(KafkaChannel channel)</code>: <ol> <li>\u8c03\u7528 <code>channel.write()</code>\uff0c\u8fd4\u56de\u53d1\u9001 byte \u6570\uff0c<ul> <li><code>KafkaChannel</code>.<code>write()</code> \u901a\u8fc7\u8c03\u7528\u81ea\u8eab\u6240\u6301\u6709 <code>NetworkSend</code> \u7684 <code>writeTo()</code> \u51fd\u6570\u8fdb\u884c\u6570\u636e\u53d1\u9001<ul> <li><code>java.nio.channels.GatheringByteChannel</code>.<code>write()</code></li> </ul> </li> </ul> </li> <li>\u901a\u8fc7 <code>channel.maybeCompleteSend()</code> \u5224\u65ad <code>NetworkSend</code> \u662f\u5426\u53d1\u9001\u5b8c\u6210\uff0c\u5982\u679c\u662f\uff0c\u5c06 send \u52a0\u5165\u5230\u81ea\u8eab\u7684 <code>completedSends</code></li> </ol> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>\u8fd9\u91cc\u6709\u4e3b\u8981\u7684\u51e0\u70b9\uff1a</p> <ol> <li>\u4f55\u65f6\u8fdb\u884c ssl, sasl \u63e1\u624b\uff1a\u5728 <code>Sender</code>.<code>run()</code> \u4e2d\u4f1a\u68c0\u67e5\u8981\u53d1\u9001\u6570\u636e\u5bf9\u5e94\u7684 leader \u662f\u5426\u8fde\u63a5\uff0c\u5982\u679c\u6ca1\u6709\u5219\u8fdb\u884c\u8fde\u63a5\uff0c\u5728 <code>Selector</code>.<code>pollSelectionKeys()</code> \u4e2d\uff0c\u5982\u679c key \u5bf9\u5e94\u7684 channel \u5df2\u8fde\u63a5\u4f46\u8fd8\u672a ready\uff0c\u5219\u8c03\u7528 <code>KafkaChannel</code>.<code>prepare()</code> \u8fdb\u884c ssl \u63e1\u624b\u548c sasl \u63e1\u624b</li> </ol>"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/1-producer%20%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/#_2","title":"\u53c2\u8003","text":"<p>https://blog.csdn.net/chunlongyu/article/details/52651960</p>"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/2-producer%20%E5%B9%82%E7%AD%89%E6%80%A7/","title":"2.2 producer \u5e42\u7b49\u6027","text":""},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/3-producer%20%E5%8A%A0%E5%AF%86%20%E8%AE%A4%E8%AF%81/","title":"2.3 producer \u52a0\u5bc6/\u8ba4\u8bc1","text":""},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/3-producer%20%E5%8A%A0%E5%AF%86%20%E8%AE%A4%E8%AF%81/#transportlayer","title":"TransportLayer","text":"<p>PlaintextTransportLayer, SslTransportLayer</p>"},{"location":"2-producer%20%E8%A7%A3%E6%9E%90/3-producer%20%E5%8A%A0%E5%AF%86%20%E8%AE%A4%E8%AF%81/#authenticator","title":"Authenticator","text":""},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/","title":"3. consumer \u89e3\u6790","text":""},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/#consumer","title":"consumer \u4ec0\u4e48\u65f6\u5019\u88ab\u8ba4\u4e3a\u4e0b\u7ebf","text":"<ol> <li>\u5f53 subscribe \u5b8c\u6210\u5e76\u8c03\u7528 poll() \u4e4b\u540e\uff0cconsumer \u4f1a\u81ea\u52a8\u52a0\u5165 group\uff0c\u5e76\u5468\u671f\u6027\u7684\u5411 server \u53d1\u9001 heartbeat\uff0c\u5f53 server \u8ddd\u4e0a\u4e00\u6b21\u6536\u5230 heartbeat \u7684\u65f6\u95f4\u8d85\u8fc7 <code>session.timeout.ms</code> \u540e\uff0c\u4f1a\u8ba4\u4e3a consumer dead</li> <li>\u53e6\u4e00\u79cd\u60c5\u51b5\u662f consumer \u5728\u6301\u7eed\u53d1\u9001 heartbeat\uff0c\u4f46\u662f\u6ca1\u6709\u5b9e\u9645\u7684\u6d88\u8d39\u884c\u4e3a\uff0c\u5f53\u8ddd\u4e0a\u4e00\u6b21\u8c03\u7528 poll() \u7684\u65f6\u95f4\u8d85\u8fc7 <code>max.poll.interval.ms</code> \u540e\uff0c\u4f1a\u5c06 consumer \u79fb\u51fa group\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b consumer \u53ef\u80fd\u4f1a\u5728\u8c03\u7528 commitSync() \u65f6\u51fa\u73b0 offset commit failure\uff0c\u8fd9\u662f\u6b63\u5e38\u7684\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u53ea\u63a5\u53d7\u6765\u81ea\u6d3b\u8dc3 consumer \u7684 offset\u3002</li> </ol> <p>consumer \u63d0\u4f9b 2 \u4e2a\u53c2\u6570\u6765\u63a7\u5236 poll loop \u7684\u884c\u4e3a\uff1a</p> <ul> <li><code>max.poll.interval.ms</code>: \u901a\u8fc7\u63d0\u9ad8\u8fd9\u4e2a\u53c2\u6570\uff0c\u53ef\u4ee5\u7ed9 consumer \u66f4\u591a\u7684\u65f6\u95f4\u5904\u7406\u5355\u6b21 poll \u62c9\u53d6\u7684\u6570\u636e\u3002</li> <li><code>max.poll.records</code>: \u9650\u5236\u5355\u6b21 poll \u8c03\u7528\u62c9\u53d6\u7684\u8bb0\u5f55\u6570\u3002</li> </ul> <p>\u5bf9\u4e8e\u6d88\u606f\u5904\u7406\u7528\u65f6\u65e0\u6cd5\u9884\u6d4b\u7684\u7528\u6237\u573a\u666f\uff0c\u4e0a\u9762 2 \u4e2a\u53c2\u6570\u662f\u4e0d\u591f\u7684\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b\u63a8\u8350\u7684\u65b9\u5f0f\u5c06\u5904\u7406\u6d88\u606f\u7684\u6b65\u9aa4\u653e\u5230\u5355\u72ec\u7684\u7ebf\u7a0b\uff0c\u786e\u4fdd consumer \u53ef\u4ee5\u6301\u7eed\u7684\u8c03\u7528 poll\uff0c\u5173\u95ed\u81ea\u52a8 offset commit\uff0c\u5728\u6d88\u606f\u5904\u7406\u5b8c\u4e4b\u540e\u8c03\u7528 commitSync/commitAsync\u3002</p> <p>\u81ea\u52a8\u63d0\u4ea4 offset\uff1a</p> <ul> <li><code>enable.auto.commit</code></li> <li><code>auto.commit.interval.ms</code></li> </ul> <p>\u624b\u52a8\u63d0\u4ea4 offset \u7684 2 \u79cd\u65b9\u5f0f\uff1a</p> <ul> <li><code>commitSync</code></li> <li><code>commitAsync</code></li> </ul> <p>\u8ba2\u9605 topic \u7684 2 \u79cd\u65b9\u5f0f\uff1a</p> <ul> <li><code>subscribe</code>: topic \u7c92\u5ea6\uff0c\u5177\u4f53\u7684 partition \u5206\u914d\u7531\u6d88\u8d39\u7ec4\u534f\u8c03\u8005\u5206\u914d</li> <li><code>assign</code>: topic-partition \u7c92\u5ea6\uff0c\u53ef\u4ee5\u7531\u81ea\u5df1\u5206\u914d partition</li> </ul>"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/#other","title":"other","text":"<ul> <li><code>offsets.retention.minutes</code>: \u6d88\u8d39\u7ec4\u8bb0\u5f55\u4fdd\u7559\u65f6\u95f4</li> </ul>"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/1-%E6%B6%88%E8%B4%B9%E7%BB%84%E7%AE%A1%E7%90%86/","title":"3.1 \u6d88\u8d39\u7ec4\u7ba1\u7406","text":""},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/1-%E6%B6%88%E8%B4%B9%E7%BB%84%E7%AE%A1%E7%90%86/#_1","title":"\u5206\u533a\u5206\u914d","text":"<ul> <li><code>partition.assignment.strategy</code><ul> <li><code>org.apache.kafka.clients.consumer.RangeAssignor</code></li> <li><code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code></li> </ul> </li> </ul>"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/1-%E6%B6%88%E8%B4%B9%E7%BB%84%E7%AE%A1%E7%90%86/#__consumer_offsets","title":"__consumer_offsets","text":"<ul> <li>Key<ul> <li>int16 version</li> <li>string group</li> <li>string topic</li> <li>int32 partition</li> </ul> </li> <li>Value<ul> <li>int16 version</li> <li>int64 offset</li> <li>string metadata</li> <li>int64 commit_timestamp</li> <li>int64 expire_timestamp</li> </ul> </li> </ul>"},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/2-consumer%20%E6%8B%89%E5%8F%96%E6%B6%88%E6%81%AF/","title":"3.1 consumer \u62c9\u53d6\u6d88\u606f","text":""},{"location":"3-consumer%20%E8%A7%A3%E6%9E%90/2-consumer%20%E6%8B%89%E5%8F%96%E6%B6%88%E6%81%AF/#_1","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>Properties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\nprops.put(\"group.id\", \"test\");\nprops.put(\"enable.auto.commit\", \"true\");\nprops.put(\"auto.commit.interval.ms\", \"1000\");\nprops.put(\"session.timeout.ms\", \"30000\");\nprops.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\nprops.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n\nKafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);\n\nconsumer.subscribe(Arrays.asList(\"foo\", \"bar\"));\n\nwhile (true) {\n    ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);\n    for (ConsumerRecord&lt;String, String&gt; record : records)\n        System.out.printf(\"offset = %d, key = %s, value = %s\", record.offset(), record.key(), record.value());\n}\n</code></pre>"},{"location":"4-kafka-network-io/","title":"4 kafka \u5bf9 nio \u7684\u5305\u88c5","text":""},{"location":"4-kafka-network-io/#networksend","title":"NetworkSend","text":"<p>\u5c5e\u6027\uff1a</p> <ul> <li><code>destination</code>: \u4ee3\u8868\u76ee\u6807\u8282\u70b9\uff0c\u4e0e KafkaChannel \u7684 id \u4e00\u81f4</li> <li><code>buffers</code>: \u53d1\u9001\u7684\u6570\u636e</li> </ul> <p>\u4f5c\u7528\uff1a</p> <ul> <li>\u4fee\u6539\u539f\u59cb\u53d1\u9001\u7684 ByteBuffer\uff0c\u5c06\u524d 4 \u5b57\u8282\u4fee\u6539\u4e3a ByteBuffer \u7684\u5927\u5c0f\uff0c\u539f\u59cb\u5185\u5bb9\u540e\u79fb 4 \u5b57\u8282</li> <li>\u5b9e\u73b0 <code>writeTo</code> \u65b9\u6cd5\uff0c\u5c06\u81ea\u8eab\u6570\u636e buffers \u5199\u5165 java nio channel\uff0c\u8fd4\u56de\u5199\u5165\u5b57\u8282\u6570</li> </ul> <p>\u53d1\u9001 <code>NetworkSend</code> \u65f6\uff0c\u53ef\u4ee5\u6839\u636e destination \u627e\u5230\u5bf9\u5e94\u7684 <code>KafkaChannel</code>\uff0c</p>"},{"location":"4-kafka-network-io/#networkreceive","title":"NetworkReceive","text":"<p>\u5c5e\u6027\uff1a</p> <ul> <li><code>source</code>: \u4ee3\u8868\u6765\u6e90\u8282\u70b9\uff0c\u4e0e KafkaChannel \u7684 id \u4e00\u81f4</li> <li><code>size</code>: \u5b58\u653e\u63a5\u6536\u6570\u636e\u524d 4 byte\uff0c\u5373\u6b64\u6b21\u6570\u636e\u7684\u5927\u5c0f</li> <li><code>buffer</code>: \u5b58\u653e\u63a5\u6536\u5230\u7684\u6570\u636e</li> </ul> <p>\u4f5c\u7528\uff1a</p> <ul> <li>\u5b9e\u73b0 <code>readFrom</code> \u65b9\u6cd5\uff0c\u4ece java nio channel \u4e2d\u8bfb\u6570\u636e\u5230 <code>buffer</code> \u4e2d<ul> <li>\u9996\u5148\u8bfb\u53d6\u524d 4 \u5b57\u8282\u6570\u636e\u5230 <code>size</code>\uff0c\u524d 4 \u5b57\u8282\u4e3a\u5f53\u524d\u6574\u4e2a\u54cd\u5e94\u5927\u5c0f</li> <li>\u5f97\u5230\u54cd\u5e94\u5927\u5c0f\u540e\uff0c\u521d\u59cb\u5316 <code>buffer</code>\uff0c\u5e76\u5c06\u540e\u7eed\u54cd\u5e94\u6570\u636e\u8bfb\u53d6\u5230 <code>buffer</code></li> </ul> </li> </ul>"},{"location":"4-kafka-network-io/#kafkachannel","title":"KafkaChannel","text":"<p>channel \u8868\u793a\u4e00\u4e2a\u5177\u4f53\u7684 TCP \u8fde\u63a5</p> <p>\u5c5e\u6027\uff1a</p> <ul> <li> <p><code>id</code>: \u8868\u793a channel \u5bf9\u5e94\u7684\u8282\u70b9\u3002selector \u53ef\u4ee5\u6839\u636e id \u53d6\u5230\u5bf9\u5e94\u7684 KafkaChannel</p> </li> <li> <p><code>Send(NetworkSend)</code>: \u53ea\u80fd\u6301\u6709\u4e00\u4e2a send\uff0cwrite \u53d1\u9001\u5f53\u524d send \u4e4b\u540e\uff0c\u4f1a\u5c06 send \u7f6e\u4e3a null\uff0c\u4e4b\u540e\u53ef\u4ee5\u8bbe\u7f6e\u4e0b\u4e00\u4e2a send</p> </li> <li> <p><code>NetworkReceive</code>: \u53ea\u80fd\u6301\u6709\u4e00\u4e2a receive\uff0cread \u63a5\u6536\u5b8c\u6574\u6570\u636e\u540e\uff0c\u5c06 receive \u7f6e\u4e3a null, \u4e4b\u540e\u53ef\u4ee5\u63a5\u6536\u4e0b\u4e00\u4e2a receive</p> </li> <li> <p><code>TransportLayer</code>: send, receive \u90fd\u901a\u8fc7 TransportLayer \u8f6c\u53d1\u7ed9 SocketChannel</p> </li> <li><code>Authenticator</code>: \u63d0\u4f9b\u5bf9\u8fde\u63a5\u7684\u8ba4\u8bc1\u652f\u6301\uff0c\u5411 TransportLayer \u8f6c\u53d1\u8bfb\u5199</li> </ul>"},{"location":"4-kafka-network-io/#transportlayer","title":"TransportLayer","text":"<p>\u7ee7\u627f\u81ea ScatteringByteChannel,GatheringByteChannel</p> <p>\u6301\u6709 <code>SelectionKey</code>, <code>SocketChannel</code></p> <p>KafkaChannel \u6240\u6709\u8bfb\u5199\u90fd\u7ecf\u8fc7 TransportLayer \u8f6c\u53d1\u7ed9\u539f\u6765\u7684 SocketChannel</p> <p>TransportLayer \u76f8\u6bd4\u4e8e\u539f\u6765\u7684 Channel \u4e3b\u8981\u4f5c\u7528\u662f\u53ef\u4ee5\u589e\u52a0\u4e86 ssl:</p> <ul> <li><code>PlaintextTransportLayer</code>\uff1a\u76f4\u63a5\u8f6c\u53d1\u8bf7\u6c42\u7ed9\u539f\u6765\u7684 <code>SocketChannel</code></li> <li><code>SslTransportLayer</code>\uff1a\u5b9e\u73b0 handshake \u65b9\u6cd5\u5b8c\u6210 ssl \u63e1\u624b\uff0cread, write \u65b9\u6cd5\u9700\u8981\u901a\u8fc7 sslEngine \u5bf9\u6570\u636e\u8fdb\u884c\u5bf9\u5e94\u7684 unwrap \u4e0e wrap</li> </ul>"},{"location":"4-kafka-network-io/#authenticator","title":"Authenticator","text":"<p>Authenticator \u63d0\u4f9b <code>authenticate()</code> \u65b9\u6cd5\u5b8c\u6210\u5bf9\u8fde\u63a5\u7684\u8ba4\u8bc1\uff0c\u63d0\u4f9b <code>principal()</code> \u65b9\u6cd5\u83b7\u53d6\u8fd9\u4e2a\u8fde\u63a5\u7684\u7528\u6237\u8eab\u4efd</p> <ul> <li><code>PlaintextAuthenticator</code>: <code>authenticate()</code> \u65b9\u6cd5\u4e3a\u7a7a\uff0c<code>principal()</code> \u65b9\u6cd5\u8fd4\u56de\u533f\u540d\u7528\u6237 <code>KafkaPrincipal.ANONYMOUS</code></li> <li><code>SslAuthenticator</code>: <code>authenticate()</code> \u65b9\u6cd5\u4e3a\u7a7a\uff0c<code>principal()</code> \u65b9\u6cd5\u53ef\u4ee5\u6839\u636e\u89c4\u5219\u6620\u5c04\u63d0\u53d6\uff0c\u5931\u8d25\u8fd4\u56de\u533f\u540d\u7528\u6237</li> <li><code>SaslClientAuthenticator</code></li> <li><code>SaslServerAuthenticator</code>: kafka server \u7aef\u7684 authenticator\uff0c<code>authenticate()</code> \u65b9\u6cd5\u5b8c\u6210\u5bf9\u5ba2\u6237\u7aef\u7684 SASL \u63e1\u624b\uff0c<code>principal()</code> \u65b9\u6cd5\u8fd4\u56de SASL \u7528\u6237</li> </ul>"},{"location":"4-kafka-network-io/#saslserverauthenticator","title":"SaslServerAuthenticator","text":"<p>\u6bcf\u4e2a\u8fde\u63a5\u5173\u8054\u4e00\u79cd\u7279\u5b9a\u7684 mechanism\uff0c\u4e0d\u540c mechanism \u5bf9\u5e94\u4e0d\u540c\u7684 callbackHandlers</p>"},{"location":"4-kafka-network-io/#authenticatecallbackhandler","title":"AuthenticateCallbackHandler","text":"<ul> <li>PlainServerCallbackHandler</li> <li>ScramServerCallbackHandler</li> <li>SaslClientCallbackHandler</li> <li>SaslServerCallbackHandler</li> </ul>"},{"location":"4-kafka-network-io/#selector","title":"Selector","text":"<p>\u53ef\u4ee5\u76d1\u542c\u591a\u4e2a\u8fde\u63a5\uff0c\u6bcf\u4e2a\u8fde\u63a5\u7531 id \u4ee5\u53ca\u5bf9\u5e94\u7684 KafkaChannel \u6807\u8bc6</p> <pre><code>private final Map&lt;String, KafkaChannel&gt; channels;\nprivate final Map&lt;String, KChannel&gt; closingKChannels;\n\nprivate final List&lt;String&gt; connected;\nprivate final List&lt;String&gt; disconnected;\nprivate final Set&lt;SelectionKey&gt; immediatelyConnectedKeys;\n\nprivate final List&lt;KSend&gt; completedSends;\nprivate final LinkedHashMap&lt;String, KReceive&gt; completedReceives;\n</code></pre>"},{"location":"4-kafka-network-io/#connect","title":"connect","text":"<pre><code>connect(String id, InetSocketAddress address)\n</code></pre> <ol> <li>\u8fde\u63a5 address\uff0c\u5f97\u5230 socketChannel</li> <li>\u6ce8\u518c socketChannel \u76d1\u542c connect \u4e8b\u4ef6</li> <li>\u5c06 socketChannel \u5305\u88c5\u4e3a KafkaChannel</li> <li>\u5c06 id \u4e0e\u8fdc\u7a0b\u8fde\u63a5\u7684 KafkaChannel \u5bf9\u5e94\u8d77\u6765</li> </ol>"},{"location":"4-kafka-network-io/#send","title":"send","text":"<pre><code>send(KafkaSend send)\n</code></pre> <ol> <li>\u53d1\u9001 NetworkSend<ol> <li>\u6839\u636e NetworkSend \u7684 destination \u53d6\u5230\u5bf9\u5e94\u7684 KafkaChannel</li> <li>\u8bbe\u7f6e KafkaChannel \u7684 send</li> </ol> </li> </ol>"},{"location":"4-kafka-network-io/#poll","title":"poll","text":"<p>nio select key\uff0c\u6839\u636e key \u627e\u5230\u5bf9\u5e94\u7684 KafkaChannel\uff0c\u6267\u884c KafkaChannel \u7684 read, write \u65b9\u6cd5</p>"},{"location":"4-kafka-network-io/#completedreceives","title":"completedReceives","text":"<pre><code>Collection&lt;KReceive&gt; completedReceives()\n</code></pre> <p>\u83b7\u53d6\u5b8c\u6210\u7684\u54cd\u5e94\uff0c\u6bcf\u6b21 poll() \u7ed3\u675f\u540e\u8981\u83b7\u53d6\u4e00\u6b21\uff0c\u5426\u5219\u4e0b\u6b21 poll() \u4f1a\u6e05\u7a7a\u3002</p>"},{"location":"4-kafka-network-io/#registerchannel","title":"registerChannel","text":"<pre><code>SelectionKey registerChannel(String id, SocketChannel socketChannel, int interestedOps)\n</code></pre> <ol> <li>\u6ce8\u518c socketChannel \u76d1\u542c interestedOps \u4e8b\u4ef6</li> <li>\u5c06 socketChannel \u5305\u88c5\u4e3a KafkaChannel</li> <li>\u5c06 id \u4e0e\u8fdc\u7a0b\u8fde\u63a5\u7684 KafkaChannel \u5bf9\u5e94\u8d77\u6765</li> </ol>"},{"location":"4-kafka-network-io/#_1","title":"\u8865\u5145\u77e5\u8bc6","text":""},{"location":"4-kafka-network-io/#interestops","title":"interestOps","text":"SelectionKey.OP_READ <code>1&lt;&lt;0</code> 0000 0001 SelectionKey.OP_WRITE <code>1&lt;&lt;2</code> 0000 0100 SelectionKey.OP_CONNECT <code>1&lt;&lt;3</code> 0000 1000 SelectionKey.OP_ACCEPT <code>1&lt;&lt;4</code> 0001 0000 0000 0000 <p>\u53d6\u6d88 OP_CONNECT\uff0c\u589e\u52a0 OP_READ</p> <pre><code>key.interestOps(key.interestOps() &amp; ~SelectionKey.OP_CONNECT | SelectionKey.OP_READ);\n</code></pre>"},{"location":"4-kafka-network-io/#scatteringbytechannelgatheringbytechannel","title":"ScatteringByteChannel,GatheringByteChannel","text":"<ul> <li>scatteringbytechannel: \u4ece\u901a\u9053\u8bfb\u53d6\u6570\u636e\u5206\u6563\u5230\u591a\u4e2a\u7f13\u51b2\u533a buffer\uff0c</li> <li>gatheringbytechannel: \u5c06\u591a\u4e2a\u7f13\u51b2\u533a buffer \u805a\u96c6\u8d77\u6765\u5199\u5230\u901a\u9053</li> </ul>"},{"location":"4-kafka-network-io/1-KafkaChannel/","title":"KafkaChannel","text":"<p>channel \u8868\u793a\u4e00\u4e2a\u5177\u4f53\u7684 TCP \u8fde\u63a5</p> <p>\u5c5e\u6027\uff1a</p> <ul> <li> <p><code>id</code>: \u8868\u793a channel \u5bf9\u5e94\u7684\u8282\u70b9\u3002selector \u53ef\u4ee5\u6839\u636e id \u53d6\u5230\u5bf9\u5e94\u7684 KafkaChannel</p> </li> <li> <p><code>Send(NetworkSend)</code>: \u53ea\u80fd\u6301\u6709\u4e00\u4e2a send\uff0cwrite \u53d1\u9001\u5f53\u524d send \u4e4b\u540e\uff0c\u4f1a\u5c06 send \u7f6e\u4e3a null\uff0c\u4e4b\u540e\u53ef\u4ee5\u8bbe\u7f6e\u4e0b\u4e00\u4e2a send</p> </li> <li> <p><code>NetworkReceive</code>: \u53ea\u80fd\u6301\u6709\u4e00\u4e2a receive\uff0cread \u63a5\u6536\u5b8c\u6574\u6570\u636e\u540e\uff0c\u5c06 receive \u7f6e\u4e3a null, \u4e4b\u540e\u53ef\u4ee5\u63a5\u6536\u4e0b\u4e00\u4e2a receive</p> </li> <li> <p><code>TransportLayer</code>: send, receive \u90fd\u901a\u8fc7 TransportLayer \u8f6c\u53d1\u7ed9 SocketChannel</p> </li> <li><code>Authenticator</code>: \u63d0\u4f9b\u5bf9\u8fde\u63a5\u7684\u8ba4\u8bc1\u652f\u6301\uff0c\u5411 TransportLayer \u8f6c\u53d1\u8bfb\u5199</li> </ul>"},{"location":"4-kafka-network-io/1-KafkaChannel/#_1","title":"\u7528\u6237\u4fe1\u606f","text":"<p>\u8fd9\u4e2a TCP \u5173\u8054\u7684\u7528\u6237\u4fe1\u606f</p> <pre><code>    /**\n     * Returns the principal returned by `authenticator.principal()`.\n     */\n    public KafkaPrincipal principal() {\n        return authenticator.principal();\n    }\n</code></pre>"},{"location":"4-kafka-network-io/1-KafkaChannel/#prepare","title":"prepare","text":"<p><code>prepare()</code> \u4f1a\u5148\u8fdb\u884c ssl \u63e1\u624b\uff0c\u5b8c\u6210\u540e\u8fdb\u884c\u7528\u6237\u8ba4\u8bc1\uff0c\u5728 ready \u4e4b\u524d\uff0cprepare \u4f1a\u8fd0\u884c\u591a\u6b21\u3002</p> <pre><code>    public void prepare() throws AuthenticationException, IOException {\n        boolean authenticating = false;\n        try {\n            if (!transportLayer.ready()) {\n                transportLayer.handshake();\n            }\n            if (transportLayer.ready() &amp;&amp; !authenticator.complete()) {\n                authenticating = true;\n                authenticator.authenticate();\n            }\n        } catch (AuthenticationException e) {\n            // Clients are notified of authentication exceptions to enable operations to be terminated\n            // without retries. Other errors are handled as network exceptions in Selector.\n            String remoteDesc = remoteAddress != null ? remoteAddress.toString() : null;\n            state = new ChannelState(ChannelState.State.AUTHENTICATION_FAILED, e, remoteDesc);\n            if (authenticating) {\n                delayCloseOnAuthenticationFailure();\n                throw new DelayedResponseAuthenticationException(e);\n            }\n            throw e;\n        }\n        if (ready()) {\n            ++successfulAuthentications;\n            state = ChannelState.READY;\n        }\n    }\n</code></pre>"},{"location":"4-kafka-network-io/1-KafkaChannel/#ready","title":"ready","text":"<p>\u8868\u793a\u63e1\u624b\u4e0e\u7528\u6237\u8ba4\u8bc1\u90fd\u5df2\u5b8c\u6210</p> <pre><code>    public boolean ready() {\n        return transportLayer.ready() &amp;&amp; authenticator.complete();\n    }\n</code></pre>"},{"location":"4-kafka-network-io/1-KafkaChannel/#_2","title":"\u5b9e\u9645\u8bfb\u5199","text":"<p>channel \u7684\u5b9e\u9645\u8bfb\u5199\u5728 <code>Selector</code> \u7684 <code>pollSelectionKeys()</code> \u65b9\u6cd5\u4e2d\u8fdb\u884c\u3002</p> <pre><code>    void pollSelectionKeys(Set&lt;SelectionKey&gt; selectionKeys,\n                           boolean isImmediatelyConnected,\n                           long currentTimeNanos) {\n        for (SelectionKey key : determineHandlingOrder(selectionKeys)) {\n            KafkaChannel channel = channel(key);\n\n            try {\n                /* if channel is not ready finish prepare */\n                if (channel.isConnected() &amp;&amp; !channel.ready()) {  // channel \u672a\u51c6\u5907\u597d\n                    channel.prepare();  // \u63e1\u624b\u548c\u8eab\u4efd\u8ba4\u8bc1\uff0c\u5982\u679c\u6267\u884c\u540e\u8fd8\u6ca1\u6709 ready\uff0c\u7b49\u5f85\u4e0b\u4e00\u6b21 poll \u7ee7\u7eed\u6267\u884c\n                    if (channel.ready()) {\n                    }\n                }\n\n                if (channel.ready() &amp;&amp; channel.state() == ChannelState.NOT_CONNECTED)\n                    channel.state(ChannelState.READY);\n\n                if (channel.ready() &amp;&amp; (key.isReadable() || channel.hasBytesBuffered()) &amp;&amp; !hasCompletedReceive(channel)\n                        &amp;&amp; !explicitlyMutedChannels.contains(channel)) {\n                    attemptRead(channel);\n                }\n\n                try {\n                    attemptWrite(key, channel, nowNanos);\n                } catch (Exception e) {\n                }\n\n                /* cancel any defunct sockets */\n                if (!key.isValid())\n                    close(channel, CloseMode.GRACEFUL);\n\n            } catch (Exception e) {\n            } finally {\n            }\n        }\n    }\n</code></pre>"},{"location":"5-release%20note/","title":"5 release note","text":"<p>\u611f\u5174\u8da3\u7684 KIP</p>"},{"location":"5-release%20note/#api-kip-455","title":"\u91cd\u5206\u533a API KIP-455","text":"<p>\u589e\u52a0\u4e86\u91cd\u5206\u533a\u533a\u7684 API\uff0c\u76f8\u6bd4\u57fa\u4e8e zookeeper \u7684\u65b9\u5f0f\uff0c\u8fd9\u79cd\u91cd\u5206\u533a\u53ef\u4ee5\u7ec8\u6b62\uff0c\u53ef\u4ee5\u5728\u4e0a\u4e00\u4e2a\u8fd8\u5728\u6267\u884c\u65f6\u589e\u52a0\u4efb\u52a1</p> <p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment</p>"},{"location":"5-release%20note/#kafka-consumer-replica-kip-392","title":"kafka consumer \u652f\u6301\u4ece replica \u62c9\u6570\u636e KIP-392","text":"<p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica</p>"},{"location":"5-release%20note/#controller-alterisrrequest-kip-497","title":"controller \u652f\u6301 AlterIsrRequest KIP-497","text":"<p>\u53ef\u4ee5\u4e0d\u518d\u4f9d\u8d56 zookeeper <code>/isr_change_notification</code> \u4f20\u9012 isr \u53d8\u5316\u4fe1\u606f</p> <p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-497%3A+Add+inter-broker+API+to+alter+ISR</p>"},{"location":"5-release%20note/#kip-62","title":"\u6d88\u8d39\u7ec4\u5728\u5355\u72ec\u7684\u7ebf\u7a0b\u53d1\u9001\u5fc3\u8df3 KIP-62","text":"<p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread</p>"},{"location":"5-release%20note/#kip-578","title":"\u5206\u533a\u6570\u9650\u5236 KIP-578","text":"<p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-578%3A+Add+configuration+to+limit+number+of+partitions</p> <p>\u5f53\u524d\u63a8\u8350\u503c\u662f\u6bcf\u4e2a broker \u4e0d\u8d85\u8fc7 4000 \u4e2a\u5206\u533a\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u4e0d\u8d85\u8fc7 200000 \u4e2a\u5206\u533a\uff0c\u4f46\u662f\u6ca1\u6709\u5f3a\u5236\u9650\u5236\u3002</p> <p>\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e <code>create.topic.policy.class.name</code> \u4e3a\u7279\u5b9a\u7684 topic \u521b\u5efa\u7b56\u7565\u5b9e\u73b0\uff0c\u5728\u5206\u533a\u6570\u8fc7\u591a\u65f6\u62d2\u7edd topic \u521b\u5efa\u8bf7\u6c42\u3002</p> <p>\u540e\u7eed\u8ba1\u5212\u589e\u52a0\u53c2\u6570\uff1a</p> <ul> <li><code>max.broker.partitions</code>\uff1a\u5355\u4e2a broker \u6700\u591a partition</li> <li><code>max.partitions</code>: \u96c6\u7fa4\u6700\u591a partition</li> </ul>"},{"location":"5-release%20note/#zookeeper-kip-500","title":"\u53bb\u9664\u5bf9 ZooKeeper \u96c6\u7fa4\u7684\u4f9d\u8d56 KIP-500","text":"<p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum</p>"},{"location":"5-release%20note/#log_dirs-kip-113","title":"\u652f\u6301\u5728 log_dirs \u4e4b\u95f4\u79fb\u52a8\u526f\u672c\u76ee\u5f55 KIP-113","text":"<p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-113%3A+Support+replicas+movement+between+log+directories</p> <ul> <li>\u589e\u52a0 <code>AlterReplicaDirRequest</code> \u8bf7\u6c42</li> <li>\u589e\u52a0 <code>DescribeLogDirsRequest</code> \u8bf7\u6c42</li> </ul>"},{"location":"5-release%20note/#kip-112","title":"\u591a\u78c1\u76d8\u4e0b\u5355\u4e2a\u78c1\u76d8\u4e0d\u53ef\u7528\u4ecd\u7136\u53ef\u63d0\u4f9b\u670d\u52a1 KIP-112","text":"<p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-112%3A+Handle+disk+failure+for+JBOD</p> <ul> <li>\u589e\u52a0 <code>/log_dir_event_notification</code> znode \u8282\u70b9\uff0c\u5411 controller \u4f20\u9012 <code>LogDirFailure</code> \u4e8b\u4ef6</li> </ul>"},{"location":"5-release%20note/#kip-36","title":"\u652f\u6301\u673a\u67b6\u526f\u672c\u5206\u5e03 KIP-36","text":"<p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-36+Rack+aware+replica+assignment</p>"},{"location":"5-release%20note/2.7/","title":"2.7","text":"<p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-554%3A+Add+Broker-side+SCRAM+Config+API</p>"},{"location":"5-release%20note/2.8.0/","title":"2.8.0","text":"<p>KIP(Kafka Improvement Proposals)</p> <ul> <li>KIP-500: which allows you to run Kafka brokers without Apache ZooKeeper, instead depending on an internal Raft implementation.</li> <li>KIP-700: decouples the AdminClient from the Metadata API by adding a new API to directly query the brokers for information about the cluster.</li> <li>KIP-684: support TLS client authentication for SASL_SSL listeners </li> </ul>"},{"location":"5-release%20note/admin/","title":"Kafka Admin Client","text":""},{"location":"5-release%20note/admin/#01000-01022","title":"0.10.0.0 ~ 0.10.2.2","text":"<p>\u53ea\u6709 <code>AdminClient.scala</code>\uff0c\u63d0\u4f9b\u4e86\u901a\u7528\u7684\u53d1\u9001\u8bf7\u6c42\u65b9\u6cd5\uff1a</p> <ul> <li>send</li> <li>sendAnyNode</li> </ul> <p>\u548c\u4e00\u4e9b broker, consumer \u76f8\u5173\u7684\u65b9\u6cd5\uff1a</p> <ul> <li>findAllBrokers</li> <li>findCoordinator</li> <li>listGroups</li> <li>listAllGroups</li> <li>listAllConsumerGroups</li> <li>listAllGroupsFlattened</li> <li>listAllConsumerGroupsFlattened</li> <li>describeGroup</li> <li> <p>describeConsumerGroup</p> </li> <li> <p>listGroupOffsets</p> </li> <li>listAllBrokerVersionInfo</li> </ul>"},{"location":"5-release%20note/admin/#01100-01102","title":"0.11.0.0 ~ 0.11.0.2","text":"<p>\u589e\u52a0\u62bd\u8c61\u7c7b <code>AdminClient.java</code> \u548c\u5176\u5b9e\u73b0\u7c7b <code>KafkaAdminClient.java</code>\uff0c<code>AdminClient.scala</code> \u88ab\u6807\u8bb0\u4e3a deprected\u3002</p> <p>\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u65b9\u6cd5\uff1a</p> <ul> <li>createTopics     0.10.1.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528\uff0cvalidateOnly \u9009\u9879\u9700\u8981 0.10.2.0 \u53ca\u4ee5\u4e0a\u7248\u672c</li> <li>deleteTopics     0.10.1.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</li> <li>listTopics</li> <li> <p>describeTopics</p> </li> <li> <p>describeCluster</p> </li> <li> <p>describeAcls     0.11.0.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</p> </li> <li>createAcls     0.11.0.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</li> <li> <p>deleteAcls     0.11.0.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</p> </li> <li> <p>describeConfigs     0.11.0.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</p> </li> <li>alterConfigs     0.11.0.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528     2.3.0 deprecated</li> </ul>"},{"location":"5-release%20note/admin/#100-102","title":"1.0.0 ~ 1.0.2","text":"<ul> <li>alterReplicaLogDirs     1.0.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528     2.2.0 \u8981\u6c42 1.1.0 \u6216\u66f4\u9ad8\u7248\u672c broker \u53ef\u7528</li> <li>describeLogDirs     1.0.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</li> <li>describeReplicaLogDirs     1.0.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</li> <li>createPartitions     1.0.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</li> </ul>"},{"location":"5-release%20note/admin/#110-111","title":"1.1.0 ~ 1.1.1","text":"<ul> <li>deleteRecords     0.11.0.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</li> </ul>"},{"location":"5-release%20note/admin/#200-211","title":"2.0.0 ~ 2.1.1","text":"<ul> <li>createDelegationToken     1.1.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</li> <li>renewDelegationToken     1.1.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</li> <li>expireDelegationToken     1.1.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</li> <li>describeDelegationToken     1.1.0 \u6216\u8005\u66f4\u9ad8\u7248\u672c\u7684 broker \u53ef\u7528</li> <li>describeConsumerGroups</li> <li>listConsumerGroups</li> <li>listConsumerGroupOffsets</li> <li>deleteConsumerGroups</li> </ul>"},{"location":"5-release%20note/admin/#220-222","title":"2.2.0 ~ 2.2.2","text":"<ul> <li>electPreferredLeaders     2.2.0 \u6216\u66f4\u9ad8\u7248\u672cbroker     deprecated\u00b7Since\u00b72.4.0     3.0.0 \u5220\u9664</li> </ul>"},{"location":"5-release%20note/admin/#230-231","title":"2.3.0 ~ 2.3.1","text":"<ul> <li>incrementalAlterConfigs     \u8981\u6c42 2.3.0 \u6216\u66f4\u9ad8\u7248\u672c broker</li> </ul>"},{"location":"5-release%20note/admin/#240-241","title":"2.4.0 ~ 2.4.1","text":"<p>\u589e\u52a0\u4e86\u63a5\u53e3 <code>Admin.java</code>\uff0c\u62bd\u8c61\u7c7b <code>AdminClient.java</code> \u88ab\u6807\u8bb0\u4e3a deprecated</p> <ul> <li>deleteConsumerGroupOffsets</li> <li>electLeaders     \u6700\u4f18 leader \u9009\u4e3e\u9700\u8981 2.2.0 \u6216\u66f4\u9ad8 broker\uff0c\u5176\u4ed6\u7c7b\u578b\u9700\u8981 2.4.0 \u6216\u66f4\u9ad8 broker</li> <li>alterPartitionReassignments</li> <li>listPartitionReassignments</li> <li>removeMembersFromConsumerGroup</li> </ul>"},{"location":"5-release%20note/admin/#250-251","title":"2.5.0 ~ 2.5.1","text":"<ul> <li>alterConsumerGroupOffsets</li> <li>listOffsets</li> </ul>"},{"location":"5-release%20note/admin/#260-263","title":"2.6.0 ~ 2.6.3","text":"<ul> <li>describeClientQuotas     \u8981\u6c42 2.6.0 \u6216\u66f4\u9ad8\u7248\u672c broker</li> <li>alterClientQuotas     \u8981\u6c42 2.6.0 \u6216\u66f4\u9ad8\u7248\u672c broker</li> </ul>"},{"location":"5-release%20note/admin/#270-272","title":"2.7.0 ~ 2.7.2","text":"<ul> <li>describeUserScramCredentials     \u8981\u6c42 2.7.0 \u6216\u66f4\u9ad8\u7248\u672c broker</li> <li>alterUserScramCredentials     \u8981\u6c42 2.7.0 \u6216\u66f4\u9ad8\u7248\u672c broker</li> <li>describeFeatures</li> <li>updateFeatures     \u8981\u6c42 2.7.0 \u6216\u66f4\u9ad8\u7248\u672c broker</li> </ul>"},{"location":"5-release%20note/admin/#280-282","title":"2.8.0 ~ 2.8.2","text":"<ul> <li>unregisterBroker     \u9002\u7528\u4e8e kraft \u96c6\u7fa4</li> </ul>"},{"location":"5-release%20note/admin/#300-302","title":"3.0.0 ~ 3.0.2","text":"<ul> <li>deleteTopics     \u5f53\u4f7f\u7528 topic id \u65f6\u8981\u6c42 2.8 \u6216\u8005\u66f4\u9ad8     \u5f53\u4f7f\u7528 topic name \u65f6\u8981\u6c42 0.10.1.0 \u6216\u8005\u66f4\u9ad8</li> <li>describeProducers</li> <li>describeTransactions</li> <li>abortTransaction</li> <li>listTransactions</li> </ul>"},{"location":"5-release%20note/admin/#310-312","title":"3.1.0 ~ 3.1.2","text":"<ul> <li>describeTopics     \u5f53\u4f7f\u7528 topic id \u65f6\u8981\u6c42 3.1.0 \u6216\u8005\u66f4\u9ad8</li> </ul>"},{"location":"5-release%20note/admin/#320-323","title":"3.2.0 ~ 3.2.3","text":"<ul> <li>fenceProducers</li> </ul>"},{"location":"5-release%20note/admin/#330","title":"3.3.0 ~","text":"<ul> <li>listConsumerGroupOffsets     \u53ef\u4ee5\u6307\u5b9a\u5177\u4f53\u7684 topic partition</li> <li>describeMetadataQuorum</li> </ul>"},{"location":"5-release%20note/api/","title":"Kafka Api keys","text":""},{"location":"5-release%20note/api/#080-0822","title":"0.8.0 ~ 0.8.2.2","text":"<ul> <li>PRODUCE</li> <li>FETCH</li> <li>LIST_OFFSETS</li> <li>METADATA</li> <li>LEADER_AND_ISR</li> <li>STOP_REPLICA</li> <li>UPDATE_METADATA_KEY</li> <li>CONTROLLED_SHUTDOWN_KEY</li> <li>OFFSET_COMMIT</li> <li>OFFSET_FETCH</li> <li>COSUMER_METADATA</li> <li>JOIN_GROUP</li> <li>HEARTBEAT</li> </ul>"},{"location":"5-release%20note/api/#0900-0901","title":"0.9.0.0 ~ 0.9.0.1","text":"<ul> <li> <p>COSUMER_METADATA -&gt; GROUP_COORDINATOR</p> </li> <li> <p>LEAVE_GROUP</p> </li> <li>SYNC_GROUP</li> <li>DESCRIBE_GROUPS</li> <li>LIST_GROUPS</li> </ul>"},{"location":"5-release%20note/api/#01000-01022","title":"0.10.0.0 ~ 0.10.2.2","text":"<ul> <li>SASL_HANDSHAKE</li> <li> <p>API_VERSIONS</p> </li> <li> <p>CREATE_TOPICS</p> </li> <li>DELETE_TOPICS</li> </ul>"},{"location":"5-release%20note/api/#01100-01102","title":"0.11.0.0 ~ 0.11.0.2","text":"<ul> <li> <p>GROUP_COORDINATOR -&gt; FIND_COORDINATOR</p> </li> <li> <p>DELETE_RECORDS</p> </li> <li>INIT_PRODUCER_ID</li> <li>OFFSET_FOR_LEADER_EPOCH</li> <li>ADD_PARTITIONS_TO_TXN</li> <li>ADD_OFFSETS_TO_TXN</li> <li>END_TXN</li> <li>WRITE_TXN_WARKERS</li> <li>TXN_OFFSET_COMMIT</li> <li>DESCRIBE_ACLS</li> <li>CREATE_ACLS</li> <li>DELETE_ACLS</li> <li>DESCRIBE_CONFIGS</li> <li>ALTER_CONFIGS</li> </ul>"},{"location":"5-release%20note/api/#100-102","title":"1.0.0 ~ 1.0.2","text":"<ul> <li>ALTER_REPLICA_LOG_DIRS</li> <li>DESCRIBE_LOG_DIRS</li> <li>SASL_AUTHENTICATE</li> <li>CREATE_PARTITIONS</li> </ul>"},{"location":"5-release%20note/api/#110-111","title":"1.1.0 ~ 1.1.1","text":""},{"location":"5-release%20note/api/#200-211","title":"2.0.0 ~ 2.1.1","text":"<ul> <li>CREATE_DELEGATION_TOKEN</li> <li>RENEW_DELEGATION_TOKEN</li> <li>EXPIRE_DELEGATION_TOKEN</li> <li>DESCRIBE_DELEGATION_TOKEN</li> <li>DELETE_GROUPS</li> </ul>"},{"location":"5-release%20note/api/#220-222","title":"2.2.0 ~ 2.2.2","text":"<ul> <li>ELECT_PREFERRED_LEADERS</li> </ul>"},{"location":"5-release%20note/api/#230-231","title":"2.3.0 ~ 2.3.1","text":"<ul> <li>INCREMENTAL_ALTER_CONFIGS</li> </ul>"},{"location":"5-release%20note/api/#240-241","title":"2.4.0 ~ 2.4.1","text":""},{"location":"5-release%20note/api/#250-251","title":"2.5.0 ~ 2.5.1","text":"<ul> <li> <p>ELECT_PREFERRED_LEADERS -&gt; ELECT_LEADERS</p> </li> <li> <p>ALTER_PARTITION_REASSIGNMENTS</p> </li> <li>LIST_PARTITION_REASSIGNMENTS</li> <li>OFFSET_DELETE</li> </ul>"},{"location":"5-release%20note/api/#260-263","title":"2.6.0 ~ 2.6.3","text":"<ul> <li>DESCRIBE_CLIENT_QUOTAS</li> <li>ALTER_CLIENT_QUOTAS</li> </ul>"},{"location":"5-release%20note/api/#270-272","title":"2.7.0 ~ 2.7.2","text":"<ul> <li>DESCRIBE_USER_SCRAM_CREDENTIALS</li> <li>ALTER_USER_SCRAM_CREDENTIALS</li> <li>VOTE</li> <li>BEGIN_QUORUM_EPOCH</li> <li>END_QUORUM_EPOCH</li> <li>DESCRIBE_QUORUM</li> <li>ALTER_ISR</li> <li>UPDATE_FEATURES</li> </ul>"},{"location":"5-release%20note/api/#280-282","title":"2.8.0 ~ 2.8.2","text":"<ul> <li>ENVELOPE</li> <li>FETCH_SNAPSHOT</li> <li>DESCRIBE_CLUSTER</li> <li>DESCRIBE_PRODUCERS</li> <li>BROKER_REGISTRATION</li> <li>BROKER_HEARTBEAT</li> <li>UNREGISTER_BROKER</li> </ul>"},{"location":"5-release%20note/api/#300-302","title":"3.0.0 ~ 3.0.2","text":""},{"location":"5-release%20note/api/#310-312","title":"3.1.0 ~ 3.1.2","text":"<ul> <li>DESCRIBE_TRANSACTIONS</li> <li>LIST_TRANSACTIONS</li> <li>ALLOCATE_PRODUCER_IDS</li> </ul>"},{"location":"5-release%20note/api/#320-323","title":"3.2.0 ~ 3.2.3","text":""},{"location":"5-release%20note/api/#330","title":"3.3.0 ~","text":"<ul> <li>ALTER_ISR -&gt; ALTER_PARTITION</li> </ul>"},{"location":"5-release%20note/api/#350","title":"3.5.0","text":"<ul> <li>CONSUMER_GROUP_HEARTBEAT</li> </ul>"},{"location":"6-src/","title":"6 \u6e90\u7801","text":""},{"location":"6-src/clients/clients/Metadata/","title":"Metadata","text":"<pre><code>implements Closeable\n</code></pre> <p>\u88ab the client thread (for partitioning) \u548c the background sender thread \u5171\u4eab</p> <p>Metadata is maintained for only a subset of topics, which can be added to over time. \u5f53\u6211\u4eec\u8bf7\u6c42 metadata \u4e2d\u6ca1\u6709\u7684 topic \u65f6\u4f1a\u89e6\u53d1 matadata \u66f4\u65b0\u3002</p> <p>\u5982\u679c topic expiry \u88ab\u8bbe\u7f6e\uff0c\u4efb\u4f55\u5728 expiry interval \u65f6\u95f4\u5185\u6ca1\u6709\u4f7f\u7528\u7684 topic \u4f1a\u88ab\u4ece metadata \u4e2d\u79fb\u9664\u3002 \u6d88\u8d39\u7ec4\u7981\u6b62 topic expiry \u56e0\u4e3a\u5b83 explicityly manage topics\uff0cproducers \u4f9d\u8d56 topic expiry to limit the refresh set.</p>"},{"location":"6-src/clients/clients/Metadata/#_1","title":"\u5c5e\u6027","text":"<pre><code>// metadata \u66f4\u65b0\u5931\u8d25\u65f6,\u4e3a\u907f\u514d\u9891\u7e41\u66f4\u65b0 meta,\u6700\u5c0f\u7684\u95f4\u9694\u65f6\u95f4,\u9ed8\u8ba4 100ms\nprivate final long refreshBackoffMs;\n// metadata \u7684\u8fc7\u671f\u65f6\u95f4, \u9ed8\u8ba4 60,000ms\nprivate final long metadataExpireMs;\n\nprivate int updateVersion;  // bumped on every metadata response\nprivate int requestVersion; // bumped on every new topic addition\n\n\n// \u6700\u540e\u4e00\u6b21\u66f4\u65b0\u7684\u65f6\u95f4\uff08\u5305\u542b\u66f4\u65b0\u5931\u8d25\u7684\u60c5\u51b5\uff09\nprivate long lastRefreshMs;\n// \u6700\u540e\u4e00\u6b21\u6210\u529f\u66f4\u65b0\u7684\u65f6\u95f4\nprivate long lastSuccessfulRefreshMs;\n\nprivate KafkaException fatalException;\n\nprivate Set&lt;String&gt; invalidTopics;\nprivate Set&lt;String&gt; unauthorizedTopics;\n\nprivate boolean needFullUpdate;\nprivate boolean needPartialUpdate;\n\nprivate final ClusterResourceListeners clusterResourceListeners;\nprivate boolean isClosed;\nprivate final Map&lt;TopicPartition, Integer&gt; lastSeenLeaderEpochs;\n\n\nprivate MetadataCache cache = MetadataCache.empty();\n</code></pre>"},{"location":"6-src/clients/clients/Metadata/#metadatacache","title":"MetadataCache","text":"<p><code>MetadataCache</code></p> <pre><code>private final String clusterId;\nprivate final Map&lt;Integer, Node&gt; nodes;\nprivate final Set&lt;String&gt; unauthorizedTopics;\nprivate final Set&lt;String&gt; invalidTopics;\nprivate final Set&lt;String&gt; internalTopics;\nprivate final Node controller;\nprivate final Map&lt;TopicPartition, PartitionMetadata&gt; metadataByPartition;\n\nprivate Cluster clusterInstance;\n</code></pre>"},{"location":"6-src/clients/clients/Metadata/#_2","title":"\u65b9\u6cd5","text":""},{"location":"6-src/clients/clients/Metadata/#add","title":"add","text":""},{"location":"6-src/clients/clients/NetworkClient/","title":"NetworkClient","text":"<pre><code>implements KafkaClient\n</code></pre>"},{"location":"6-src/clients/clients/NetworkClient/#_1","title":"\u65b9\u6cd5","text":""},{"location":"6-src/clients/clients/NetworkClient/#poll","title":"poll","text":"<pre><code>/**\n * Do actual reads and writes to sockets.\n *\n * @param timeout The maximum amount of time to wait (in ms) for responses if there are none immediately,\n *                must be non-negative. The actual timeout will be the minimum of timeout, request timeout and\n *                metadata timeout\n * @param now The current time in milliseconds\n * @return The list of responses received\n */\n@Override\npublic List&lt;ClientResponse&gt; poll(long timeout, long now) {\n    ensureActive();\n\n    if (!abortedSends.isEmpty()) {\n        // If there are aborted sends because of unsupported version exceptions or disconnects,\n        // handle them immediately without waiting for Selector#poll.\n        List&lt;ClientResponse&gt; responses = new ArrayList&lt;&gt;();\n        handleAbortedSends(responses);\n        completeResponses(responses);\n        return responses;\n    }\n\n    long metadataTimeout = metadataUpdater.maybeUpdate(now);\n    try {\n        this.selector.poll(Utils.min(timeout, metadataTimeout, defaultRequestTimeoutMs));\n    } catch (IOException e) {\n        log.error(\"Unexpected error during I/O\", e);\n    }\n\n    // process completed actions\n    long updatedNow = this.time.milliseconds();\n    List&lt;ClientResponse&gt; responses = new ArrayList&lt;&gt;();\n    handleCompletedSends(responses, updatedNow);\n    handleCompletedReceives(responses, updatedNow);\n    handleDisconnections(responses, updatedNow);\n    handleConnections();\n    handleInitiateApiVersionRequests(updatedNow);\n    handleTimedOutConnections(responses, updatedNow);\n    handleTimedOutRequests(responses, updatedNow);\n    completeResponses(responses);\n\n    return responses;\n}\n</code></pre> <ol> <li>\u5982\u679c\u9700\u8981\u66f4\u65b0 Metadata\uff0c\u90a3\u4e48\u5c31\u53d1\u9001 Metadata \u8bf7\u6c42</li> <li>\u8c03\u7528 <code>Selector</code> \u8fdb\u884c\u76f8\u5e94\u7684 IO \u64cd\u4f5c</li> <li>\u5904\u7406 server \u7aef\u7684 response \u53ca\u4e00\u4e9b\u5176\u4ed6\u64cd\u4f5c</li> </ol>"},{"location":"6-src/clients/clients/NetworkClient/#send","title":"send","text":"<pre><code>/**\n * Queue up the given request for sending. Requests can only be sent out to ready nodes.\n * @param request The request\n * @param now The current timestamp\n */\n@Override\npublic void send(ClientRequest request, long now) {\n    doSend(request, false, now);\n}\n</code></pre>"},{"location":"6-src/clients/clients/NetworkClient/#dosend","title":"doSend","text":""},{"location":"6-src/clients/clients/NetworkClient/#_2","title":"\u5185\u90e8\u7c7b","text":""},{"location":"6-src/clients/clients/NetworkClient/#defaultmetadataupdater","title":"DefaultMetadataUpdater","text":""},{"location":"6-src/clients/clients/NetworkClient/#maybeupdate","title":"maybeUpdate","text":"<p>\u5982\u679c Metadata \u9700\u8981\u66f4\u65b0\uff0c\u9009\u62e9\u8fde\u63a5\u6570\u6700\u5c0f\u7684 node \u53d1\u9001 Metadata \u8bf7\u6c42</p>"},{"location":"6-src/clients/clients/producer/KafkaProducer/","title":"Producer","text":"<ol> <li>\u751f\u4ea7\u8005\u7ef4\u6301\u4e00\u4e2a pool of buffer space \u5b58\u50a8\u8fd8\u6ca1\u6709\u53d1\u5f80 server \u7684 record</li> <li>\u540c\u65f6\u6709\u80cc\u540e\u7684 I/O thread \u8d1f\u8d23\u5c06 record \u53d1\u5f80 server</li> <li><code>send</code> \u65b9\u6cd5\u662f\u5f02\u6b65\u7684\uff0c\u5f53\u8c03\u7528\u65f6\u628a record \u52a0\u5165\u4e00\u4e2a buffer of pending record sends \u5e76\u7acb\u5373\u8fd4\u56de\u3002\u8fd9\u5141\u8bb8 producer \u901a\u8fc7\u6253\u5305\u5355\u72ec\u7684 record \u63d0\u4f9b\u6548\u7387</li> <li><code>batch.size</code>: producer \u4e3a\u6bcf\u4e2a partition \u7ef4\u62a4\u4e00\u4e2a\u672a\u53d1\u9001 record \u7684 buffer\uff0cbuffer \u7684\u5927\u5c0f\u7531 <code>batch.size</code> \u914d\u7f6e\u3002\u589e\u52a0\u914d\u7f6e\u53ef\u4ee5\u8ba9 record \u6253\u5305\u589e\u5927\uff0c\u63d0\u9ad8\u53d1\u9001\u6548\u7387\uff0c\u4f46\u4f1a\u589e\u52a0\u5185\u5b58\u4f7f\u7528\uff0c\u964d\u4f4e\u53d1\u9001\u65f6\u6548</li> <li><code>linger.ms</code>: \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0ca buffer is available to send immediately even if there is additional unused space in the buffseer</li> <li><code>buffer.memory</code>: \u63a7\u5236 producer \u53ef\u4ee5\u4f7f\u7528\u7684 buffer \u7684\u5927\u5c0f\uff0c\u5982\u679c record \u53d1\u9001\u5feb\u4e8e producer \u4e0e server \u7684\u4ea4\u4e92\uff0cbuffer \u53ef\u7528\u7a7a\u95f4\u4f1a\u88ab\u8017\u5c3d\uff0c<code>send</code> \u8c03\u7528\u4f1a\u88ab\u963b\u585e\u3002<code>max.block.ms</code> \u914d\u7f6e\u963b\u585e\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u5982\u679c\u8d85\u65f6\uff0c\u4f1a\u629b\u51fa <code>TimeoutException</code> \u5f02\u5e38</li> <li>0.11 \u4e4b\u540e\uff0cKafka \u652f\u6301\u5e42\u7b49(idempotent) \u4e0e\u4e8b\u52a1(transacional)<ol> <li>\u4e3a\u4e86\u652f\u6301\u5e42\u7b49\uff0c<code>enable.idempotence</code> \u5fc5\u987b\u88ab\u8bbe\u7f6e\u4e3a <code>true</code>\uff0c\u8bbe\u7f6e\u4e4b\u540e <code>retries</code> \u4f1a\u88ab\u9ed8\u8ba4\u8bbe\u4e3a <code>Integer.MAX_VALUE</code> \u5e76\u4e14 <code>acks</code> \u4f1a\u9ed8\u8ba4\u8bbe\u4e3a <code>all</code></li> <li>\u4e3a\u4e86\u652f\u6301\u4e8b\u52a1\uff0c<code>transactional.id</code> \u5fc5\u987b\u88ab\u8bbe\u7f6e\uff0c\u8bbe\u7f6e\u4e4b\u540e\uff0c<code>enable.idempotence</code> \u4f1a\u81ea\u52a8\u5f00\u542f\u3002\u6b64\u5916:<ol> <li>topic which are included in transactions \u5e94\u8be5 configured for durability\uff0c\u5177\u4f53\u7684: <code>replication.factor</code> \u81f3\u5c11\u4e3a 3\uff0c<code>min.insync.replicas</code> \u5e94\u8be5\u8bbe\u7f6e\u4e3a 2\u3002</li> <li>\u4e3a\u4e86\u7aef\u5230\u7aef\u7684\u4e8b\u52a1\u6027\u4fdd\u8bc1\uff0cconsumer \u5e94\u8be5\u88ab\u8bbe\u7f6e\u4e3a\u53ea\u8bfb committed messages</li> </ol> </li> </ol> </li> </ol>"},{"location":"6-src/clients/clients/producer/KafkaProducer/#_1","title":"\u5c5e\u6027","text":"<pre><code>private final Logger log;\nprivate static final String JMX_PREFIX = \"kafka.producer\";\npublic static final String NETWORK_THREAD_PREFIX = \"kafka-producer-network-thread\";\npublic static final String PRODUCER_METRIC_GROUP_NAME = \"producer-metrics\";\n\nprivate final String clientId;\n// Visible for testing\nfinal Metrics metrics;\nprivate final Partitioner partitioner;\nprivate final int maxRequestSize;\nprivate final long totalMemorySize;\nprivate final ProducerMetadata metadata;\nprivate final RecordAccumulator accumulator;\nprivate final Sender sender;\nprivate final Thread ioThread;\nprivate final CompressionType compressionType;\nprivate final Sensor errors;\nprivate final Time time;\nprivate final Serializer&lt;K&gt; keySerializer;\nprivate final Serializer&lt;V&gt; valueSerializer;\nprivate final ProducerConfig producerConfig;\nprivate final long maxBlockTimeMs;\nprivate final ProducerInterceptors&lt;K, V&gt; interceptors;\nprivate final ApiVersions apiVersions;\nprivate final TransactionManager transactionManager;\n</code></pre>"},{"location":"6-src/clients/clients/producer/KafkaProducer/#_2","title":"\u6784\u9020\u65b9\u6cd5","text":"<p><code>KafkaProducer</code> \u6784\u9020\u65b9\u6cd5\u4e2d\uff0c\u521d\u59cb\u5982\u4e0b\u5173\u952e\u5c5e\u6027</p> <ul> <li><code>this.partitioner</code> \u5206\u533a\u7b97\u6cd5</li> <li><code>this.accumulator = new RecordAccumulator</code></li> <li><code>this.metadata = new ProducerMetadata</code> <pre><code>this.metadata = new ProducerMetadata(retryBackoffMs,\n        config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),\n        config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),\n        logContext,\n        clusterResourceListeners,\n        Time.SYSTEM);\nthis.metadata.bootstrap(addresses);\n</code></pre></li> <li><code>this.sender = newSender(logContext, kafkaClient, this.metadata)</code></li> </ul> <p>\u540c\u65f6\u5f00\u542f\u7ebf\u7a0b\uff0c\u8fd0\u884c <code>sender</code></p> <pre><code>this.ioThread = new KafkaThread(ioThreadName, this.sender, true);\nthis.ioThread.start();\n</code></pre>"},{"location":"6-src/clients/clients/producer/KafkaProducer/#send","title":"send","text":"<pre><code>/**\n * Asynchronously send a record to a topic and invoke the provided callback when the send has been acknowledged.\n * &lt;p&gt;\n * The send is asynchronous and this method will return immediately once the record has been stored in the buffer of\n * records waiting to be sent. This allows sending many records in parallel without blocking to wait for the\n * response after each one.\n * &lt;p&gt;\n * The result of the send is a {@link RecordMetadata} specifying the partition the record was sent to, the offset\n * it was assigned and the timestamp of the record. If\n * {@link org.apache.kafka.common.record.TimestampType#CREATE_TIME CreateTime} is used by the topic, the timestamp\n * will be the user provided timestamp or the record send time if the user did not specify a timestamp for the\n * record. If {@link org.apache.kafka.common.record.TimestampType#LOG_APPEND_TIME LogAppendTime} is used for the\n * topic, the timestamp will be the Kafka broker local time when the message is appended.\n * &lt;p&gt;\n * Since the send call is asynchronous it returns a {@link java.util.concurrent.Future Future} for the\n * {@link RecordMetadata} that will be assigned to this record. Invoking {@link java.util.concurrent.Future#get()\n * get()} on this future will block until the associated request completes and then return the metadata for the record\n * or throw any exception that occurred while sending the record.\n * &lt;p&gt;\n * If you want to simulate a simple blocking call you can call the &lt;code&gt;get()&lt;/code&gt; method immediately:\n *\n * &lt;pre&gt;\n * {@code\n * byte[] key = \"key\".getBytes();\n * byte[] value = \"value\".getBytes();\n * ProducerRecord&lt;byte[],byte[]&gt; record = new ProducerRecord&lt;byte[],byte[]&gt;(\"my-topic\", key, value)\n * producer.send(record).get();\n * }&lt;/pre&gt;\n * &lt;p&gt;\n * Fully non-blocking usage can make use of the {@link Callback} parameter to provide a callback that\n * will be invoked when the request is complete.\n *\n * &lt;pre&gt;\n * {@code\n * ProducerRecord&lt;byte[],byte[]&gt; record = new ProducerRecord&lt;byte[],byte[]&gt;(\"the-topic\", key, value);\n * producer.send(myRecord,\n *               new Callback() {\n *                   public void onCompletion(RecordMetadata metadata, Exception e) {\n *                       if(e != null) {\n *                          e.printStackTrace();\n *                       } else {\n *                          System.out.println(\"The offset of the record we just sent is: \" + metadata.offset());\n *                       }\n *                   }\n *               });\n * }\n * &lt;/pre&gt;\n *\n * Callbacks for records being sent to the same partition are guaranteed to execute in order. That is, in the\n * following example &lt;code&gt;callback1&lt;/code&gt; is guaranteed to execute before &lt;code&gt;callback2&lt;/code&gt;:\n *\n * &lt;pre&gt;\n * {@code\n * producer.send(new ProducerRecord&lt;byte[],byte[]&gt;(topic, partition, key1, value1), callback1);\n * producer.send(new ProducerRecord&lt;byte[],byte[]&gt;(topic, partition, key2, value2), callback2);\n * }\n * &lt;/pre&gt;\n * &lt;p&gt;\n * When used as part of a transaction, it is not necessary to define a callback or check the result of the future\n * in order to detect errors from &lt;code&gt;send&lt;/code&gt;. If any of the send calls failed with an irrecoverable error,\n * the final {@link #commitTransaction()} call will fail and throw the exception from the last failed send. When\n * this happens, your application should call {@link #abortTransaction()} to reset the state and continue to send\n * data.\n * &lt;/p&gt;\n * &lt;p&gt;\n * Some transactional send errors cannot be resolved with a call to {@link #abortTransaction()}.  In particular,\n * if a transactional send finishes with a {@link ProducerFencedException}, a {@link org.apache.kafka.common.errors.OutOfOrderSequenceException},\n * a {@link org.apache.kafka.common.errors.UnsupportedVersionException}, or an\n * {@link org.apache.kafka.common.errors.AuthorizationException}, then the only option left is to call {@link #close()}.\n * Fatal errors cause the producer to enter a defunct state in which future API calls will continue to raise\n * the same underyling error wrapped in a new {@link KafkaException}.\n * &lt;/p&gt;\n * &lt;p&gt;\n * It is a similar picture when idempotence is enabled, but no &lt;code&gt;transactional.id&lt;/code&gt; has been configured.\n * In this case, {@link org.apache.kafka.common.errors.UnsupportedVersionException} and\n * {@link org.apache.kafka.common.errors.AuthorizationException} are considered fatal errors. However,\n * {@link ProducerFencedException} does not need to be handled. Additionally, it is possible to continue\n * sending after receiving an {@link org.apache.kafka.common.errors.OutOfOrderSequenceException}, but doing so\n * can result in out of order delivery of pending messages. To ensure proper ordering, you should close the\n * producer and create a new instance.\n * &lt;/p&gt;\n * &lt;p&gt;\n * If the message format of the destination topic is not upgraded to 0.11.0.0, idempotent and transactional\n * produce requests will fail with an {@link org.apache.kafka.common.errors.UnsupportedForMessageFormatException}\n * error. If this is encountered during a transaction, it is possible to abort and continue. But note that future\n * sends to the same topic will continue receiving the same exception until the topic is upgraded.\n * &lt;/p&gt;\n * &lt;p&gt;\n * Note that callbacks will generally execute in the I/O thread of the producer and so should be reasonably fast or\n * they will delay the sending of messages from other threads. If you want to execute blocking or computationally\n * expensive callbacks it is recommended to use your own {@link java.util.concurrent.Executor} in the callback body\n * to parallelize processing.\n *\n * @param record The record to send\n * @param callback A user-supplied callback to execute when the record has been acknowledged by the server (null\n *        indicates no callback)\n *\n * @throws AuthenticationException if authentication fails. See the exception for more details\n * @throws AuthorizationException fatal error indicating that the producer is not allowed to write\n * @throws IllegalStateException if a transactional.id has been configured and no transaction has been started, or\n *                               when send is invoked after producer has been closed.\n * @throws InterruptException If the thread is interrupted while blocked\n * @throws SerializationException If the key or value are not valid objects given the configured serializers\n * @throws KafkaException If a Kafka related error occurs that does not belong to the public API exceptions.\n */\n@Override\npublic Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) {\n    // intercept the record, which can be potentially modified; this method does not throw exceptions\n    ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record);\n    return doSend(interceptedRecord, callback);\n}\n</code></pre>"},{"location":"6-src/clients/clients/producer/KafkaProducer/#dosend","title":"doSend","text":"<pre><code>/**\n * Implementation of asynchronously send a record to a topic.\n */\nprivate Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) {\n    TopicPartition tp = null;\n    try {\n        throwIfProducerClosed();\n        // first make sure the metadata for the topic is available\n        long nowMs = time.milliseconds();\n        ClusterAndWaitTime clusterAndWaitTime;\n        try {\n            clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);\n        } catch (KafkaException e) {\n            if (metadata.isClosed())\n                throw new KafkaException(\"Producer closed while send in progress\", e);\n            throw e;\n        }\n        nowMs += clusterAndWaitTime.waitedOnMetadataMs;\n        long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);\n        Cluster cluster = clusterAndWaitTime.cluster;\n        // 3. \u5e8f\u5217\u5316 key, value\n        byte[] serializedKey;\n        try {\n            serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());\n        } catch (ClassCastException cce) {\n            throw new SerializationException(\"Can't convert key of class \" + record.key().getClass().getName() +\n                    \" to class \" + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +\n                    \" specified in key.serializer\", cce);\n        }\n        byte[] serializedValue;\n        try {\n            serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());\n        } catch (ClassCastException cce) {\n            throw new SerializationException(\"Can't convert value of class \" + record.value().getClass().getName() +\n                    \" to class \" + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +\n                    \" specified in value.serializer\", cce);\n        }\n        // 4. \u8ba1\u7b97\u6d88\u606f\u6240\u5904\u5206\u533a\n        int partition = partition(record, serializedKey, serializedValue, cluster);\n        tp = new TopicPartition(record.topic(), partition);\n\n        setReadOnly(record.headers());\n        Header[] headers = record.headers().toArray();\n\n        // 5. \u8ba1\u7b97\u6d88\u606f\u5927\u5c0f\uff0c\u786e\u5b9a\u4e0d\u8d85\u8fc7 `max.request.size`, `buffer.memory`\n        int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),\n                compressionType, serializedKey, serializedValue, headers);\n        ensureValidRecordSize(serializedSize);\n        // 6. \u83b7\u53d6\u6d88\u606f timestamp\uff0c\u5982\u679c\u6ca1\u6709\u5c31\u7528\u5f53\u524d\u65f6\u95f4\n        long timestamp = record.timestamp() == null ? nowMs : record.timestamp();\n        if (log.isTraceEnabled()) {\n            log.trace(\"Attempting to append record {} with callback {} to topic {} partition {}\", record, callback, record.topic(), partition);\n        }\n        // producer callback will make sure to call both 'callback' and interceptor callback\n        Callback interceptCallback = new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp);\n\n        if (transactionManager != null &amp;&amp; transactionManager.isTransactional()) {\n            transactionManager.failIfNotReadyForSend();\n        }\n        // 7. \u5c06\u6d88\u606f\u8ffd\u52a0\u5230 accumulator\n        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,\n                serializedValue, headers, interceptCallback, remainingWaitMs, true, nowMs);\n\n        if (result.abortForNewBatch) {\n            int prevPartition = partition;\n            partitioner.onNewBatch(record.topic(), cluster, prevPartition);\n            partition = partition(record, serializedKey, serializedValue, cluster);\n            tp = new TopicPartition(record.topic(), partition);\n            if (log.isTraceEnabled()) {\n                log.trace(\"Retrying append due to new batch creation for topic {} partition {}. The old partition was {}\", record.topic(), partition, prevPartition);\n            }\n            // producer callback will make sure to call both 'callback' and interceptor callback\n            interceptCallback = new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp);\n\n            result = accumulator.append(tp, timestamp, serializedKey,\n                serializedValue, headers, interceptCallback, remainingWaitMs, false, nowMs);\n        }\n\n        if (transactionManager != null &amp;&amp; transactionManager.isTransactional())\n            transactionManager.maybeAddPartitionToTransaction(tp);\n\n        // 8. \u5982\u679c `batch` \u6ee1\u4e86\u6216\u8005\u65b0\u521b\u5efa\u4e86 `batch`\uff0c\u5524\u9192 sender\n        if (result.batchIsFull || result.newBatchCreated) {\n            log.trace(\"Waking up the sender since topic {} partition {} is either full or getting a new batch\", record.topic(), partition);\n            this.sender.wakeup();\n        }\n        return result.future;\n        // handling exceptions and record the errors;\n        // for API exceptions return them in the future,\n        // for other exceptions throw directly\n    } catch (ApiException e) {\n        log.debug(\"Exception occurred during message send:\", e);\n        if (callback != null)\n            callback.onCompletion(null, e);\n        this.errors.record();\n        this.interceptors.onSendError(record, tp, e);\n        return new FutureFailure(e);\n    } catch (InterruptedException e) {\n        this.errors.record();\n        this.interceptors.onSendError(record, tp, e);\n        throw new InterruptException(e);\n    } catch (KafkaException e) {\n        this.errors.record();\n        this.interceptors.onSendError(record, tp, e);\n        throw e;\n    } catch (Exception e) {\n        // we notify interceptor about all exceptions, since onSend is called before anything else in this method\n        this.interceptors.onSendError(record, tp, e);\n        throw e;\n    }\n}\n</code></pre>"},{"location":"6-src/clients/clients/producer/internals/ProducerBatch/","title":"ProducerBatch","text":""},{"location":"6-src/clients/clients/producer/internals/ProducerMetadata/","title":"ProducerMetadata","text":"<pre><code>extends Metadata\n</code></pre> <p>This class is shared by the client thread(for partitioning) and background sender thread.</p>"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/","title":"RecordAccumulator","text":"<p>This class acts as a queue that accumulates records into <code>MemoryRecords</code> instances to be send to the server.</p>"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#_1","title":"\u5c5e\u6027","text":"<p><code>batches</code></p> <pre><code>private final ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; batches\n</code></pre> <pre><code>{\n    // TopicPartition: Deque&lt;ProducerBatch&gt;\n}\n</code></pre>"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#_2","title":"\u65b9\u6cd5","text":""},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#append","title":"append","text":"<p>\u5c06\u6d88\u606f\u52a0\u5165 <code>batches</code>\uff0c<code>batches</code> \u7684\u7ed3\u6784\u5982\u4e0a</p> <pre><code>public RecordAppendResult append(TopicPartition tp,\n                                 long timestamp,\n                                 byte[] key,\n                                 byte[] value,\n                                 Header[] headers,\n                                 Callback callback,\n                                 long maxTimeToBlock,\n                                 boolean abortOnNewBatch,\n                                 long nowMs) throws InterruptedException {\n    // We keep track of the number of appending thread to make sure we do not miss batches in\n    // abortIncompleteBatches().\n    appendsInProgress.incrementAndGet();\n    ByteBuffer buffer = null;\n    if (headers == null) headers = Record.EMPTY_HEADERS;\n    try {\n        // check if we have an in-progress batch\n        Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);\n        synchronized (dq) {\n            if (closed)\n                throw new KafkaException(\"Producer closed while send in progress\");\n            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);  // \u8ffd\u52a0\u6570\u636e\n            if (appendResult != null)  // topic-partition \u5df2\u7ecf\u8ffd\u52a0\u5230\u5b58\u5728\u7684 batch\uff0c\u76f4\u63a5\u8fd4\u56de\n                return appendResult;\n        }\n\n        // \u5426\u5219\u5c1d\u8bd5\u521b\u5efa\u65b0\u7684 batch\n\n\n        // we don't have an in-progress record batch try to allocate a new batch\n        if (abortOnNewBatch) {\n            // Return a result that will cause another call to append.\n            return new RecordAppendResult(null, false, false, true);\n        }\n\n        byte maxUsableMagic = apiVersions.maxUsableProduceMagic();\n        int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));\n        log.trace(\"Allocating a new {} byte message buffer for topic {} partition {} with remaining timeout {}ms\", size, tp.topic(), tp.partition(), maxTimeToBlock);\n        buffer = free.allocate(size, maxTimeToBlock);\n\n        // Update the current time in case the buffer allocation blocked above.\n        nowMs = time.milliseconds();\n        synchronized (dq) {\n            // Need to check if producer is closed again after grabbing the dequeue lock.\n            if (closed)\n                throw new KafkaException(\"Producer closed while send in progress\");\n\n            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);\n            if (appendResult != null) {\n                // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often...\n                return appendResult;\n            }\n\n            MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);\n            ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, nowMs);\n            FutureRecordMetadata future = Objects.requireNonNull(batch.tryAppend(timestamp, key, value, headers,\n                    callback, nowMs));\n\n            dq.addLast(batch);\n            incomplete.add(batch);\n\n            // Don't deallocate this buffer in the finally block as it's being used in the record batch\n            buffer = null;\n            // \u5982\u679c dp.size() &gt; 1 \u6216\u8005 batch.isFull() \u5c31\u8bf4\u660e\u6709 batch \u53ef\u4ee5\u53d1\u9001\n            return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true, false);\n        }\n    } finally {\n        if (buffer != null)\n            free.deallocate(buffer);\n        appendsInProgress.decrementAndGet();\n    }\n}\n</code></pre>"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#ready","title":"ready","text":"<p>\u83b7\u53d6\u5df2\u7ecf\u51c6\u5907\u597d\u53d1\u9001\u7684 batch \u5bf9\u5e94\u7684 leader (node)</p>"},{"location":"6-src/clients/clients/producer/internals/RecordAccumulator/#drain","title":"drain","text":"<p>\u8fd4\u56de</p> <pre><code>{\n    // Map&lt;Integer, List&lt;ProducerBatch&gt;&gt;\n    nodeId: [],\n    nodeId: [],\n    ...\n}\n</code></pre>"},{"location":"6-src/clients/clients/producer/internals/Sender/","title":"Sender","text":"<pre><code>implements Runnable\n</code></pre> <p>The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata requests to renew its view of the cluster and then send produce requests to the appropriate nodes.</p> <ul> <li>KafkaClient</li> <li>RecordAccumulator</li> <li>ProducerMetadata</li> <li><code>Map&lt;TopicPartition, List&lt;ProducerBatch&gt;&gt; inFlightBatches</code></li> </ul>"},{"location":"6-src/clients/clients/producer/internals/Sender/#_1","title":"\u65b9\u6cd5","text":""},{"location":"6-src/clients/clients/producer/internals/Sender/#run","title":"run","text":"<pre><code>while (running) {\n    try {\n        runOnce();\n    } catch (Exception e) {\n        log.error(\"Uncaught error in kafka producer I/O thread: \", e);\n    }\n}\n</code></pre>"},{"location":"6-src/clients/clients/producer/internals/Sender/#runonce","title":"runOnce","text":"<pre><code>/**\n * Run a single iteration of sending\n *\n */\nvoid runOnce() {\n    if (transactionManager != null) {\n        try {\n            transactionManager.maybeResolveSequences();\n\n            // do not continue sending if the transaction manager is in a failed state\n            if (transactionManager.hasFatalError()) {\n                RuntimeException lastError = transactionManager.lastError();\n                if (lastError != null)\n                    maybeAbortBatches(lastError);\n                client.poll(retryBackoffMs, time.milliseconds());\n                return;\n            }\n\n            // Check whether we need a new producerId. If so, we will enqueue an InitProducerId\n            // request which will be sent below\n            transactionManager.bumpIdempotentEpochAndResetIdIfNeeded();\n\n            if (maybeSendAndPollTransactionalRequest()) {\n                return;\n            }\n        } catch (AuthenticationException e) {\n            // This is already logged as error, but propagated here to perform any clean ups.\n            log.trace(\"Authentication exception while processing transactional request\", e);\n            transactionManager.authenticationFailed(e);\n        }\n    }\n\n    long currentTimeMs = time.milliseconds();\n    // \u5c06 record batch \u8f6c\u79fb\u5230\u6bcf\u4e2a\u8282\u70b9\u7684\u751f\u4ea7\u8bf7\u6c42\u5217\u8868\u4e2d\n    long pollTimeout = sendProducerData(currentTimeMs);\n    // `NetworkClient.poll` Do actual reads and writes to sockets\n    client.poll(pollTimeout, currentTimeMs);\n}\n</code></pre>"},{"location":"6-src/clients/clients/producer/internals/Sender/#sendproducerdata","title":"sendProducerData","text":"<pre><code>private long sendProducerData(long now) {\n    // 1. \u83b7\u53d6 metadata\n    Cluster cluster = metadata.fetch();\n    // get the list of partitions with data ready to send\n    // 2. \u4ece accumulator \u7684 batchers \u4e2d\u53d6\u51fa\u53ef\u4ee5\u53d1\u9001\u7684\u6570\u636e\uff0c\n    //     1. \u83b7\u53d6\u5bf9\u5e94\u7684 leader \u8282\u70b9\u5217\u8868\n    //     2. \u627e\u4e0d\u5230 leader \u7684 topic \u5217\u8868\n    RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);\n\n    // if there are any partitions whose leaders are not known yet, force metadata update\n    if (!result.unknownLeaderTopics.isEmpty()) {\n        // The set of topics with unknown leader contains topics with leader election pending as well as\n        // topics which may have expired. Add the topic again to metadata to ensure it is included\n        // and request metadata update, since there are messages to send to the topic.\n        for (String topic : result.unknownLeaderTopics)\n            this.metadata.add(topic, now);\n\n        log.debug(\"Requesting metadata update due to unknown leader topics from the batched records: {}\",\n            result.unknownLeaderTopics);\n        this.metadata.requestUpdate();\n    }\n\n    // remove any nodes we aren't ready to send to\n    Iterator&lt;Node&gt; iter = result.readyNodes.iterator();\n    long notReadyTimeout = Long.MAX_VALUE;\n    // 4. \u904d\u5386 leader \u8282\u70b9\u5217\u8868\n    while (iter.hasNext()) {\n        Node node = iter.next();\n        // 4.1 \u5224\u65ad leader \u8282\u70b9\u662f\u5426\u5df2\u7ecf\u8fde\u63a5\u5e76\u53ef\u4ee5\u53d1\u9001\u6570\u636e\uff0c\u5982\u679c\u662f\u8fd4\u56de true\uff0c\u4e0d\u662f\u5219\u8fd4\u56de false \u5e76\u5f00\u59cb\u8fdb\u884c\u8fde\u63a5\n        if (!this.client.ready(node, now)) {\n            iter.remove();\n            notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));\n        }\n    }\n\n    // create produce requests\n    // 5. \u83b7\u53d6\u6bcf\u4e2a leader \u5bf9\u5e94\u7684\u53ef\u4ee5\u53d1\u9001\u7684 `ProducerBatch` \u5217\u8868\n    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);\n    // 6. \u5728 Sender \u7684 inFlightBatches \u91cc\u8bb0\u5f55 topicPartition \u5bf9\u5e94\u7684 batch \u5217\u8868\n    addToInflightBatches(batches);\n    // 7. \u5982\u679c\u9700\u8981\u4fdd\u8bc1\u6d88\u606f\u7684\u987a\u5e8f\uff0c\u5c06 topicPartition \u52a0\u5165\u5230 accumulator \u7684 muted \u8bb0\u5f55\n    if (guaranteeMessageOrder) {\n        // Mute all the partitions drained\n        for (List&lt;ProducerBatch&gt; batchList : batches.values()) {\n            for (ProducerBatch batch : batchList)\n                this.accumulator.mutePartition(batch.topicPartition);\n        }\n    }\n\n    accumulator.resetNextBatchExpiryTime();\n    List&lt;ProducerBatch&gt; expiredInflightBatches = getExpiredInflightBatches(now);\n    List&lt;ProducerBatch&gt; expiredBatches = this.accumulator.expiredBatches(now);\n    expiredBatches.addAll(expiredInflightBatches);\n\n    // Reset the producer id if an expired batch has previously been sent to the broker. Also update the metrics\n    // for expired batches. see the documentation of @TransactionState.resetIdempotentProducerId to understand why\n    // we need to reset the producer id here.\n    if (!expiredBatches.isEmpty())\n        log.trace(\"Expired {} batches in accumulator\", expiredBatches.size());\n    for (ProducerBatch expiredBatch : expiredBatches) {\n        String errorMessage = \"Expiring \" + expiredBatch.recordCount + \" record(s) for \" + expiredBatch.topicPartition\n            + \":\" + (now - expiredBatch.createdMs) + \" ms has passed since batch creation\";\n        failBatch(expiredBatch, -1, NO_TIMESTAMP, new TimeoutException(errorMessage), false);\n        if (transactionManager != null &amp;&amp; expiredBatch.inRetry()) {\n            // This ensures that no new batches are drained until the current in flight batches are fully resolved.\n            transactionManager.markSequenceUnresolved(expiredBatch);\n        }\n    }\n    sensors.updateProduceRequestMetrics(batches);\n\n    // If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately\n    // loop and try sending more data. Otherwise, the timeout will be the smaller value between next batch expiry\n    // time, and the delay time for checking data availability. Note that the nodes may have data that isn't yet\n    // sendable due to lingering, backing off, etc. This specifically does not include nodes with sendable data\n    // that aren't ready to send since they would cause busy looping.\n    long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);\n    pollTimeout = Math.min(pollTimeout, this.accumulator.nextExpiryTimeMs() - now);\n    pollTimeout = Math.max(pollTimeout, 0);\n    if (!result.readyNodes.isEmpty()) {\n        log.trace(\"Nodes with data ready to send: {}\", result.readyNodes);\n        // if some partitions are already ready to be sent, the select time would be 0;\n        // otherwise if some partition already has some data accumulated but not ready yet,\n        // the select time will be the time difference between now and its linger expiry time;\n        // otherwise the select time will be the time difference between now and the metadata expiry time;\n        pollTimeout = 0;\n    }\n    sendProduceRequests(batches, now);\n    return pollTimeout;\n}\n</code></pre>"},{"location":"6-src/clients/clients/producer/internals/Sender/#sendproducerequests","title":"sendProduceRequests","text":"<pre><code>/**\n * Transfer the record batches into a list of produce requests on a per-node basis\n */\nprivate void sendProduceRequests(Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; collated, long now) {\n    for (Map.Entry&lt;Integer, List&lt;ProducerBatch&gt;&gt; entry : collated.entrySet())\n        sendProduceRequest(now, entry.getKey(), acks, requestTimeoutMs, entry.getValue());\n}\n</code></pre>"},{"location":"6-src/clients/clients/producer/internals/Sender/#sendproducerequest","title":"sendProduceRequest","text":"<pre><code>/**\n * Create a produce request from the given record batches\n */\nprivate void sendProduceRequest(long now, int destination, short acks, int timeout, List&lt;ProducerBatch&gt; batches) {\n    if (batches.isEmpty())\n        return;\n\n    final Map&lt;TopicPartition, ProducerBatch&gt; recordsByPartition = new HashMap&lt;&gt;(batches.size());\n\n    // find the minimum magic version used when creating the record sets\n    byte minUsedMagic = apiVersions.maxUsableProduceMagic();\n    for (ProducerBatch batch : batches) {\n        if (batch.magic() &lt; minUsedMagic)\n            minUsedMagic = batch.magic();\n    }\n    ProduceRequestData.TopicProduceDataCollection tpd = new ProduceRequestData.TopicProduceDataCollection();\n    for (ProducerBatch batch : batches) {\n        TopicPartition tp = batch.topicPartition;\n        MemoryRecords records = batch.records();\n\n        // down convert if necessary to the minimum magic used. In general, there can be a delay between the time\n        // that the producer starts building the batch and the time that we send the request, and we may have\n        // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use\n        // the new message format, but found that the broker didn't support it, so we need to down-convert on the\n        // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may\n        // not all support the same message format version. For example, if a partition migrates from a broker\n        // which is supporting the new magic version to one which doesn't, then we will need to convert.\n        if (!records.hasMatchingMagic(minUsedMagic))\n            records = batch.records().downConvert(minUsedMagic, 0, time).records();\n        ProduceRequestData.TopicProduceData tpData = tpd.find(tp.topic());\n        if (tpData == null) {\n            tpData = new ProduceRequestData.TopicProduceData().setName(tp.topic());\n            tpd.add(tpData);\n        }\n        tpData.partitionData().add(new ProduceRequestData.PartitionProduceData()\n                .setIndex(tp.partition())\n                .setRecords(records));\n        recordsByPartition.put(tp, batch);\n    }\n\n    String transactionalId = null;\n    if (transactionManager != null &amp;&amp; transactionManager.isTransactional()) {\n        transactionalId = transactionManager.transactionalId();\n    }\n\n    ProduceRequest.Builder requestBuilder = ProduceRequest.forMagic(minUsedMagic,\n            new ProduceRequestData()\n                    .setAcks(acks)\n                    .setTimeoutMs(timeout)\n                    .setTransactionalId(transactionalId)\n                    .setTopicData(tpd));\n    RequestCompletionHandler callback = response -&gt; handleProduceResponse(response, recordsByPartition, time.milliseconds());\n\n    String nodeId = Integer.toString(destination);\n    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0,\n            requestTimeoutMs, callback);\n    client.send(clientRequest, now);\n    log.trace(\"Sent produce request to {}: {}\", nodeId, requestBuilder);\n}\n</code></pre>"},{"location":"6-src/clients/clients/producer/internals/Sender/#wekeup","title":"wekeup","text":"<pre><code>/**\n * Wake up the selector associated with this send thread\n */\npublic void wakeup() {\n    this.client.wakeup();\n}\n</code></pre>"},{"location":"6-src/clients/common/network/KafkaChannel/","title":"KafkaChannel","text":""},{"location":"6-src/clients/common/network/KafkaChannel/#_1","title":"\u5c5e\u6027","text":"<pre><code>// \u552f\u4e00\u7684 id \u6807\u8bc6\nprivate final String id;\n// \u8bfb\u5199\u90fd\u4f1a\u8f6c\u4ea4\u4e2a transportLayer\nprivate final TransportLayer transportLayer;\nprivate final Supplier&lt;Authenticator&gt; authenticatorCreator;\n// \u7528\u6765 authentication \uff08 \u6216\u8005 re-authentication\uff09\uff0c\u8bfb\u5199\u540c\u6837\u901a\u8fc7 transportLayer \u8fdb\u884c\nprivate Authenticator authenticator;\n// Tracks accumulated network thread time. This is updated on the network thread.\n// The values are read and reset after each response is sent.\nprivate long networkThreadTimeNanos;\nprivate final int maxReceiveSize;\nprivate final MemoryPool memoryPool;\nprivate final ChannelMetadataRegistry metadataRegistry;\n// \u6682\u5b58\u63a5\u6536\u7684\u6570\u636e\nprivate NetworkReceive receive;\n// \u5c06\u8981\u53d1\u9001\u7684\u6570\u636e\nprivate NetworkSend send;\n// Track connection and mute state of channels to enable outstanding requests on channels to be\n// processed after the channel is disconnected.\nprivate boolean disconnected;\nprivate ChannelMuteState muteState;\nprivate ChannelState state;\nprivate SocketAddress remoteAddress;\nprivate int successfulAuthentications;\nprivate boolean midWrite;\nprivate long lastReauthenticationStartNanos;\n</code></pre>"},{"location":"6-src/clients/common/network/KafkaChannel/#_2","title":"\u65b9\u6cd5","text":""},{"location":"6-src/clients/common/network/KafkaChannel/#setsend","title":"setSend","text":"<pre><code>public void setSend(Send send) {\n    // \u4e00\u4e2a channel \u53ea\u80fd\u6682\u5b58\u4e00\u4e2a Send\uff0c\u5f53\u524d Send \u672a\u53d1\u9001\u524d\u4e0d\u80fd\u53d1\u9001\u4e0b\u4e00\u4e2a\n    if (this.send != null)\n        throw new IllegalStateException(\"Attempt to begin a send operation with prior send operation still in progress, connection id is \" + id);\n    this.send = send;\n    this.transportLayer.addInterestOps(SelectionKey.OP_WRITE);\n}\n</code></pre>"},{"location":"6-src/clients/common/network/KafkaChannel/#read-receive","title":"read &amp; receive","text":"<pre><code>public long read() throws IOException {\n    if (receive == null) {\n        receive = new NetworkReceive(maxReceiveSize, id, memoryPool);\n    }\n\n    long bytesReceived = receive(this.receive);\n\n    if (this.receive.requiredMemoryAmountKnown() &amp;&amp; !this.receive.memoryAllocated() &amp;&amp; isInMutableState()) {\n        //pool must be out of memory, mute ourselves.\n        mute();\n    }\n    return bytesReceived;\n}\n</code></pre> <pre><code>private long receive(NetworkReceive receive) throws IOException {\n    try {\n        return receive.readFrom(transportLayer);\n    } catch (SslAuthenticationException e) {\n        // With TLSv1.3, post-handshake messages may throw SSLExceptions, which are\n        // handled as authentication failures\n        String remoteDesc = remoteAddress != null ? remoteAddress.toString() : null;\n        state = new ChannelState(ChannelState.State.AUTHENTICATION_FAILED, e, remoteDesc);\n        throw e;\n    }\n}\n</code></pre>"},{"location":"6-src/clients/common/network/KafkaChannel/#write","title":"write","text":"<pre><code>public long write() throws IOException {\n    if (send == null)\n        return 0;\n\n    midWrite = true;\n    return send.writeTo(transportLayer);\n}\n</code></pre>"},{"location":"6-src/clients/common/network/NetworkReceive/","title":"NetworkReceive","text":"<pre><code>/**\n * A size delimited Receive that consists of a 4 byte network-ordered size N followed by N bytes of content\n */\npublic class NetworkReceive implements Receive {\n\n    public final static String UNKNOWN_SOURCE = \"\";\n    public final static int UNLIMITED = -1;\n    private static final Logger log = LoggerFactory.getLogger(NetworkReceive.class);\n    private static final ByteBuffer EMPTY_BUFFER = ByteBuffer.allocate(0);\n\n    private final String source;\n    // \u5934\u90e8 4 byte \u7684 buffer\n    private final ByteBuffer size;\n    private final int maxSize;\n    private final MemoryPool memoryPool;\n    private int requestedBufferSize = -1;\n    // \u6d88\u606f\u9664\u5934\u90e8 4 byte \u4e4b\u540e\uff0c\u5269\u4f59\u7684\u5185\u5bb9\n    private ByteBuffer buffer;\n\n\n    public NetworkReceive(String source, ByteBuffer buffer) {\n        this.source = source;\n        this.buffer = buffer;\n        this.size = null;\n        this.maxSize = UNLIMITED;\n        this.memoryPool = MemoryPool.NONE;\n    }\n\n    public NetworkReceive(String source) {\n        this.source = source;\n        this.size = ByteBuffer.allocate(4);\n        this.buffer = null;\n        this.maxSize = UNLIMITED;\n        this.memoryPool = MemoryPool.NONE;\n    }\n\n    public NetworkReceive(int maxSize, String source) {\n        this.source = source;\n        this.size = ByteBuffer.allocate(4);\n        this.buffer = null;\n        this.maxSize = maxSize;\n        this.memoryPool = MemoryPool.NONE;\n    }\n\n    public NetworkReceive(int maxSize, String source, MemoryPool memoryPool) {\n        this.source = source;\n        this.size = ByteBuffer.allocate(4);\n        this.buffer = null;\n        this.maxSize = maxSize;\n        this.memoryPool = memoryPool;\n    }\n\n    public NetworkReceive() {\n        this(UNKNOWN_SOURCE);\n    }\n\n    @Override\n    public String source() {\n        return source;\n    }\n\n    @Override\n    public boolean complete() {\n        return !size.hasRemaining() &amp;&amp; buffer != null &amp;&amp; !buffer.hasRemaining();\n    }\n\n    public long readFrom(ScatteringByteChannel channel) throws IOException {\n        int read = 0;\n        if (size.hasRemaining()) {\n            int bytesRead = channel.read(size);\n            if (bytesRead &lt; 0)\n                throw new EOFException();\n            read += bytesRead;\n            if (!size.hasRemaining()) {\n                size.rewind();\n                int receiveSize = size.getInt();\n                if (receiveSize &lt; 0)\n                    throw new InvalidReceiveException(\"Invalid receive (size = \" + receiveSize + \")\");\n                if (maxSize != UNLIMITED &amp;&amp; receiveSize &gt; maxSize)\n                    throw new InvalidReceiveException(\"Invalid receive (size = \" + receiveSize + \" larger than \" + maxSize + \")\");\n                requestedBufferSize = receiveSize; //may be 0 for some payloads (SASL)\n                if (receiveSize == 0) {\n                    buffer = EMPTY_BUFFER;\n                }\n            }\n        }\n        if (buffer == null &amp;&amp; requestedBufferSize != -1) { //we know the size we want but havent been able to allocate it yet\n            buffer = memoryPool.tryAllocate(requestedBufferSize);\n            if (buffer == null)\n                log.trace(\"Broker low on memory - could not allocate buffer of size {} for source {}\", requestedBufferSize, source);\n        }\n        if (buffer != null) {\n            int bytesRead = channel.read(buffer);\n            if (bytesRead &lt; 0)\n                throw new EOFException();\n            read += bytesRead;\n        }\n\n        return read;\n    }\n\n    @Override\n    public boolean requiredMemoryAmountKnown() {\n        return requestedBufferSize != -1;\n    }\n\n    @Override\n    public boolean memoryAllocated() {\n        return buffer != null;\n    }\n\n\n    @Override\n    public void close() throws IOException {\n        if (buffer != null &amp;&amp; buffer != EMPTY_BUFFER) {\n            memoryPool.release(buffer);\n            buffer = null;\n        }\n    }\n\n    public ByteBuffer payload() {\n        return this.buffer;\n    }\n\n    public int bytesRead() {\n        if (buffer == null)\n            return size.position();\n        return buffer.position() + size.position();\n    }\n\n    /**\n     * Returns the total size of the receive including payload and size buffer\n     * for use in metrics. This is consistent with {@link NetworkSend#size()}\n     */\n    public int size() {\n        return payload().limit() + size.limit();\n    }\n\n}\n</code></pre>"},{"location":"6-src/clients/common/network/NetworkSend/","title":"NetworkSend","text":"<pre><code>public class NetworkSend implements Send {\n    private final String destinationId;\n    private final Send send;\n\n    public NetworkSend(String destinationId, Send send) {\n        this.destinationId = destinationId;\n        this.send = send;\n    }\n\n    public String destinationId() {\n        return destinationId;\n    }\n\n    @Override\n    public boolean completed() {\n        return send.completed();\n    }\n\n    @Override\n    public long writeTo(TransferableChannel channel) throws IOException {\n        return send.writeTo(channel);\n    }\n\n    @Override\n    public long size() {\n        return send.size();\n    }\n\n}\n</code></pre>"},{"location":"6-src/clients/common/network/Receive/","title":"Receive","text":"<pre><code>package org.apache.kafka.common.network;\n\nimport java.io.Closeable;\nimport java.io.IOException;\nimport java.nio.channels.ScatteringByteChannel;\n\n/**\n * This interface models the in-progress reading of data from a channel to a source identified by an integer id\n */\npublic interface Receive extends Closeable {\n\n    /**\n     * The numeric id of the source from which we are receiving data.\n     */\n    String source();\n\n    /**\n     * Are we done receiving data?\n     */\n    boolean complete();\n\n    /**\n     * Read bytes into this receive from the given channel\n     * @param channel The channel to read from\n     * @return The number of bytes read\n     * @throws IOException If the reading fails\n     */\n    long readFrom(ScatteringByteChannel channel) throws IOException;\n\n    /**\n     * Do we know yet how much memory we require to fully read this\n     */\n    boolean requiredMemoryAmountKnown();\n\n    /**\n     * Has the underlying memory required to complete reading been allocated yet?\n     */\n    boolean memoryAllocated();\n}\n</code></pre>"},{"location":"6-src/clients/common/network/Selector/","title":"Selector","text":"<p>a nioSelector interface for doing non-blocking multi-connection network I/O.</p> <p>This class works with <code>NetworkSend</code> and <code>NetworkReceive</code> to transmit size-delimited network requests and responses.</p> <p>a connection can be added to the nioSelector associated with an integer id by doing.</p> <pre><code>nioSelector.connect(\"42\", new InetSocketAddress(\"google.com\", server.port), 64000, 64000);\n</code></pre> <p>the <code>connect</code> call does not block on the creation of the TCP connection, so the <code>connect</code> method only begins initiating the connection. the successful invocation of this method does not mean a valid connection has been established.</p> <p>Sending requests, receiving responses, processing connection completions, and disconnections on the existing connections are all done using the <code>poll()</code> call.</p> <pre><code>nioSelector.send(new NetworkSend(myDestination, myBytes));\nnioSelector.send(new NetworkSend(myOtherDestination, myOtherBytes));\nnioSelector.poll(TIMEOUT_MS);\n</code></pre> <p>The nioSelector maintains several lists that are reset by each call to <code>poll()</code> which are available via various getters. These are reset by each call to <code>poll()</code>.</p> <p>This class is not thread safe!</p>"},{"location":"6-src/clients/common/network/Selector/#_1","title":"\u5c5e\u6027","text":"<pre><code>private final Logger log;\n\n// java nio Selector\nprivate final java.nio.channels.Selector nioSelector;\n\n// key \u4e3a connection id\uff0c\u503c\u4e3a\u5bf9\u5e94\u7684 KafkaChannel\nprivate final Map&lt;String, KafkaChannel&gt; channels;\n// \nprivate final Set&lt;KafkaChannel&gt; explicitlyMutedChannels;\n\nprivate boolean outOfMemory;\n\n// \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u53d1\u9001\u5b8c\u6210\u7684 Send\nprivate final List&lt;NetworkSend&gt; completedSends;\n// \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u63a5\u6536\u5b8c\u6210\u7684 Receive\nprivate final LinkedHashMap&lt;String, NetworkReceive&gt; completedReceives;\n// \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u8fde\u63a5 connection \u5bf9\u5e94\u7684 id\nprivate final List&lt;String&gt; connected;\n// \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u5df2\u65ad\u5f00\u8fde\u63a5 connection \u5bf9\u5e94\u7684 id \u548c \u72b6\u6001\nprivate final Map&lt;String, ChannelState&gt; disconnected;\n// \u8c03\u7528 poll \u4e4b\u540e\uff0c\u8bb0\u5f55\u53d1\u9001\u5931\u8d25\u7684 Send \u5bf9\u5e94\u7684 connection id\nprivate final List&lt;String&gt; failedSends;\n\n// \u8bb0\u5f55\u7acb\u5373\u5df2\u8fde\u63a5\u7684 connection key\nprivate final Set&lt;SelectionKey&gt; immediatelyConnectedKeys;\n\n// \nprivate final Map&lt;String, KafkaChannel&gt; closingChannels;\n\n// \nprivate Set&lt;SelectionKey&gt; keysWithBufferedRead;\n\n\n\nprivate final Time time;\nprivate final SelectorMetrics sensors;\nprivate final ChannelBuilder channelBuilder;\n// Max size in bytes of a single network receive (use {@link NetworkReceive#UNLIMITED} for no limit)jk:w\nprivate final int maxReceiveSize;\nprivate final boolean recordTimePerConnection;\nprivate final IdleExpiryManager idleExpiryManager;\nprivate final LinkedHashMap&lt;String, DelayedAuthenticationFailureClose&gt; delayedClosingChannels;\nprivate final MemoryPool memoryPool;\nprivate final long lowMemThreshold;\nprivate final int failedAuthenticationDelayMs;\n</code></pre>"},{"location":"6-src/clients/common/network/Selector/#_2","title":"\u65b9\u6cd5","text":"<p>\u6784\u9020\u65b9\u6cd5\u4e2d\uff0c\u6267\u884c\uff1a</p> <pre><code>try {\n    this.nioSelector = java.nio.channels.Selector.open();\n} catch (IOException e) {\n    throw new KafkaException(e);\n}\n</code></pre>"},{"location":"6-src/clients/common/network/Selector/#connect","title":"connect","text":"<pre><code>/**\n * Begin connecting to the given address and add the connection to this nioSelector associated with the given id\n * number.\n * &lt;p&gt;\n * Note that this call only initiates the connection, which will be completed on a future {@link #poll(long)}\n * call. Check {@link #connected()} to see which (if any) connections have completed after a given poll call.\n * @param id The id for the new connection\n * @param address The address to connect to\n * @param sendBufferSize The send buffer for the new connection\n * @param receiveBufferSize The receive buffer for the new connection\n * @throws IllegalStateException if there is already a connection for that id\n * @throws IOException if DNS resolution fails on the hostname or if the broker is down\n */\n@Override\npublic void connect(String id, InetSocketAddress address, int sendBufferSize, int receiveBufferSize) throws IOException {\n    ensureNotRegistered(id);\n    SocketChannel socketChannel = SocketChannel.open();\n    SelectionKey key = null;\n    try {\n        configureSocketChannel(socketChannel, sendBufferSize, receiveBufferSize);\n        // \u8c03\u7528 channel.connect(address)\n        boolean connected = doConnect(socketChannel, address);\n        // 1. \u5c06 channel \u6ce8\u518c\u5230 nioSelector\uff0c\u83b7\u5f97 SelectionKey\n        // 2. \u6784\u9020 KafkaChannel\n        // 3. \u5c06 id, KafkaChannel \u5bf9\u5e94\u5173\u7cfb\u8bb0\u5f55\u5230 this.channels\n        // 4. \u5982\u679c\u6709 idelExpiryManage\uff0c\u5c06 connection id \u52a0\u5165\n        // 5. \u5e76\u8fd4\u56de SelectionKey\n        key = registerChannel(id, socketChannel, SelectionKey.OP_CONNECT);\n\n        if (connected) {\n            // OP_CONNECT won't trigger for immediately connected channels\n            log.debug(\"Immediately connected to node {}\", id);\n            immediatelyConnectedKeys.add(key);\n            key.interestOps(0);\n        }\n    } catch (IOException | RuntimeException e) {\n        if (key != null)\n            immediatelyConnectedKeys.remove(key);\n        channels.remove(id);\n        socketChannel.close();\n        throw e;\n    }\n}\n</code></pre>"},{"location":"6-src/clients/common/network/Selector/#send","title":"send","text":"<pre><code>/**\n * Queue the given request for sending in the subsequent {@link #poll(long)} calls\n * @param send The request to send\n */\npublic void send(NetworkSend send) {\n    // 1. destinationId \u5373\u4e3a this.channels \u4e2d\u7684 connectionId\n    String connectionId = send.destinationId();\n    // 2. \u6839\u636e id \u4ece this.channels \u6216\u8005 closingChannels \u4e2d\u83b7\u53d6 id \u5bf9\u5e94\u7684 KafkaChannel\n    KafkaChannel channel = openOrClosingChannelOrFail(connectionId);\n    if (closingChannels.containsKey(connectionId)) {\n        // ensure notification via `disconnected`, leave channel in the state in which closing was triggered\n        this.failedSends.add(connectionId);\n    } else {\n        try {\n            // 3.1. \u5c06 KafkaChannel \u7684 send \u8bbe\u7f6e\u4e3a send\n            // 3.2. TransportLayer SelectionKey \u8bbe\u7f6e OP_WRITE \u4e8b\u4ef6\n            channel.setSend(send);\n        } catch (Exception e) {\n            // update the state for consistency, the channel will be discarded after `close`\n            channel.state(ChannelState.FAILED_SEND);\n            // ensure notification via `disconnected` when `failedSends` are processed in the next poll\n            this.failedSends.add(connectionId);\n            close(channel, CloseMode.DISCARD_NO_NOTIFY);\n            if (!(e instanceof CancelledKeyException)) {\n                log.error(\"Unexpected exception during send, closing connection {} and rethrowing exception {}\",\n                        connectionId, e);\n                throw e;\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"6-src/clients/common/network/Selector/#poll","title":"poll","text":"<pre><code>/**\n * Do whatever I/O can be done on each connection without blocking. This includes completing connections, completing\n * disconnections, initiating new sends, or making progress on in-progress sends or receives.\n *\n * When this call is completed the user can check for completed sends, receives, connections or disconnects using\n * {@link #completedSends()}, {@link #completedReceives()}, {@link #connected()}, {@link #disconnected()}. These\n * lists will be cleared at the beginning of each `poll` call and repopulated by the call if there is\n * any completed I/O.\n *\n * In the \"Plaintext\" setting, we are using socketChannel to read &amp; write to the network. But for the \"SSL\" setting,\n * we encrypt the data before we use socketChannel to write data to the network, and decrypt before we return the responses.\n * This requires additional buffers to be maintained as we are reading from network, since the data on the wire is encrypted\n * we won't be able to read exact no.of bytes as kafka protocol requires. We read as many bytes as we can, up to SSLEngine's\n * application buffer size. This means we might be reading additional bytes than the requested size.\n * If there is no further data to read from socketChannel selector won't invoke that channel and we have additional bytes\n * in the buffer. To overcome this issue we added \"keysWithBufferedRead\" map which tracks channels which have data in the SSL\n * buffers. If there are channels with buffered data that can by processed, we set \"timeout\" to 0 and process the data even\n * if there is no more data to read from the socket.\n *\n * Atmost one entry is added to \"completedReceives\" for a channel in each poll. This is necessary to guarantee that\n * requests from a channel are processed on the broker in the order they are sent. Since outstanding requests added\n * by SocketServer to the request queue may be processed by different request handler threads, requests on each\n * channel must be processed one-at-a-time to guarantee ordering.\n *\n * @param timeout The amount of time to wait, in milliseconds, which must be non-negative\n * @throws IllegalArgumentException If `timeout` is negative\n * @throws IllegalStateException If a send is given for which we have no existing connection or for which there is\n *         already an in-progress send\n */\n@Override\npublic void poll(long timeout) throws IOException {\n    if (timeout &lt; 0)\n        throw new IllegalArgumentException(\"timeout should be &gt;= 0\");\n\n    boolean madeReadProgressLastCall = madeReadProgressLastPoll;\n    // 1. \u6e05\u7a7a completedSends, completedReceives \u7b49\u8bb0\u5f55\n    clear();\n\n    boolean dataInBuffers = !keysWithBufferedRead.isEmpty();\n\n    if (!immediatelyConnectedKeys.isEmpty() || (madeReadProgressLastCall &amp;&amp; dataInBuffers))\n        timeout = 0;\n\n    if (!memoryPool.isOutOfMemory() &amp;&amp; outOfMemory) {\n        //we have recovered from memory pressure. unmute any channel not explicitly muted for other reasons\n        log.trace(\"Broker no longer low on memory - unmuting incoming sockets\");\n        for (KafkaChannel channel : channels.values()) {\n            if (channel.isInMutableState() &amp;&amp; !explicitlyMutedChannels.contains(channel)) {\n                channel.maybeUnmute();\n            }\n        }\n        outOfMemory = false;\n    }\n\n    /* check ready keys */\n    long startSelect = time.nanoseconds();\n    // \u8c03\u7528 java nio select\uff0c\u5e76\u8fd4\u56de\uff08select \u8fd9\u91cc\u4f1a\u963b\u585e\uff09\n    int numReadyKeys = select(timeout);\n    long endSelect = time.nanoseconds();\n    this.sensors.selectTime.record(endSelect - startSelect, time.milliseconds());\n\n    if (numReadyKeys &gt; 0 || !immediatelyConnectedKeys.isEmpty() || dataInBuffers) {\n        // \u83b7\u53d6\u6709\u4e8b\u4ef6\u53d1\u751f\u7684 key\n        Set&lt;SelectionKey&gt; readyKeys = this.nioSelector.selectedKeys();\n\n        // Poll from channels that have buffered data (but nothing more from the underlying socket)\n        if (dataInBuffers) {\n            keysWithBufferedRead.removeAll(readyKeys); //so no channel gets polled twice\n            Set&lt;SelectionKey&gt; toPoll = keysWithBufferedRead;\n            keysWithBufferedRead = new HashSet&lt;&gt;(); //poll() calls will repopulate if needed\n            pollSelectionKeys(toPoll, false, endSelect);\n        }\n\n        // Poll from channels where the underlying socket has more data\n        pollSelectionKeys(readyKeys, false, endSelect);\n        // Clear all selected keys so that they are included in the ready count for the next select\n        readyKeys.clear();\n\n        pollSelectionKeys(immediatelyConnectedKeys, true, endSelect);\n        immediatelyConnectedKeys.clear();\n    } else {\n        madeReadProgressLastPoll = true; //no work is also \"progress\"\n    }\n\n    long endIo = time.nanoseconds();\n    this.sensors.ioTime.record(endIo - endSelect, time.milliseconds());\n\n    // Close channels that were delayed and are now ready to be closed\n    completeDelayedChannelClose(endIo);\n\n    // we use the time at the end of select to ensure that we don't close any connections that\n    // have just been processed in pollSelectionKeys\n    maybeCloseOldestConnection(endSelect);\n}\n</code></pre>"},{"location":"6-src/clients/common/network/Selector/#pollselectionkeys","title":"pollSelectionKeys","text":"<pre><code>    /**\n     * handle any ready I/O on a set of selection keys\n     * @param selectionKeys set of keys to handle\n     * @param isImmediatelyConnected true if running over a set of keys for just-connected sockets\n     * @param currentTimeNanos time at which set of keys was determined\n     */\n    // package-private for testing\n    void pollSelectionKeys(Set&lt;SelectionKey&gt; selectionKeys,\n                           boolean isImmediatelyConnected,\n                           long currentTimeNanos) {\n        for (SelectionKey key : determineHandlingOrder(selectionKeys)) {\n            KafkaChannel channel = channel(key);\n            long channelStartTimeNanos = recordTimePerConnection ? time.nanoseconds() : 0;\n            boolean sendFailed = false;\n            String nodeId = channel.id();\n\n            // register all per-connection metrics at once\n            sensors.maybeRegisterConnectionMetrics(nodeId);\n            if (idleExpiryManager != null)\n                idleExpiryManager.update(nodeId, currentTimeNanos);\n\n            try {\n                /* complete any connections that have finished their handshake (either normally or immediately) */\n                if (isImmediatelyConnected || key.isConnectable()) {\n                    if (channel.finishConnect()) {\n                        this.connected.add(nodeId);\n                        this.sensors.connectionCreated.record();\n\n                        SocketChannel socketChannel = (SocketChannel) key.channel();\n                        log.debug(\"Created socket with SO_RCVBUF = {}, SO_SNDBUF = {}, SO_TIMEOUT = {} to node {}\",\n                                socketChannel.socket().getReceiveBufferSize(),\n                                socketChannel.socket().getSendBufferSize(),\n                                socketChannel.socket().getSoTimeout(),\n                                nodeId);\n                    } else {\n                        continue;\n                    }\n                }\n\n                /* if channel is not ready finish prepare */\n                if (channel.isConnected() &amp;&amp; !channel.ready()) {\n                    channel.prepare();\n                    if (channel.ready()) {\n                        long readyTimeMs = time.milliseconds();\n                        boolean isReauthentication = channel.successfulAuthentications() &gt; 1;\n                        if (isReauthentication) {\n                            sensors.successfulReauthentication.record(1.0, readyTimeMs);\n                            if (channel.reauthenticationLatencyMs() == null)\n                                log.warn(\n                                    \"Should never happen: re-authentication latency for a re-authenticated channel was null; continuing...\");\n                            else\n                                sensors.reauthenticationLatency\n                                    .record(channel.reauthenticationLatencyMs().doubleValue(), readyTimeMs);\n                        } else {\n                            sensors.successfulAuthentication.record(1.0, readyTimeMs);\n                            if (!channel.connectedClientSupportsReauthentication())\n                                sensors.successfulAuthenticationNoReauth.record(1.0, readyTimeMs);\n                        }\n                        log.debug(\"Successfully {}authenticated with {}\", isReauthentication ?\n                            \"re-\" : \"\", channel.socketDescription());\n                    }\n                }\n                if (channel.ready() &amp;&amp; channel.state() == ChannelState.NOT_CONNECTED)\n                    channel.state(ChannelState.READY);\n                Optional&lt;NetworkReceive&gt; responseReceivedDuringReauthentication = channel.pollResponseReceivedDuringReauthentication();\n                responseReceivedDuringReauthentication.ifPresent(receive -&gt; {\n                    long currentTimeMs = time.milliseconds();\n                    addToCompletedReceives(channel, receive, currentTimeMs);\n                });\n\n                //if channel is ready and has bytes to read from socket or buffer, and has no\n                //previous completed receive then read from it\n                if (channel.ready() &amp;&amp; (key.isReadable() || channel.hasBytesBuffered()) &amp;&amp; !hasCompletedReceive(channel)\n                        &amp;&amp; !explicitlyMutedChannels.contains(channel)) {\n                    attemptRead(channel);\n                }\n\n                if (channel.hasBytesBuffered()) {\n                    //this channel has bytes enqueued in intermediary buffers that we could not read\n                    //(possibly because no memory). it may be the case that the underlying socket will\n                    //not come up in the next poll() and so we need to remember this channel for the\n                    //next poll call otherwise data may be stuck in said buffers forever. If we attempt\n                    //to process buffered data and no progress is made, the channel buffered status is\n                    //cleared to avoid the overhead of checking every time.\n                    keysWithBufferedRead.add(key);\n                }\n\n                /* if channel is ready write to any sockets that have space in their buffer and for which we have data */\n\n                long nowNanos = channelStartTimeNanos != 0 ? channelStartTimeNanos : currentTimeNanos;\n                try {\n                    attemptWrite(key, channel, nowNanos);\n                } catch (Exception e) {\n                    sendFailed = true;\n                    throw e;\n                }\n\n                /* cancel any defunct sockets */\n                if (!key.isValid())\n                    close(channel, CloseMode.GRACEFUL);\n\n            } catch (Exception e) {\n                String desc = channel.socketDescription();\n                if (e instanceof IOException) {\n                    log.debug(\"Connection with {} disconnected\", desc, e);\n                } else if (e instanceof AuthenticationException) {\n                    boolean isReauthentication = channel.successfulAuthentications() &gt; 0;\n                    if (isReauthentication)\n                        sensors.failedReauthentication.record();\n                    else\n                        sensors.failedAuthentication.record();\n                    String exceptionMessage = e.getMessage();\n                    if (e instanceof DelayedResponseAuthenticationException)\n                        exceptionMessage = e.getCause().getMessage();\n                    log.info(\"Failed {}authentication with {} ({})\", isReauthentication ? \"re-\" : \"\",\n                        desc, exceptionMessage);\n                } else {\n                    log.warn(\"Unexpected error from {}; closing connection\", desc, e);\n                }\n\n                if (e instanceof DelayedResponseAuthenticationException)\n                    maybeDelayCloseOnAuthenticationFailure(channel);\n                else\n                    close(channel, sendFailed ? CloseMode.NOTIFY_ONLY : CloseMode.GRACEFUL);\n            } finally {\n                maybeRecordTimePerConnection(channel, channelStartTimeNanos);\n            }\n        }\n    }\n</code></pre>"},{"location":"6-src/clients/common/network/Selector/#wakeup","title":"wakeup","text":"<pre><code>/**\n * Interrupt the nioSelector if it is blocked waiting to do I/O.\n */\n@Override\npublic void wakeup() {\n    this.nioSelector.wakeup();\n}\n</code></pre>"},{"location":"6-src/clients/common/network/Selector/#write","title":"write","text":"<pre><code>// package-private for testing\nvoid write(KafkaChannel channel) throws IOException {\n    String nodeId = channel.id();\n    long bytesSent = channel.write();\n    Send send = channel.maybeCompleteSend();\n    // We may complete the send with bytesSent &lt; 1 if `TransportLayer.hasPendingWrites` was true and `channel.write()`\n    // caused the pending writes to be written to the socket channel buffer\n    if (bytesSent &gt; 0 || send != null) {\n        long currentTimeMs = time.milliseconds();\n        if (bytesSent &gt; 0)\n            this.sensors.recordBytesSent(nodeId, bytesSent, currentTimeMs);\n        if (send != null) {\n            this.completedSends.add(send);\n            this.sensors.recordCompletedSend(nodeId, send.size(), currentTimeMs);\n        }\n    }\n}\n</code></pre>"},{"location":"6-src/clients/common/network/Send/","title":"Send","text":"<pre><code>package org.apache.kafka.common.network;\n\nimport java.io.IOException;\n\n/**\n * This interface models the in-progress sending of data.\n */\npublic interface Send {\n\n    /**\n     * Is this send complete?\n     */\n    boolean completed();\n\n    /**\n     * Write some as-yet unwritten bytes from this send to the provided channel. It may take multiple calls for the send\n     * to be completely written\n     * @param channel The Channel to write to\n     * @return The number of bytes written\n     * @throws IOException If the write fails\n     */\n    long writeTo(TransferableChannel channel) throws IOException;\n\n    /**\n     * Size of the send\n     */\n    long size();\n\n}\n</code></pre>"},{"location":"6-src/clients/common/network/TransportLayer/","title":"TransportLayer","text":"<p>Transport layer for underlying communication.</p> <p>At very basic level it is wrapper around SocketChannel and can be used as substitute for SocketChannel and other network Channel implementations.</p> <p>As NetworkClient replaces BlockingChannel and other implementations we will be using KafkaChannel as a network I/O channel.</p> <pre><code>public interface TransportLayer extends ScatteringByteChannel, GatheringByteChannel {\n\n    /**\n     * Returns true if the channel has handshake and authentication done.\n     */\n    boolean ready();\n\n    /**\n     * Finishes the process of connecting a socket channel.\n     */\n    boolean finishConnect() throws IOException;\n\n    /**\n     * disconnect socketChannel\n     */\n    void disconnect();\n\n    /**\n     * Tells whether or not this channel's network socket is connected.\n     */\n    boolean isConnected();\n\n    /**\n     * returns underlying socketChannel\n     */\n    SocketChannel socketChannel();\n\n    /**\n     * Get the underlying selection key\n     */\n    SelectionKey selectionKey();\n\n    /**\n     * This a no-op for the non-secure PLAINTEXT implementation. For SSL, this performs\n     * SSL handshake. The SSL handshake includes client authentication if configured using\n     * {@link org.apache.kafka.common.config.SslConfigs#SSL_CLIENT_AUTH_CONFIG}.\n     * @throws AuthenticationException if handshake fails due to an {@link javax.net.ssl.SSLException}.\n     * @throws IOException if read or write fails with an I/O error.\n    */\n    void handshake() throws AuthenticationException, IOException;\n\n    /**\n     * Returns true if there are any pending writes\n     */\n    boolean hasPendingWrites();\n\n    /**\n     * Returns `SSLSession.getPeerPrincipal()` if this is an SslTransportLayer and there is an authenticated peer,\n     * `KafkaPrincipal.ANONYMOUS` is returned otherwise.\n     */\n    Principal peerPrincipal() throws IOException;\n\n    void addInterestOps(int ops);\n\n    void removeInterestOps(int ops);\n\n    boolean isMute();\n\n    /**\n     * @return true if channel has bytes to be read in any intermediate buffers\n     * which may be processed without reading additional data from the network.\n     */\n    boolean hasBytesBuffered();\n\n    /**\n     * Transfers bytes from `fileChannel` to this `TransportLayer`.\n     *\n     * This method will delegate to {@link FileChannel#transferTo(long, long, java.nio.channels.WritableByteChannel)},\n     * but it will unwrap the destination channel, if possible, in order to benefit from zero copy. This is required\n     * because the fast path of `transferTo` is only executed if the destination buffer inherits from an internal JDK\n     * class.\n     *\n     * @param fileChannel The source channel\n     * @param position The position within the file at which the transfer is to begin; must be non-negative\n     * @param count The maximum number of bytes to be transferred; must be non-negative\n     * @return The number of bytes, possibly zero, that were actually transferred\n     * @see FileChannel#transferTo(long, long, java.nio.channels.WritableByteChannel)\n     */\n    long transferFrom(FileChannel fileChannel, long position, long count) throws IOException;\n}\n</code></pre>"},{"location":"6-src/core/kafka/","title":"Kafka","text":"<ul> <li>admin: \u7ba1\u7406\u547d\u4ee4<ul> <li>TopicCommand</li> </ul> </li> <li>api:</li> <li>cluster: \u96c6\u7fa4\u76f8\u5173\u4ee3\u7801\uff0c\u5305\u542b Partition, Replica \u7b49<ul> <li>Broker: Broker \u5b9a\u4e49</li> <li>BrokerEndPoint</li> <li>Cluster: Cluster \u5b9a\u4e49\uff0cThe set of active brokers in the cluster</li> <li>EndPoint</li> <li>Partition: Data structure that represents a topic partition. The leader maintains the AR, ISR, CUR, RAR</li> <li>Replica</li> </ul> </li> <li>common: \u4e00\u4e9b\u5e38\u7528\u7684\u516c\u5171\u7c7b(Exception, ...)</li> <li>consumer: \u6d88\u8d39\u8005\u4ee3\u7801\uff08\u5df2\u7ecf deprecated\uff0c\u5e76\u4e14\u632a\u5230 clients \u6a21\u5757\uff09</li> <li>controller: KafkaController</li> <li>coordinator:<ul> <li>group<ul> <li>GroupCoordinator</li> </ul> </li> <li>transaction</li> </ul> </li> <li>log: log \u7ba1\u7406\u6a21\u5757\uff08kafka \u5b58\u50a8\u6d88\u606f\u7684\u65b9\u5f0f\uff09</li> <li>message<ul> <li>CompressionCodec: \u538b\u7f29\u65b9\u5f0f\u7684\u5b9a\u4e49</li> </ul> </li> <li>metrics: kafka \u76d1\u63a7\u7edf\u8ba1</li> <li>network: \u7f51\u7edc\u5c42\u5904\u7406\uff0cnio \u7684\u4e00\u5c42\u5c01\u88c5<ul> <li>RequestChannel</li> <li>SocketServer</li> </ul> </li> <li>raft</li> <li>security: \u6743\u9650\u7ba1\u7406<ul> <li>auth</li> </ul> </li> <li>serializer: \u5b9a\u4e49 Encoder \u548c Docoder \u63a5\u53e3\u548c\u4e00\u4e9b\u57fa\u7840\u7684\u5982 String, Long \u7684\u5b9e\u73b0<ul> <li>Decoder</li> </ul> </li> <li>server: kafka server \u7684\u4e3b\u8981\u5b9e\u73b0\u903b\u8f91<ul> <li>checkpoints</li> <li>epoch</li> <li>kafkaServer</li> <li>KafkaSErverStartable: \u521b\u5efa <code>KafkaServer</code></li> <li>KafkaConfig: kafka \u914d\u7f6e\u8bbe\u7f6e\u5b9e\u73b0<ul> <li><code>object Defaults</code></li> <li><code>object KafkaConifg</code></li> </ul> </li> </ul> </li> <li>tools: \u5404\u79cd\u53ef\u4ee5\u72ec\u7acb\u8fd0\u884c\u7684\u5de5\u5177<ul> <li>ConsoleConsumer</li> <li>ConsoleProducer</li> </ul> </li> <li>utils: \u5404\u79cd\u5de5\u5177\u7c7b<ul> <li>json</li> <li>timer</li> </ul> </li> <li>zk<ul> <li>AdminZkClient</li> <li>KafkaZkClient</li> <li>ZkData</li> </ul> </li> <li>zookeeper<ul> <li>ZooKeeperClient</li> </ul> </li> <li>Kafka: kafka \u542f\u52a8\u5165\u53e3<ul> <li><code>object Kafka extends Logging</code>\uff1a\u83b7\u53d6\u914d\u7f6e\u9879\uff0c\u542f\u52a8\u4ee3\u7406 <code>KafkaServerStartable</code></li> </ul> </li> </ul>"},{"location":"6-src/core/kafka/controller/ControllerState/","title":"ControllerState","text":"<pre><code>sealed abstract class ControllerState {\n\n  def value: Byte\n\n  def rateAndTimeMetricName: Option[String] =\n    if (hasRateAndTimeMetric) Some(s\"${toString}RateAndTimeMs\") else None\n\n  protected def hasRateAndTimeMetric: Boolean = true\n}\n\nobject ControllerState {\n\n  // Note: `rateAndTimeMetricName` is based on the case object name by default. Changing a name is a breaking change\n  // unless `rateAndTimeMetricName` is overridden.\n\n  case object Idle extends ControllerState {\n    def value = 0\n    override protected def hasRateAndTimeMetric: Boolean = false\n  }\n\n  case object ControllerChange extends ControllerState {\n    def value = 1\n  }\n\n  case object BrokerChange extends ControllerState {\n    def value = 2\n    // The LeaderElectionRateAndTimeMs metric existed before `ControllerState` was introduced and we keep the name\n    // for backwards compatibility. The alternative would be to have the same metric under two different names.\n    override def rateAndTimeMetricName = Some(\"LeaderElectionRateAndTimeMs\")\n  }\n\n  case object TopicChange extends ControllerState {\n    def value = 3\n  }\n\n  case object TopicDeletion extends ControllerState {\n    def value = 4\n  }\n\n  case object AlterPartitionReassignment extends ControllerState {\n    def value = 5\n\n    override def rateAndTimeMetricName: Option[String] = Some(\"PartitionReassignmentRateAndTimeMs\")\n  }\n\n  case object AutoLeaderBalance extends ControllerState {\n    def value = 6\n  }\n\n  case object ManualLeaderBalance extends ControllerState {\n    def value = 7\n  }\n\n  case object ControlledShutdown extends ControllerState {\n    def value = 8\n  }\n\n  case object IsrChange extends ControllerState {\n    def value = 9\n  }\n\n  case object LeaderAndIsrResponseReceived extends ControllerState {\n    def value = 10\n  }\n\n  case object LogDirChange extends ControllerState {\n    def value = 11\n  }\n\n  case object ControllerShutdown extends ControllerState {\n    def value = 12\n  }\n\n  case object UncleanLeaderElectionEnable extends ControllerState {\n    def value = 13\n  }\n\n  case object TopicUncleanLeaderElectionEnable extends ControllerState {\n    def value = 14\n  }\n\n  case object ListPartitionReassignment extends ControllerState {\n    def value = 15\n  }\n\n  case object UpdateMetadataResponseReceived extends ControllerState {\n    def value = 16\n\n    override protected def hasRateAndTimeMetric: Boolean = false\n  }\n\n  case object UpdateFeatures extends ControllerState {\n    def value = 17\n  }\n\n  val values: Seq[ControllerState] = Seq(Idle, ControllerChange, BrokerChange, TopicChange, TopicDeletion,\n    AlterPartitionReassignment, AutoLeaderBalance, ManualLeaderBalance, ControlledShutdown, IsrChange,\n    LeaderAndIsrResponseReceived, LogDirChange, ControllerShutdown, UncleanLeaderElectionEnable,\n    TopicUncleanLeaderElectionEnable, ListPartitionReassignment, UpdateMetadataResponseReceived,\n    UpdateFeatures)\n}\n</code></pre>"},{"location":"6-src/core/kafka/controller/ReplicaStateMachine/","title":"ReplicaStateMachine","text":""},{"location":"6-src/core/kafka/controller/ControllerContext/","title":"ControllerContext","text":""},{"location":"6-src/core/kafka/controller/ControllerContext/ReplicaAssignment/","title":"ReplicaAssignment","text":"<p>\u8bb0\u5f55\uff1a - replicas - addingReplicas - removingReplicas - originReplicas - targetReplicas</p> <pre><code>object ReplicaAssignment {\n  def apply(replicas: Seq[Int]): ReplicaAssignment = {\n    apply(replicas, Seq.empty, Seq.empty)\n  }\n\n  val empty: ReplicaAssignment = apply(Seq.empty)\n}\n\n\n/**\n * @param replicas the sequence of brokers assigned to the partition. It includes the set of brokers\n *                 that were added (`addingReplicas`) and removed (`removingReplicas`).\n * @param addingReplicas the replicas that are being added if there is a pending reassignment\n * @param removingReplicas the replicas that are being removed if there is a pending reassignment\n */\ncase class ReplicaAssignment private (replicas: Seq[Int],\n                                      addingReplicas: Seq[Int],\n                                      removingReplicas: Seq[Int]) {\n\n  lazy val originReplicas: Seq[Int] = replicas.diff(addingReplicas)\n  lazy val targetReplicas: Seq[Int] = replicas.diff(removingReplicas)\n\n  def isBeingReassigned: Boolean = {\n    addingReplicas.nonEmpty || removingReplicas.nonEmpty\n  }\n\n  def reassignTo(target: Seq[Int]): ReplicaAssignment = {\n    val fullReplicaSet = (target ++ originReplicas).distinct\n    ReplicaAssignment(\n      fullReplicaSet,\n      fullReplicaSet.diff(originReplicas),\n      fullReplicaSet.diff(target)\n    )\n  }\n\n  def removeReplica(replica: Int): ReplicaAssignment = {\n    ReplicaAssignment(\n      replicas.filterNot(_ == replica),\n      addingReplicas.filterNot(_ == replica),\n      removingReplicas.filterNot(_ == replica)\n    )\n  }\n\n  override def toString: String = s\"ReplicaAssignment(\" +\n    s\"replicas=${replicas.mkString(\",\")}, \" +\n    s\"addingReplicas=${addingReplicas.mkString(\",\")}, \" +\n    s\"removingReplicas=${removingReplicas.mkString(\",\")})\"\n}\n</code></pre>"},{"location":"6-src/core/kafka/controller/ControllerEventManager/","title":"ControllerEventManager","text":"<p><code>ControllerEventManager</code> \u7ef4\u62a4 <code>QueuedEvent</code>(\u5bf9 <code>ControllerEvent</code> \u7684\u5305\u88c5) \u7684\u961f\u5217\uff0c\u5176\u542f\u52a8\u7ebf\u7a0b\uff1a 1. \u4ece <code>queue</code> \u4e2d\u53d6\u4e8b\u4ef6 <code>QueuedEvent</code> 2. \u901a\u8fc7 <code>processor</code> \u5bf9\u4e8b\u4ef6\u8fdb\u884c\u5904\u7406</p> <p>\u8fd9\u91cc\u7684 <code>processor</code> \u662f <code>KafkaController</code>\uff0c\u5176\u5b9e\u73b0\u4e86 <code>ControllerEventProcessor</code> \u5b9a\u4e49\u7684\u65b9\u6cd5</p> <pre><code>class ControllerEventManager(controllerId: Int,\n                             processor: ControllerEventProcessor,\n                             time: Time,\n                             rateAndTimeMetrics: Map[ControllerState, KafkaTimer],\n                             eventQueueTimeTimeoutMs: Long = 300000) extends KafkaMetricsGroup {\n  import ControllerEventManager._\n\n  @volatile private var _state: ControllerState = ControllerState.Idle\n  private val putLock = new ReentrantLock()\n  private val queue = new LinkedBlockingQueue[QueuedEvent]\n  // Visible for test\n  private[controller] var thread = new ControllerEventThread(ControllerEventThreadName)\n\n  private val eventQueueTimeHist = newHistogram(EventQueueTimeMetricName)\n\n  newGauge(EventQueueSizeMetricName, () =&gt; queue.size)\n\n  def state: ControllerState = _state\n\n  def start(): Unit = thread.start()\n\n  def close(): Unit = {\n    try {\n      thread.initiateShutdown()\n      clearAndPut(ShutdownEventThread)\n      thread.awaitShutdown()\n    } finally {\n      removeMetric(EventQueueTimeMetricName)\n      removeMetric(EventQueueSizeMetricName)\n    }\n  }\n\n  def put(event: ControllerEvent): QueuedEvent = inLock(putLock) {\n    val queuedEvent = new QueuedEvent(event, time.milliseconds())\n    queue.put(queuedEvent)\n    queuedEvent\n  }\n\n  def clearAndPut(event: ControllerEvent): QueuedEvent = inLock(putLock){\n    val preemptedEvents = new ArrayList[QueuedEvent]()\n    queue.drainTo(preemptedEvents)\n    preemptedEvents.forEach(_.preempt(processor))\n    put(event)\n  }\n\n  def isEmpty: Boolean = queue.isEmpty\n\n  class ControllerEventThread(name: String) extends ShutdownableThread(name = name, isInterruptible = false) {\n    logIdent = s\"[ControllerEventThread controllerId=$controllerId] \"\n\n    override def doWork(): Unit = {\n      val dequeued = pollFromEventQueue()\n      dequeued.event match {\n        case ShutdownEventThread =&gt; // The shutting down of the thread has been initiated at this point. Ignore this event.\n        case controllerEvent =&gt;\n          _state = controllerEvent.state\n\n          eventQueueTimeHist.update(time.milliseconds() - dequeued.enqueueTimeMs)\n\n          try {\n            def process(): Unit = dequeued.process(processor)\n\n            rateAndTimeMetrics.get(state) match {\n              case Some(timer) =&gt; timer.time { process() }\n              case None =&gt; process()\n            }\n          } catch {\n            case e: Throwable =&gt; error(s\"Uncaught error processing event $controllerEvent\", e)\n          }\n\n          _state = ControllerState.Idle\n      }\n    }\n  }\n\n  private def pollFromEventQueue(): QueuedEvent = {\n    val count = eventQueueTimeHist.count()\n    if (count != 0) {\n      val event  = queue.poll(eventQueueTimeTimeoutMs, TimeUnit.MILLISECONDS)\n      if (event == null) {\n        eventQueueTimeHist.clear()\n        queue.take()\n      } else {\n        event\n      }\n    } else {\n      queue.take()\n    }\n  }\n\n}\n</code></pre>"},{"location":"6-src/core/kafka/controller/ControllerEventManager/ControllerEventProcessor/","title":"ControllerEventProcessor","text":"<p>\u5904\u7406 <code>ControllerEvent</code></p> <pre><code>trait ControllerEventProcessor {\n  def process(event: ControllerEvent): Unit\n  def preempt(event: ControllerEvent): Unit\n}\n</code></pre>"},{"location":"6-src/core/kafka/controller/ControllerEventManager/ControllerEventThread/","title":"ControllerEventThread","text":"<p>\u62c9\u53d6 <code>ControllerEventManager</code> \u4e2d\u7684\u4e8b\u4ef6\uff0c\u8fdb\u884c\u5904\u7406</p> <pre><code>class ControllerEventThread(name: String) extends ShutdownableThread(name = name, isInterruptible = false) {\n  logIdent = s\"[ControllerEventThread controllerId=$controllerId] \"\n\n  override def doWork(): Unit = {\n    val dequeued = pollFromEventQueue()\n    dequeued.event match {\n      case ShutdownEventThread =&gt; // The shutting down of the thread has been initiated at this point. Ignore this event.\n      case controllerEvent =&gt;\n        _state = controllerEvent.state\n\n        eventQueueTimeHist.update(time.milliseconds() - dequeued.enqueueTimeMs)\n\n        try {\n          def process(): Unit = dequeued.process(processor)\n\n          rateAndTimeMetrics.get(state) match {\n            case Some(timer) =&gt; timer.time { process() }\n            case None =&gt; process()\n          }\n        } catch {\n          case e: Throwable =&gt; error(s\"Uncaught error processing event $controllerEvent\", e)\n        }\n\n        _state = ControllerState.Idle\n    }\n  }\n}\n</code></pre>"},{"location":"6-src/core/kafka/controller/ControllerEventManager/QueueEvent/","title":"QueueEvent","text":"<pre><code>class QueuedEvent(val event: ControllerEvent,\n                  val enqueueTimeMs: Long) {\n  val processingStarted = new CountDownLatch(1)\n  val spent = new AtomicBoolean(false)\n\n  def process(processor: ControllerEventProcessor): Unit = {\n    if (spent.getAndSet(true))\n      return\n    processingStarted.countDown()\n    processor.process(event)\n  }\n\n  def preempt(processor: ControllerEventProcessor): Unit = {\n    if (spent.getAndSet(true))\n      return\n    processor.preempt(event)\n  }\n\n  def awaitProcessing(): Unit = {\n    processingStarted.await()\n  }\n\n  override def toString: String = {\n    s\"QueuedEvent(event=$event, enqueueTimeMs=$enqueueTimeMs)\"\n  }\n}\n</code></pre>"},{"location":"6-src/core/kafka/controller/KafkaController/","title":"KafkaController","text":"<p><code>KafkaController</code> \u5b9e\u73b0 <code>ControllerEventProcessor</code> \u7684 <code>process()</code> \u4e0e <code>preempt()</code> \u65b9\u6cd5\u3002</p>"},{"location":"6-src/core/kafka/controller/KafkaController/ControllerEvent/","title":"ControllerEvent","text":"<pre><code>sealed trait ControllerEvent {\n  def state: ControllerState\n  // preempt() is not executed by `ControllerEventThread` but by the main thread.\n  def preempt(): Unit\n}\n\ncase object ControllerChange extends ControllerEvent {\n  override def state: ControllerState = ControllerState.ControllerChange\n  override def preempt(): Unit = {}\n}\n\ncase object Reelect extends ControllerEvent {\n  override def state: ControllerState = ControllerState.ControllerChange\n  override def preempt(): Unit = {}\n}\n\ncase object RegisterBrokerAndReelect extends ControllerEvent {\n  override def state: ControllerState = ControllerState.ControllerChange\n  override def preempt(): Unit = {}\n}\n\ncase object Expire extends ControllerEvent {\n  override def state: ControllerState = ControllerState.ControllerChange\n  override def preempt(): Unit = {}\n}\n\ncase object ShutdownEventThread extends ControllerEvent {\n  override def state: ControllerState = ControllerState.ControllerShutdown\n  override def preempt(): Unit = {}\n}\n\ncase object AutoPreferredReplicaLeaderElection extends ControllerEvent {\n  override def state: ControllerState = ControllerState.AutoLeaderBalance\n  override def preempt(): Unit = {}\n}\n\ncase object UncleanLeaderElectionEnable extends ControllerEvent {\n  override def state: ControllerState = ControllerState.UncleanLeaderElectionEnable\n  override def preempt(): Unit = {}\n}\n\ncase class TopicUncleanLeaderElectionEnable(topic: String) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.TopicUncleanLeaderElectionEnable\n  override def preempt(): Unit = {}\n}\n\ncase class ControlledShutdown(id: Int, brokerEpoch: Long, controlledShutdownCallback: Try[Set[TopicPartition]] =&gt; Unit) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.ControlledShutdown\n  override def preempt(): Unit = controlledShutdownCallback(Failure(new ControllerMovedException(\"Controller moved to another broker\")))\n}\n\ncase class LeaderAndIsrResponseReceived(leaderAndIsrResponse: LeaderAndIsrResponse, brokerId: Int) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.LeaderAndIsrResponseReceived\n  override def preempt(): Unit = {}\n}\n\ncase class UpdateMetadataResponseReceived(updateMetadataResponse: UpdateMetadataResponse, brokerId: Int) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.UpdateMetadataResponseReceived\n  override def preempt(): Unit = {}\n}\n\ncase class TopicDeletionStopReplicaResponseReceived(replicaId: Int,\n                                                    requestError: Errors,\n                                                    partitionErrors: Map[TopicPartition, Errors]) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.TopicDeletion\n  override def preempt(): Unit = {}\n}\n\ncase object Startup extends ControllerEvent {\n  override def state: ControllerState = ControllerState.ControllerChange\n  override def preempt(): Unit = {}\n}\n\ncase object BrokerChange extends ControllerEvent {\n  override def state: ControllerState = ControllerState.BrokerChange\n  override def preempt(): Unit = {}\n}\n\ncase class BrokerModifications(brokerId: Int) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.BrokerChange\n  override def preempt(): Unit = {}\n}\n\ncase object TopicChange extends ControllerEvent {\n  override def state: ControllerState = ControllerState.TopicChange\n  override def preempt(): Unit = {}\n}\n\ncase object LogDirEventNotification extends ControllerEvent {\n  override def state: ControllerState = ControllerState.LogDirChange\n  override def preempt(): Unit = {}\n}\n\ncase class PartitionModifications(topic: String) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.TopicChange\n  override def preempt(): Unit = {}\n}\n\ncase object TopicDeletion extends ControllerEvent {\n  override def state: ControllerState = ControllerState.TopicDeletion\n  override def preempt(): Unit = {}\n}\n\ncase object ZkPartitionReassignment extends ControllerEvent {\n  override def state: ControllerState = ControllerState.AlterPartitionReassignment\n  override def preempt(): Unit = {}\n}\n\ncase class ApiPartitionReassignment(reassignments: Map[TopicPartition, Option[Seq[Int]]],\n                                    callback: AlterReassignmentsCallback) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.AlterPartitionReassignment\n  override def preempt(): Unit = callback(Right(new ApiError(Errors.NOT_CONTROLLER)))\n}\n\ncase class PartitionReassignmentIsrChange(partition: TopicPartition) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.AlterPartitionReassignment\n  override def preempt(): Unit = {}\n}\n\ncase object IsrChangeNotification extends ControllerEvent {\n  override def state: ControllerState = ControllerState.IsrChange\n  override def preempt(): Unit = {}\n}\n\ncase class AlterIsrReceived(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n                            callback: AlterIsrCallback) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.IsrChange\n  override def preempt(): Unit = {}\n}\n\ncase class ReplicaLeaderElection(\n  partitionsFromAdminClientOpt: Option[Set[TopicPartition]],\n  electionType: ElectionType,\n  electionTrigger: ElectionTrigger,\n  callback: ElectLeadersCallback = _ =&gt; {}\n) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.ManualLeaderBalance\n\n  override def preempt(): Unit = callback(\n    partitionsFromAdminClientOpt.fold(Map.empty[TopicPartition, Either[ApiError, Int]]) { partitions =&gt;\n      partitions.iterator.map(partition =&gt; partition -&gt; Left(new ApiError(Errors.NOT_CONTROLLER, null))).toMap\n    }\n  )\n}\n\n/**\n  * @param partitionsOpt - an Optional set of partitions. If not present, all reassigning partitions are to be listed\n  */\ncase class ListPartitionReassignments(partitionsOpt: Option[Set[TopicPartition]],\n                                      callback: ListReassignmentsCallback) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.ListPartitionReassignment\n  override def preempt(): Unit = callback(Right(new ApiError(Errors.NOT_CONTROLLER, null)))\n}\n\ncase class UpdateFeatures(request: UpdateFeaturesRequest,\n                          callback: UpdateFeaturesCallback) extends ControllerEvent {\n  override def state: ControllerState = ControllerState.UpdateFeatures\n  override def preempt(): Unit = {}\n}\n\n\n// Used only in test cases\nabstract class MockEvent(val state: ControllerState) extends ControllerEvent {\n  def process(): Unit\n  def preempt(): Unit\n}\n</code></pre>"},{"location":"6-src/core/kafka/network/RequestChannel/","title":"RequestChannel","text":"<p><code>RequestChannel</code> \u8d1f\u8d23\u5c06\u8bf7\u6c42\u4ece\u7f51\u7edc\u5c42\u8f6c\u63a5\u5230\u4e1a\u52a1\u5c42\uff0c\u4ee5\u53ca\u5c06\u4e1a\u52a1\u5c42\u7684\u5904\u7406\u7ed3\u679c\u4ea4\u4ed8\u7ed9\u7f51\u7edc\u5c42\u8fdb\u800c\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002</p> <p>\u6bcf\u4e00\u4e2a <code>SocketServer</code> \u53ea\u6709\u4e00\u4e2a <code>RequestChannel</code> \u5bf9\u8c61\uff0c\u5728 <code>SocketServer</code> \u4e2d\u6784\u9020\u3002</p> <p><code>RequestChannel</code> \u6784\u9020\u65b9\u6cd5\u4e2d\u521d\u59cb\u5316\u4e86 <code>requestQueue</code>\uff0c\u7528\u6765\u5b58\u653e\u7f51\u7edc\u5c42\u63a5\u6536\u5230\u7684\u8bf7\u6c42\uff0c\u8fd9\u4e9b\u8bf7\u6c42\u5373\u5c06\u4ea4\u4ed8\u7ed9\u4e1a\u52a1\u5c42\u8fdb\u884c\u5904\u7406\uff08<code>KafkaRequestHandler</code> \u8fdb\u884c\u5904\u7406\uff09\u3002</p> <p>\u540c\u65f6\uff0c\u521d\u59cb\u5316\u4e86 <code>processors</code>\uff0c\u5728 <code>SocketServer</code> \u521b\u5efa <code>Processor</code>  \u65f6\u4f1a\u5c06 <code>Processor</code> \u540c\u6b65\u8bb0\u5f55\u5230\u8fd9\u91cc\uff0c\u5f53 <code>RequestChannel</code> \u8c03\u7528 <code>sendResponse()</code> \u65f6\u4f1a\u4ece\u8fd9\u91cc\u53d6 response \u5bf9\u5e94\u7684 <code>Processor</code>  \u5e76\u5c06 response \u8fd4\u56de\u7ed9\u5bf9\u5e94\u7684 <code>Processor</code>\uff0c\u8fd9\u4e9b response \u5373\u5c06\u4ea4\u4ed8\u7ed9\u7f51\u7edc\u5c42\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002</p>"},{"location":"6-src/core/kafka/network/RequestChannel/#_1","title":"\u5c5e\u6027","text":"<pre><code>private val requestQueue = new ArrayBlockingQueue[BaseRequest](queueSize)\n// Processor \u8bb0\u5f55\nprivate val processors = new ConcurrentHashMap[Int, Processor]()\n</code></pre>"},{"location":"6-src/core/kafka/network/RequestChannel/#_2","title":"\u65b9\u6cd5","text":""},{"location":"6-src/core/kafka/network/RequestChannel/#addprocesor","title":"addProcesor","text":""},{"location":"6-src/core/kafka/network/RequestChannel/#removeprocessor","title":"removeProcessor","text":""},{"location":"6-src/core/kafka/network/RequestChannel/#sendrequest","title":"sendRequest","text":"<pre><code>/** Send a request to be handled, potentially blocking until there is room in the queue for the request */\ndef sendRequest(request: RequestChannel.Request): Unit = {\n  requestQueue.put(request)\n}\n</code></pre>"},{"location":"6-src/core/kafka/network/RequestChannel/#sendresponse","title":"sendResponse","text":"<pre><code>/** Send a response back to the socket server to be sent over the network */\ndef sendResponse(response: RequestChannel.Response): Unit = {\n\n  if (isTraceEnabled) {\n    val requestHeader = response.request.header\n    val message = response match {\n      case sendResponse: SendResponse =&gt;\n        s\"Sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} of ${sendResponse.responseSend.size} bytes.\"\n      case _: NoOpResponse =&gt;\n        s\"Not sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} as it's not required.\"\n      case _: CloseConnectionResponse =&gt;\n        s\"Closing connection for client ${requestHeader.clientId} due to error during ${requestHeader.apiKey}.\"\n      case _: StartThrottlingResponse =&gt;\n        s\"Notifying channel throttling has started for client ${requestHeader.clientId} for ${requestHeader.apiKey}\"\n      case _: EndThrottlingResponse =&gt;\n        s\"Notifying channel throttling has ended for client ${requestHeader.clientId} for ${requestHeader.apiKey}\"\n    }\n    trace(message)\n  }\n\n  response match {\n    // We should only send one of the following per request\n    case _: SendResponse | _: NoOpResponse | _: CloseConnectionResponse =&gt;\n      val request = response.request\n      val timeNanos = time.nanoseconds()\n      request.responseCompleteTimeNanos = timeNanos\n      if (request.apiLocalCompleteTimeNanos == -1L)\n        request.apiLocalCompleteTimeNanos = timeNanos\n    // For a given request, these may happen in addition to one in the previous section, skip updating the metrics\n    case _: StartThrottlingResponse | _: EndThrottlingResponse =&gt; ()\n  }\n\n  val processor = processors.get(response.processor)\n  // The processor may be null if it was shutdown. In this case, the connections\n  // are closed, so the response is dropped.\n  if (processor != null) {\n    processor.enqueueResponse(response)\n  }\n}\n</code></pre>"},{"location":"6-src/core/kafka/network/RequestChannel/#receiverequest","title":"receiveRequest","text":"<pre><code>/** Get the next request or block until specified time has elapsed */\ndef receiveRequest(timeout: Long): RequestChannel.BaseRequest =\n  requestQueue.poll(timeout, TimeUnit.MILLISECONDS)\n\n/** Get the next request or block until there is one */\ndef receiveRequest(): RequestChannel.BaseRequest =\n  requestQueue.take()\n</code></pre>"},{"location":"6-src/core/kafka/network/SocketServer/","title":"SocketServer","text":"<p>\u652f\u6301 2 \u79cd\u7c7b\u578b\u7684\u8bf7\u6c42\uff1a</p> <ol> <li>\u6570\u636e\uff1a<ul> <li>\u5904\u7406\u6765\u81ea client \u6216\u8005\u540c\u96c6\u7fa4\u5176\u4ed6 broker \u7684\u8bf7\u6c42</li> <li>\u7ebf\u7a0b\u6a21\u578b\u662f\uff1a<ul> <li>\u53ef\u4ee5\u914d\u7f6e\u591a\u4e2a listener\uff0c\u6bcf\u4e2a listener \u5bf9\u5e94 1 \u4e2a Acceptor \u7ebf\u7a0b\uff0c\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42</li> <li>Acceptor \u6709 N \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea sockets \u7684\u8bf7\u6c42</li> <li>M \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b</li> </ul> </li> </ul> </li> <li>\u63a7\u5236\uff1a<ul> <li>\u5904\u7406\u6765\u81ea controller \u7684\u8bf7\u6c42\u3002\u8fd9\u662f\u53ef\u9009\u7684\u5e76\u4e14\u901a\u8fc7\u6307\u5b9a <code>control.plane.listener.name</code> \u8fdb\u884c\u914d\u7f6e\u3002\u5982\u679c\u6ca1\u6709\u914d\u7f6e\uff0ccontroller \u7684\u8bf7\u6c42\u4e5f\u4f1a\u50cf\u6570\u636e\u8bf7\u6c42\u4e00\u6837\u901a\u8fc7 listener \u5904\u7406</li> <li>\u7ebf\u7a0b\u6a21\u578b\u662f\uff1a<ul> <li>1 \u4e2a Acceptor \u7ebf\u7a0b\u5904\u7406\u65b0\u7684\u8fde\u63a5\u8bf7\u6c42</li> <li>Acceptor \u6709 1 \u4e2a Processor \u7ebf\u7a0b\uff0cProcessor \u6709\u81ea\u5df1\u7684 selector \u7528\u6765\u8bfb\u6765\u81ea socket \u7684\u8bf7\u6c42</li> <li>1 \u4e2a Handler \u7ebf\u7a0b\u5904\u7406\u8bf7\u6c42\u5e76\u4ea7\u751f\u54cd\u5e94\u53d1\u56de\u7ed9 Processor \u7ebf\u7a0b</li> </ul> </li> </ul> </li> </ol>"},{"location":"6-src/core/kafka/network/SocketServer/#_1","title":"\u5c5e\u6027","text":"<pre><code>private val maxQueuedRequests = config.queuedMaxRequests\n\nprivate val logContext = new LogContext(s\"[SocketServer brokerId=${config.brokerId}] \")\nthis.logIdent = logContext.logPrefix\n\nprivate val memoryPoolSensor = metrics.sensor(\"MemoryPoolUtilization\")\nprivate val memoryPoolDepletedPercentMetricName = metrics.metricName(\"MemoryPoolAvgDepletedPercent\", MetricsGroup)\nprivate val memoryPoolDepletedTimeMetricName = metrics.metricName(\"MemoryPoolDepletedTimeTotal\", MetricsGroup)\nmemoryPoolSensor.add(new Meter(TimeUnit.MILLISECONDS, memoryPoolDepletedPercentMetricName, memoryPoolDepletedTimeMetricName))\nprivate val memoryPool = if (config.queuedMaxBytes &gt; 0) new SimpleMemoryPool(config.queuedMaxBytes, config.socketRequestMaxBytes, false, memoryPoolSensor) else MemoryPool.NONE\n// data-plane\n// Processor \u8bb0\u5f55\nprivate val dataPlaneProcessors = new ConcurrentHashMap[Int, Processor]()\nprivate[network] val dataPlaneAcceptors = new ConcurrentHashMap[EndPoint, Acceptor]()\nval dataPlaneRequestChannel = new RequestChannel(maxQueuedRequests, DataPlaneMetricPrefix, time)\n// control-plane\nprivate var controlPlaneProcessorOpt : Option[Processor] = None\nprivate[network] var controlPlaneAcceptorOpt : Option[Acceptor] = None\nval controlPlaneRequestChannelOpt: Option[RequestChannel] = config.controlPlaneListenerName.map(_ =&gt;\n  new RequestChannel(20, ControlPlaneMetricPrefix, time))\n\nprivate var nextProcessorId = 0\nprivate var connectionQuotas: ConnectionQuotas = _\nprivate var startedProcessingRequests = false\nprivate var stoppedProcessingRequests = false\n</code></pre>"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/","title":"Acceptor","text":""},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#_1","title":"\u5c5e\u6027","text":"<pre><code>private val nioSelector = NSelector.open()\nval serverChannel = openServerSocket(endPoint.host, endPoint.port)\n// \u5173\u8054\u7684 Processor \u8bb0\u5f55\nprivate val processors = new ArrayBuffer[Processor]()\n</code></pre>"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#_2","title":"\u65b9\u6cd5","text":""},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#run","title":"run","text":"<p>\u5728 run \u4e2d\u4e0d\u65ad\u5c1d\u8bd5\u5904\u7406\u8fde\u63a5\uff1a</p> <pre><code>/**\n * Accept loop that checks for new connection attempts\n */\ndef run(): Unit = {\n  serverChannel.register(nioSelector, SelectionKey.OP_ACCEPT)\n  startupComplete()\n  try {\n    while (isRunning) {\n      try {\n        acceptNewConnections()\n        closeThrottledConnections()\n      }\n      catch {\n        // We catch all the throwables to prevent the acceptor thread from exiting on exceptions due\n        // to a select operation on a specific channel or a bad request. We don't want\n        // the broker to stop responding to requests from other clients in these scenarios.\n        case e: ControlThrowable =&gt; throw e\n        case e: Throwable =&gt; error(\"Error occurred\", e)\n      }\n    }\n  } finally {\n    debug(\"Closing server socket, selector, and any throttled sockets.\")\n    CoreUtils.swallow(serverChannel.close(), this, Level.ERROR)\n    CoreUtils.swallow(nioSelector.close(), this, Level.ERROR)\n    throttledSockets.foreach(throttledSocket =&gt; closeSocket(throttledSocket.socket))\n    throttledSockets.clear()\n    shutdownComplete()\n  }\n}\n</code></pre>"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#acceptnewconnections","title":"acceptNewConnections","text":"<p>\u5728 <code>acceptNewConnections</code> \u4e2d\u901a\u8fc7 nio select \u7b5b\u9009\u53ef\u8fde\u63a5\u7684 channel\uff0caccept \u4e4b\uff0c\u5bf9\u6bcf\u4e2a accepted channel\uff0c \u5c1d\u8bd5\u5728 <code>processors</code> \u4e2d\u9009\u62e9\u4e00\u4e2a <code>processor</code> \u5e76\u5206\u914d\u4e4b</p> <pre><code>/**\n * Listen for new connections and assign accepted connections to processors using round-robin.\n */\nprivate def acceptNewConnections(): Unit = {\n  val ready = nioSelector.select(500)\n  if (ready &gt; 0) {\n    val keys = nioSelector.selectedKeys()\n    val iter = keys.iterator()\n    while (iter.hasNext &amp;&amp; isRunning) {\n      try {\n        val key = iter.next\n        iter.remove()\n\n        if (key.isAcceptable) {\n          accept(key).foreach { socketChannel =&gt;\n            // Assign the channel to the next processor (using round-robin) to which the\n            // channel can be added without blocking. If newConnections queue is full on\n            // all processors, block until the last one is able to accept a connection.\n            var retriesLeft = synchronized(processors.length)\n            var processor: Processor = null\n            do {\n              retriesLeft -= 1\n              processor = synchronized {\n                // adjust the index (if necessary) and retrieve the processor atomically for\n                // correct behaviour in case the number of processors is reduced dynamically\n                currentProcessorIndex = currentProcessorIndex % processors.length\n                processors(currentProcessorIndex)\n              }\n              currentProcessorIndex += 1\n            } while (!assignNewConnection(socketChannel, processor, retriesLeft == 0))\n          }\n        } else\n          throw new IllegalStateException(\"Unrecognized key state for acceptor thread.\")\n      } catch {\n        case e: Throwable =&gt; error(\"Error while accepting connection\", e)\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"6-src/core/kafka/network/SocketServer/Acceptor/#assignnewconnection","title":"assignNewConnection","text":"<p><code>assignNewConnection</code> \u901a\u8fc7\u8c03\u7528 <code>processor.accept</code> \u5c06 socketChannel \u52a0\u5165\u5230 processor \u81ea\u8eab\u7ef4\u62a4\u7684 <code>newConnections</code> \u961f\u5217\u4e2d</p> <pre><code>private def assignNewConnection(socketChannel: SocketChannel, processor: Processor, mayBlock: Boolean): Boolean = {\n  if (processor.accept(socketChannel, mayBlock, blockedPercentMeter)) {\n    debug(s\"Accepted connection from ${socketChannel.socket.getRemoteSocketAddress} on\" +\n      s\" ${socketChannel.socket.getLocalSocketAddress} and assigned it to processor ${processor.id},\" +\n      s\" sendBufferSize [actual|requested]: [${socketChannel.socket.getSendBufferSize}|$sendBufferSize]\" +\n      s\" recvBufferSize [actual|requested]: [${socketChannel.socket.getReceiveBufferSize}|$recvBufferSize]\")\n    true\n  } else\n    false\n}\n</code></pre>"},{"location":"6-src/core/kafka/network/SocketServer/Processor/","title":"Processor","text":""},{"location":"6-src/core/kafka/network/SocketServer/Processor/#_1","title":"\u5c5e\u6027","text":"<pre><code>// Acceptor \u5904\u7406\u5f97\u5230\u7684\u5df2\u8fde\u63a5\u8bf7\u6c42\u4f1a\u8bb0\u5f55\u5230\u8fd9\u91cc\nprivate val newConnections = new ArrayBlockingQueue[SocketChannel](connectionQueueSize)\nprivate val inflightResponses = mutable.Map[String, RequestChannel.Response]()\n// RequestChannel \u8c03\u7528 sendResponse() \u4f1a\u628a\u54cd\u5e94\u8bb0\u5f55\u5728\u8fd9\u91cc\nprivate val responseQueue = new LinkedBlockingDeque[RequestChannel.Response]()\n</code></pre> <p>Acceptor \u901a\u8fc7 <code>processor.accept</code> \u5c06 socketChannel \u52a0\u5165\u5230 processor \u7684 <code>newConnections</code> \u4e2d</p> <p>Processor \u6570\u91cf\u7531 <code>num.networker.threads</code> \u51b3\u5b9a</p> <p>processor \u7684 <code>run</code> \u5904\u7406 socketChannel</p> <pre><code>override def run(): Unit = {\n  startupComplete()\n  try {\n    while (isRunning) {\n      try {\n        // setup any new connections that have been queued up\n        // \u8fd9\u91cc\u4ece newConnections \u4e2d\u53d6\u51fa channel\uff0c\u5e76\u6ce8\u518c\u5230 selector\n        configureNewConnections()\n        // register any new responses for writing\n        // \u4ece responseQueue \u4e2d\u53d6\u51fa response\uff0c\u8fdb\u884c \uff1f\u5904\u7406\n        processNewResponses()\n        // select keys, \u8fdb\u884c\u5b9e\u9645\u7684\u8bfb\u5199\u4e8b\u4ef6\u5904\u7406\n        poll()\n        // \u904d\u5386\u6bcf\u4e2a\u5df2\u7ecf\u5b8c\u6210\u63a5\u6536\u7684 receive\uff0c\u901a\u8fc7 requestChannel.sendRequest(req)\uff0c\u5c06\u8bf7\u6c42\u653e\u5230 RequestChannel \u7684 requestQueue \u4e2d\uff0c\u4f9b\u540e\u7eed\u5904\u7406\n        processCompletedReceives()\n        // \u904d\u5386\u6bcf\u4e2a\u5df2\u7ecf\u5b8c\u6210\u53d1\u9001\u7684 send\n        processCompletedSends()\n        processDisconnected()\n        closeExcessConnections()\n      } catch {\n        // We catch all the throwables here to prevent the processor thread from exiting. We do this because\n        // letting a processor exit might cause a bigger impact on the broker. This behavior might need to be\n        // reviewed if we see an exception that needs the entire broker to stop. Usually the exceptions thrown would\n        // be either associated with a specific socket channel or a bad request. These exceptions are caught and\n        // processed by the individual methods above which close the failing channel and continue processing other\n        // channels. So this catch block should only ever see ControlThrowables.\n        case e: Throwable =&gt; processException(\"Processor got uncaught exception.\", e)\n      }\n    }\n  } finally {\n    debug(s\"Closing selector - processor $id\")\n    CoreUtils.swallow(closeAll(), this, Level.ERROR)\n    shutdownComplete()\n  }\n}\n</code></pre>"},{"location":"6-src/core/kafka/server/KafkaApis/","title":"KafkaApis","text":"<p>\u5728 <code>handle</code> \u65b9\u6cd5\u4e2d\u5904\u7406 kafka \u8bf7\u6c42</p> <p>\u4ee5 <code>handleProduceRequest</code> \u65b9\u6cd5\u4e3a\u4f8b\uff1a</p> <pre><code>      // call the replica manager to append messages to the replicas\n      replicaManager.appendRecords(\n        timeout = produceRequest.timeout.toLong,\n        requiredAcks = produceRequest.acks,\n        internalTopicsAllowed = internalTopicsAllowed,\n        origin = AppendOrigin.Client,\n        entriesPerPartition = authorizedRequestInfo,\n        responseCallback = sendResponseCallback,\n        recordConversionStatsCallback = processingStatsCallback)\n</code></pre> <ol> <li>\u5c06 response \u901a\u8fc7 <code>RequestChannel</code> \u7684 <code>sendResponse</code> \u5199\u56de\u5230\u5bf9\u5e94\u7684 <code>Processor</code> \u7684 <code>responseQueue</code>\uff0c<code>Processor</code>  \u7684 <code>processNewResponses</code> \u4f1a\u4ece <code>responseQueue</code> \u4e2d\u53d6\u56de response</li> <li>replicaManager</li> </ol>"},{"location":"6-src/core/kafka/server/KafkaServer/","title":"KafkaServer","text":""},{"location":"6-src/core/kafka/server/KafkaServer/#_1","title":"\u65b9\u6cd5","text":""},{"location":"6-src/core/kafka/server/KafkaServer/#startup","title":"startup","text":"<pre><code>/**\n * Start up API for bringing up a single instance of the Kafka server.\n * Instantiates the LogManager, the SocketServer and the request handlers - KafkaRequestHandlers\n */\noverride def startup(): Unit = {\n  try {\n    info(\"starting\")\n\n    if (isShuttingDown.get)\n      throw new IllegalStateException(\"Kafka server is still shutting down, cannot re-start!\")\n\n    if (startupComplete.get)\n      return\n\n    val canStartup = isStartingUp.compareAndSet(false, true)\n    if (canStartup) {\n      brokerState.set(BrokerState.STARTING)\n\n      /* setup zookeeper */\n      initZkClient(time)\n      configRepository = new ZkConfigRepository(new AdminZkClient(zkClient))\n\n      /* initialize features */\n      _featureChangeListener = new FinalizedFeatureChangeListener(featureCache, _zkClient)\n      if (config.isFeatureVersioningSupported) {\n        _featureChangeListener.initOrThrow(config.zkConnectionTimeoutMs)\n      }\n\n      /* Get or create cluster_id */\n      _clusterId = getOrGenerateClusterId(zkClient)\n      info(s\"Cluster ID = ${clusterId}\")\n\n      /* load metadata */\n      val (preloadedBrokerMetadataCheckpoint, initialOfflineDirs) =\n        BrokerMetadataCheckpoint.getBrokerMetadataAndOfflineDirs(config.logDirs, ignoreMissing = true)\n\n      if (preloadedBrokerMetadataCheckpoint.version != 0) {\n        throw new RuntimeException(s\"Found unexpected version in loaded `meta.properties`: \" +\n          s\"$preloadedBrokerMetadataCheckpoint. Zk-based brokers only support version 0 \" +\n          \"(which is implicit when the `version` field is missing).\")\n      }\n\n      /* check cluster id */\n      if (preloadedBrokerMetadataCheckpoint.clusterId.isDefined &amp;&amp; preloadedBrokerMetadataCheckpoint.clusterId.get != clusterId)\n        throw new InconsistentClusterIdException(\n          s\"The Cluster ID ${clusterId} doesn't match stored clusterId ${preloadedBrokerMetadataCheckpoint.clusterId} in meta.properties. \" +\n          s\"The broker is trying to join the wrong cluster. Configured zookeeper.connect may be wrong.\")\n\n      /* generate brokerId */\n      config.brokerId = getOrGenerateBrokerId(preloadedBrokerMetadataCheckpoint)\n      logContext = new LogContext(s\"[KafkaServer id=${config.brokerId}] \")\n      this.logIdent = logContext.logPrefix\n\n      // initialize dynamic broker configs from ZooKeeper. Any updates made after this will be\n      // applied after DynamicConfigManager starts.\n      config.dynamicConfig.initialize(zkClient)\n\n      /* start scheduler */\n      kafkaScheduler = new KafkaScheduler(config.backgroundThreads)\n      kafkaScheduler.startup()\n\n      /* create and configure metrics */\n      kafkaYammerMetrics = KafkaYammerMetrics.INSTANCE\n      kafkaYammerMetrics.configure(config.originals)\n      metrics = Server.initializeMetrics(config, time, clusterId)\n\n      /* register broker metrics */\n      _brokerTopicStats = new BrokerTopicStats\n\n      quotaManagers = QuotaFactory.instantiate(config, metrics, time, threadNamePrefix.getOrElse(\"\"))\n      KafkaBroker.notifyClusterListeners(clusterId, kafkaMetricsReporters ++ metrics.reporters.asScala)\n\n      logDirFailureChannel = new LogDirFailureChannel(config.logDirs.size)\n\n      /* start log manager */\n      logManager = LogManager(config, initialOfflineDirs,\n        new ZkConfigRepository(new AdminZkClient(zkClient)),\n        kafkaScheduler, time, brokerTopicStats, logDirFailureChannel, config.usesTopicId)\n      brokerState.set(BrokerState.RECOVERY)\n      logManager.startup(zkClient.getAllTopicsInCluster())\n\n      metadataCache = MetadataCache.zkMetadataCache(config.brokerId)\n      // Enable delegation token cache for all SCRAM mechanisms to simplify dynamic update.\n      // This keeps the cache up-to-date if new SCRAM mechanisms are enabled dynamically.\n      tokenCache = new DelegationTokenCache(ScramMechanism.mechanismNames)\n      credentialProvider = new CredentialProvider(ScramMechanism.mechanismNames, tokenCache)\n\n      if (enableForwarding) {\n        this.forwardingManager = Some(ForwardingManager(\n          config,\n          metadataCache,\n          time,\n          metrics,\n          threadNamePrefix\n        ))\n        forwardingManager.foreach(_.start())\n      }\n\n      val apiVersionManager = ApiVersionManager(\n        ListenerType.ZK_BROKER,\n        config,\n        forwardingManager,\n        brokerFeatures,\n        featureCache\n      )\n\n      // Create and start the socket server acceptor threads so that the bound port is known.\n      // Delay starting processors until the end of the initialization sequence to ensure\n      // that credentials have been loaded before processing authentications.\n      //\n      // Note that we allow the use of KRaft mode controller APIs when forwarding is enabled\n      // so that the Envelope request is exposed. This is only used in testing currently.\n      socketServer = new SocketServer(config, metrics, time, credentialProvider, apiVersionManager)\n      socketServer.startup(startProcessingRequests = false)\n\n      /* start replica manager */\n      alterIsrManager = if (config.interBrokerProtocolVersion.isAlterIsrSupported) {\n        AlterIsrManager(\n          config = config,\n          metadataCache = metadataCache,\n          scheduler = kafkaScheduler,\n          time = time,\n          metrics = metrics,\n          threadNamePrefix = threadNamePrefix,\n          brokerEpochSupplier = () =&gt; kafkaController.brokerEpoch,\n          config.brokerId\n        )\n      } else {\n        AlterIsrManager(kafkaScheduler, time, zkClient)\n      }\n      alterIsrManager.start()\n\n      replicaManager = createReplicaManager(isShuttingDown)\n      replicaManager.startup()\n\n      val brokerInfo = createBrokerInfo\n      val brokerEpoch = zkClient.registerBroker(brokerInfo)\n\n      // Now that the broker is successfully registered, checkpoint its metadata\n      checkpointBrokerMetadata(ZkMetaProperties(clusterId, config.brokerId))\n\n      /* start token manager */\n      tokenManager = new DelegationTokenManager(config, tokenCache, time , zkClient)\n      tokenManager.startup()\n\n      /* start kafka controller */\n      kafkaController = new KafkaController(config, zkClient, time, metrics, brokerInfo, brokerEpoch, tokenManager, brokerFeatures, featureCache, threadNamePrefix)\n      kafkaController.startup()\n\n      adminManager = new ZkAdminManager(config, metrics, metadataCache, zkClient)\n\n      /* start group coordinator */\n      // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue\n      groupCoordinator = GroupCoordinator(config, replicaManager, Time.SYSTEM, metrics)\n      groupCoordinator.startup(() =&gt; zkClient.getTopicPartitionCount(Topic.GROUP_METADATA_TOPIC_NAME).getOrElse(config.offsetsTopicPartitions))\n\n      /* start transaction coordinator, with a separate background thread scheduler for transaction expiration and log loading */\n      // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue\n      transactionCoordinator = TransactionCoordinator(config, replicaManager, new KafkaScheduler(threads = 1, threadNamePrefix = \"transaction-log-manager-\"),\n        () =&gt; new ProducerIdManager(config.brokerId, zkClient), metrics, metadataCache, Time.SYSTEM)\n      transactionCoordinator.startup(\n        () =&gt; zkClient.getTopicPartitionCount(Topic.TRANSACTION_STATE_TOPIC_NAME).getOrElse(config.transactionTopicPartitions))\n\n      /* start auto topic creation manager */\n      this.autoTopicCreationManager = AutoTopicCreationManager(\n        config,\n        metadataCache,\n        time,\n        metrics,\n        threadNamePrefix,\n        Some(adminManager),\n        Some(kafkaController),\n        groupCoordinator,\n        transactionCoordinator,\n        enableForwarding\n      )\n      autoTopicCreationManager.start()\n\n      /* Get the authorizer and initialize it if one is specified.*/\n      authorizer = config.authorizer\n      authorizer.foreach(_.configure(config.originals))\n      val authorizerFutures: Map[Endpoint, CompletableFuture[Void]] = authorizer match {\n        case Some(authZ) =&gt;\n          authZ.start(brokerInfo.broker.toServerInfo(clusterId, config)).asScala.map { case (ep, cs) =&gt;\n            ep -&gt; cs.toCompletableFuture\n          }\n        case None =&gt;\n          brokerInfo.broker.endPoints.map { ep =&gt;\n            ep.toJava -&gt; CompletableFuture.completedFuture[Void](null)\n          }.toMap\n      }\n\n      val fetchManager = new FetchManager(Time.SYSTEM,\n        new FetchSessionCache(config.maxIncrementalFetchSessionCacheSlots,\n          KafkaServer.MIN_INCREMENTAL_FETCH_SESSION_EVICTION_MS))\n\n      /* start processing requests */\n      val zkSupport = ZkSupport(adminManager, kafkaController, zkClient, forwardingManager, metadataCache)\n      dataPlaneRequestProcessor = new KafkaApis(socketServer.dataPlaneRequestChannel, zkSupport, replicaManager, groupCoordinator, transactionCoordinator,\n        autoTopicCreationManager, config.brokerId, config, configRepository, metadataCache, metrics, authorizer, quotaManagers,\n        fetchManager, brokerTopicStats, clusterId, time, tokenManager, apiVersionManager)\n\n      dataPlaneRequestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.dataPlaneRequestChannel, dataPlaneRequestProcessor, time,\n        config.numIoThreads, s\"${SocketServer.DataPlaneMetricPrefix}RequestHandlerAvgIdlePercent\", SocketServer.DataPlaneThreadPrefix)\n\n      socketServer.controlPlaneRequestChannelOpt.foreach { controlPlaneRequestChannel =&gt;\n        controlPlaneRequestProcessor = new KafkaApis(controlPlaneRequestChannel, zkSupport, replicaManager, groupCoordinator, transactionCoordinator,\n          autoTopicCreationManager, config.brokerId, config, configRepository, metadataCache, metrics, authorizer, quotaManagers,\n          fetchManager, brokerTopicStats, clusterId, time, tokenManager, apiVersionManager)\n\n        controlPlaneRequestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.controlPlaneRequestChannelOpt.get, controlPlaneRequestProcessor, time,\n          1, s\"${SocketServer.ControlPlaneMetricPrefix}RequestHandlerAvgIdlePercent\", SocketServer.ControlPlaneThreadPrefix)\n      }\n\n      Mx4jLoader.maybeLoad()\n\n      /* Add all reconfigurables for config change notification before starting config handlers */\n      config.dynamicConfig.addReconfigurables(this)\n\n      /* start dynamic config manager */\n      dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -&gt; new TopicConfigHandler(logManager, config, quotaManagers, kafkaController),\n                                                         ConfigType.Client -&gt; new ClientIdConfigHandler(quotaManagers),\n                                                         ConfigType.User -&gt; new UserConfigHandler(quotaManagers, credentialProvider),\n                                                         ConfigType.Broker -&gt; new BrokerConfigHandler(config, quotaManagers),\n                                                         ConfigType.Ip -&gt; new IpConfigHandler(socketServer.connectionQuotas))\n\n      // Create the config manager. start listening to notifications\n      dynamicConfigManager = new DynamicConfigManager(zkClient, dynamicConfigHandlers)\n      dynamicConfigManager.startup()\n\n      socketServer.startProcessingRequests(authorizerFutures)\n\n      brokerState.set(BrokerState.RUNNING)\n      shutdownLatch = new CountDownLatch(1)\n      startupComplete.set(true)\n      isStartingUp.set(false)\n      AppInfoParser.registerAppInfo(Server.MetricsPrefix, config.brokerId.toString, metrics, time.milliseconds())\n      info(\"started\")\n    }\n  }\n  catch {\n    case e: Throwable =&gt;\n      fatal(\"Fatal error during KafkaServer startup. Prepare to shutdown\", e)\n      isStartingUp.set(false)\n      shutdown()\n      throw e\n  }\n}\n</code></pre> <p>\u521b\u5efa <code>SocketServer</code> \u5e76\u8c03\u7528 <code>socketServer.startup()</code></p>"},{"location":"6-src/core/kafka/server/KafkaRequestHandler/","title":"KafkaRequestHandler","text":"<p>\u4ece <code>RequestChannel</code> \u7684 <code>requestQueue</code> \u4e2d\u53d6\u51fa\u8bf7\u6c42\uff0c\u5e76\u901a\u8fc7 <code>ApiRequestHandler.handle</code> \u8fdb\u884c\u5904\u7406\uff08\u8fd9\u91cc\u7684\u5b9e\u73b0\u5728 <code>KafkaApis</code> \u4e2d\uff09</p> <pre><code>def run(): Unit = {\n  while (!stopped) {\n    // We use a single meter for aggregate idle percentage for the thread pool.\n    // Since meter is calculated as total_recorded_value / time_window and\n    // time_window is independent of the number of threads, each recorded idle\n    // time should be discounted by # threads.\n    val startSelectTime = time.nanoseconds\n\n    val req = requestChannel.receiveRequest(300)\n    val endTime = time.nanoseconds\n    val idleTime = endTime - startSelectTime\n    aggregateIdleMeter.mark(idleTime / totalHandlerThreads.get)\n\n    req match {\n      case RequestChannel.ShutdownRequest =&gt;\n        debug(s\"Kafka request handler $id on broker $brokerId received shut down command\")\n        shutdownComplete.countDown()\n        return\n\n      case request: RequestChannel.Request =&gt;\n        try {\n          request.requestDequeueTimeNanos = endTime\n          trace(s\"Kafka request handler $id on broker $brokerId handling request $request\")\n          apis.handle(request)\n        } catch {\n          case e: FatalExitError =&gt;\n            shutdownComplete.countDown()\n            Exit.exit(e.statusCode)\n          case e: Throwable =&gt; error(\"Exception when handling request\", e)\n        } finally {\n          request.releaseBuffer()\n        }\n\n      case null =&gt; // continue\n    }\n  }\n  shutdownComplete.countDown()\n}\n</code></pre>"},{"location":"6-src/core/kafka/server/KafkaRequestHandler/KafkaRequestHandlerPool/","title":"KafkaRequestHandlerPool","text":"<p>\u521b\u5efa\u591a\u4e2a <code>KafkaRequestHandler</code> \uff08\u7531 <code>num.io.threads</code> \u914d\u7f6e\uff09</p>"},{"location":"7-other/1-topic-name-limit/","title":"7.1 topic \u540d\u89c4\u5219","text":""},{"location":"7-other/1-topic-name-limit/#topic","title":"topic","text":"<ol> <li>topic \u540d \u4e0d\u80fd\u4e3a\u7a7a\u5b57\u7b26\u4e32</li> <li>topic \u540d \u4e0d\u80fd\u4e3a <code>.</code> \u6216\u8005 <code>..</code></li> <li>topic \u540d \u4e0d\u80fd\u8d85\u8fc7 249 \u4e2a\u5b57\u7b26</li> <li>topic \u540d \u7531 <code>[a-zA-Z0-9._-]</code> \u8fd9\u4e9b\u5b57\u7b26\u7ec4\u6210</li> </ol> <pre><code>public static final String LEGAL_CHARS = \"[a-zA-Z0-9._-]\";\n\nprivate static final int MAX_NAME_LENGTH = 249;\n\npublic static void validate(String topic) {\n    if (topic.isEmpty())\n        throw new InvalidTopicException(\"Topic name is illegal, it can't be empty\");\n    if (topic.equals(\".\") || topic.equals(\"..\"))\n        throw new InvalidTopicException(\"Topic name cannot be \\\".\\\" or \\\"..\\\"\");\n    if (topic.length() &gt; MAX_NAME_LENGTH)\n        throw new InvalidTopicException(\"Topic name is illegal, it can't be longer than \" + MAX_NAME_LENGTH +\n                \" characters, topic name: \" + topic);\n    if (!containsValidPattern(topic))\n        throw new InvalidTopicException(\"Topic name \\\"\" + topic + \"\\\" is illegal, it contains a character other than \" +\n                \"ASCII alphanumerics, '.', '_' and '-'\");\n}\n\n/**\n * Valid characters for Kafka topics are the ASCII alphanumerics, '.', '_', and '-'\n */\nstatic boolean containsValidPattern(String topic) {\n    for (int i = 0; i &lt; topic.length(); ++i) {\n        char c = topic.charAt(i);\n\n        // We don't use Character.isLetterOrDigit(c) because it's slower\n        boolean validChar = (c &gt;= 'a' &amp;&amp; c &lt;= 'z') || (c &gt;= '0' &amp;&amp; c &lt;= '9') || (c &gt;= 'A' &amp;&amp; c &lt;= 'Z') || c == '.' ||\n                c == '_' || c == '-';\n        if (!validChar)\n            return false;\n    }\n    return true;\n}\n</code></pre>"},{"location":"7-other/1-topic-name-limit/#group-id","title":"group id","text":"<p>group id \u6ca1\u6709\u627e\u5230\u9650\u5236</p> <pre><code>private def isValidGroupId(groupId: String, api: ApiKeys): Boolean = {\n  api match {\n    case ApiKeys.OFFSET_COMMIT | ApiKeys.OFFSET_FETCH | ApiKeys.DESCRIBE_GROUPS | ApiKeys.DELETE_GROUPS =&gt;\n      // For backwards compatibility, we support the offset commit APIs for the empty groupId, and also\n      // in DescribeGroups and DeleteGroups so that users can view and delete state of all groups.\n      groupId != null\n    case _ =&gt;\n      // The remaining APIs are groups using Kafka for group coordination and must have a non-empty groupId\n      groupId != null &amp;&amp; !groupId.isEmpty\n  }\n}\n</code></pre>"},{"location":"7-other/2-kafka-page-cache/","title":"page cache","text":"<ol> <li>producer \u751f\u4ea7\u6d88\u606f\u65f6\uff0c\u4f1a\u4f7f\u7528 <code>pwrite()</code> \u7cfb\u7edf\u8c03\u7528(\u5bf9\u5e94 Java NIO <code>FileChannel.write()</code> API)\u6309\u504f\u79fb\u91cf\u5199\u5165\u6570\u636e\uff0c\u5e76\u4e14\u90fd\u4f1a\u5148\u5199\u5165 page cache \u91cc</li> <li>consumer \u6d88\u8d39\u6d88\u606f\u65f6\uff0c\u4f1a\u4f7f\u7528 <code>sendfile()</code> \u7cfb\u7edf\u8c03\u7528(\u5bf9\u5e94 Java NIO <code>FileChannel.transferTo()</code> API)\uff0c\u96f6\u62f7\u8d1d\u5730\u5c06\u6570\u636e\u4ece page cache \u4f20\u8f93\u5230 broker \u7684 socket buffer\uff0c\u518d\u901a\u8fc7\u7f51\u7edc\u4f20\u8f93</li> <li>follower \u540c\u6b65\u6d88\u606f\uff0c\u4e0e consumer \u540c\u7406</li> <li>page cache \u4e2d\u7684\u6570\u636e\u4f1a\u968f\u7740\u5185\u6838\u4e2d flusher \u7ebf\u7a0b\u7684\u8c03\u5ea6\u4ee5\u53ca\u5bf9 <code>sync()/fsync()</code> \u7684\u8c03\u7528\u5199\u56de\u5230\u78c1\u76d8\uff0c\u5c31\u7b97\u8fdb\u7a0b\u5d29\u6e83\uff0c\u4e5f\u4e0d\u7528\u62c5\u5fc3\u6570\u636e\u4e22\u5931\u3002</li> </ol> <p>\u53e6\u5916\uff0c\u5982\u679c consumer \u8981\u6d88\u8d39\u7684\u6d88\u606f\u4e0d\u5728 page cache \u91cc\uff0c\u624d\u4f1a\u53bb\u78c1\u76d8\u8bfb\u53d6\uff0c\u5e76\u4e14\u4f1a\u987a\u4fbf\u9884\u8bfb\u51fa\u4e00\u4e9b\u76f8\u90bb\u7684\u5757\u653e\u5165 page cache\uff0c\u4ee5\u65b9\u4fbf\u4e0b\u4e00\u6b21\u8bfb\u53d6\u3002</p> <p>\u7531\u6b64\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u91cd\u8981\u7684\u7ed3\u8bba\uff1a\u5982\u679c Kafka producer \u7684\u751f\u4ea7\u901f\u7387\u4e0e consumer \u7684\u6d88\u8d39\u901f\u7387\u76f8\u5dee\u4e0d\u5927\uff0c\u90a3\u4e48\u5c31\u80fd\u51e0\u4e4e\u53ea\u9760\u5bf9 broker page cache \u7684\u8bfb\u5199\u5b8c\u6210\u6574\u4e2a\u751f\u4ea7-\u6d88\u8d39\u8fc7\u7a0b\uff0c\u78c1\u76d8\u8bbf\u95ee\u975e\u5e38\u5c11\u3002\u5e76\u4e14 Kafka \u6301\u4e45\u5316\u6d88\u606f\u5230\u5404\u4e2a topic \u7684 partition \u6587\u4ef6\u65f6\uff0c\u662f\u53ea\u8ffd\u52a0\u7684\u987a\u5e8f\u5199\uff0c\u5145\u5206\u5229\u7528\u4e86\u78c1\u76d8\u987a\u5e8f\u8bbf\u95ee\u5feb\u7684\u7279\u6027\uff0c\u6548\u7387\u9ad8\u3002</p>"},{"location":"7-other/2-kafka-page-cache/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://cloud.tencent.com/developer/article/1488144</li> <li>https://www.cnblogs.com/xiaolincoding/p/13719610.html</li> </ul>"},{"location":"7-other/3-reassign-partition/","title":"kafka reassign partition","text":""},{"location":"7-other/3-reassign-partition/#zk","title":"zk \u7ba1\u7406\u65b9\u5f0f","text":"<p>\u76f4\u63a5\u5c06\u5206\u533a\u5206\u914d\u65b9\u6848\u5199\u5165\u5230 ZooKeeper \u7684 <code>/admin/reassign_partitions</code> \u8282\u70b9\uff0c\u7531 Controller \u76d1\u542c\u5230\u540e\u6267\u884c\u3002</p> <ul> <li>\u5199\u5165\u540e\uff0c\u65e0\u6cd5\u53d6\u6d88\u4efb\u52a1</li> <li> <p>\u5199\u5165\u540e\uff0c\u65e0\u6cd5\u589e\u52a0\u65b0\u7684\u5206\u533a\u5206\u914d\u64cd\u4f5c</p> </li> <li> <p>generateAssignment</p> </li> <li>executeAssignment</li> <li>verifyAssignment</li> </ul>"},{"location":"7-other/3-reassign-partition/#admin-api","title":"\u589e\u52a0 admin api","text":"<p>\u5206\u533a\u5206\u914d\u4efb\u52a1\u8fd0\u884c\u4e2d\uff0c\u53ef\u4ee5\u652f\u6301\u589e\u52a0\u65b0\u7684\u5206\u914d\u64cd\u4f5c\uff0c\u652f\u6301\u53d6\u6d88\uff08\u6062\u590d\uff09\u5206\u533a\u5206\u914d\u4efb\u52a1\u3002</p> <p>\u4fe1\u606f\u5b58\u50a8\u5728 ZooKeeper \u7684 <code>/brokers/topics/[topic]</code> \u8282\u70b9\u4e2d\u3002</p> <ul> <li>generateAssignment</li> <li>executeAssignment</li> <li>verifyAssignment</li> <li>cancelAssignment</li> <li>listReassignments</li> </ul> <p>\u589e\u52a0 2 \u4e2a\u65b0\u7684 API\uff1a</p> <ul> <li><code>alterPartitionAssignments</code><ul> <li>\u6539\u53d8\uff0c\u53ef\u4ee5\u65b0\u589e\uff0c\u4e5f\u53ef\u4ee5\u51cf\u5c11</li> <li>\u63d0\u4f9b\u7a7a\u7684\u5206\u914d\u8ba1\u5212\u5c06\u4f1a\u53d6\u6d88\u5206\u533a\u5206\u914d</li> </ul> </li> <li><code>listPartitionReassignments</code></li> </ul>"},{"location":"7-other/3-reassign-partition/#_1","title":"\u53c2\u8003","text":"<ul> <li>KIP-455: \u589e\u52a0\u7ba1\u7406 API \u7528\u6765\u5206\u533a\u91cd\u5206\u914d\uff1ahttps://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment</li> </ul>"}]}